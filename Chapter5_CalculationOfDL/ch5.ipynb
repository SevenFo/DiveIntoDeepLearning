{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.1009, -0.0407,  0.1246,  0.0582, -0.0555, -0.0764, -0.0761, -0.0391,\n          0.0492,  0.0110],\n        [-0.0154,  0.0865, -0.0603,  0.0951, -0.0533,  0.0984, -0.1372, -0.0331,\n          0.0872, -0.1189]], grad_fn=<AddmmBackward>)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use sequential to dribe a nn\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(20,256),# first layer\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256,10)\n",
    ")\n",
    "X = torch.rand(2,20)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# to create a block class\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20,256)\n",
    "        self.out = nn.Linear(256,10)\n",
    "\n",
    "    def forward(self,X): #\n",
    "        return self.out(F.relu(self.hidden(X)))\n",
    "# we can also add sequential into our blockm\n",
    "# attention: it should have a forward function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[-0.3487, -0.2946,  0.4397,  0.0554, -0.1817],\n",
      "        [ 0.3711, -0.1712,  0.0006, -0.0135, -0.2769],\n",
      "        [ 0.0731,  0.3509, -0.1399, -0.4341, -0.0101]])), ('bias', tensor([ 0.0704,  0.4454, -0.3070]))])\n",
      "<generator object Module.named_parameters at 0x7f04250a6c10>\n",
      "Parameter containing:\n",
      "tensor([ 0.0704,  0.4454, -0.3070], requires_grad=True)\n",
      "tensor([ 0.0704,  0.4454, -0.3070])\n",
      "[('weight', torch.Size([5, 2])), ('bias', torch.Size([5]))]\n",
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (module_0_in_block2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=2, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (module_1_in_block2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=2, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (module_2_in_block2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=2, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (module_3_in_block2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=2, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n",
      "OrderedDict([('0.module_0_in_block2.0.weight', tensor([[-0.4891, -0.0394, -0.4701, -0.4492],\n",
      "        [ 0.4861, -0.1761, -0.1634,  0.2727],\n",
      "        [ 0.4398, -0.4203, -0.4112, -0.4548],\n",
      "        [-0.3739,  0.0251,  0.3044, -0.1877],\n",
      "        [-0.2260, -0.0874,  0.1666, -0.2230],\n",
      "        [-0.2023,  0.4906,  0.2962, -0.3676],\n",
      "        [-0.2494,  0.3278,  0.3804, -0.1556],\n",
      "        [-0.2266, -0.1522, -0.4344, -0.1927]])), ('0.module_0_in_block2.0.bias', tensor([ 0.4732, -0.3508,  0.0901,  0.1175, -0.3480,  0.1250, -0.1844, -0.3953])), ('0.module_0_in_block2.2.weight', tensor([[ 0.1074,  0.2972, -0.1169,  0.1393,  0.1554, -0.2383,  0.2781, -0.3515],\n",
      "        [-0.1917, -0.1751,  0.2483,  0.0047, -0.1691, -0.1473,  0.0039,  0.0122]])), ('0.module_0_in_block2.2.bias', tensor([ 0.0111, -0.2300])), ('0.module_1_in_block2.0.weight', tensor([[ 0.0692, -0.2898,  0.0706, -0.1523],\n",
      "        [ 0.1861,  0.1010, -0.1512,  0.2493],\n",
      "        [-0.2239, -0.1636,  0.2297, -0.2475],\n",
      "        [-0.0228, -0.4940,  0.4678,  0.1463],\n",
      "        [-0.1347, -0.4991, -0.3952,  0.2217],\n",
      "        [ 0.4393, -0.2051,  0.1845,  0.0090],\n",
      "        [-0.2877,  0.4232,  0.0227,  0.3153],\n",
      "        [ 0.4473, -0.0594,  0.2197, -0.2968]])), ('0.module_1_in_block2.0.bias', tensor([-0.4697, -0.4412, -0.1609, -0.1941,  0.0292,  0.0081,  0.0356,  0.4025])), ('0.module_1_in_block2.2.weight', tensor([[-0.1712, -0.1850, -0.0745, -0.2182, -0.2212, -0.1744, -0.2979,  0.3232],\n",
      "        [-0.1073, -0.0157,  0.1057, -0.0463, -0.1355,  0.2978,  0.1130, -0.1435]])), ('0.module_1_in_block2.2.bias', tensor([ 0.1007, -0.2491])), ('0.module_2_in_block2.0.weight', tensor([[ 0.4425,  0.1021,  0.4436, -0.3706],\n",
      "        [ 0.2864,  0.1222, -0.4200, -0.1856],\n",
      "        [ 0.1404, -0.2742, -0.0247,  0.0507],\n",
      "        [-0.0429, -0.0591,  0.0332, -0.4606],\n",
      "        [-0.3641,  0.1936, -0.0432,  0.1204],\n",
      "        [-0.2048, -0.1145, -0.3346,  0.3067],\n",
      "        [-0.2942,  0.3348,  0.2575, -0.3377],\n",
      "        [-0.0516, -0.4216,  0.0115, -0.4522]])), ('0.module_2_in_block2.0.bias', tensor([ 0.1810, -0.4668,  0.3819, -0.3846, -0.1089, -0.0691, -0.3975, -0.4225])), ('0.module_2_in_block2.2.weight', tensor([[ 0.2914,  0.0685,  0.3145, -0.1954, -0.2715, -0.2623,  0.3215, -0.1811],\n",
      "        [-0.1217, -0.1509, -0.0694,  0.1687,  0.1167, -0.3252,  0.2906, -0.1928]])), ('0.module_2_in_block2.2.bias', tensor([0.2880, 0.2806])), ('0.module_3_in_block2.0.weight', tensor([[-0.2796, -0.3094,  0.2410, -0.1456],\n",
      "        [-0.2956, -0.0587,  0.0496,  0.1053],\n",
      "        [ 0.1359,  0.2889,  0.3567,  0.4037],\n",
      "        [-0.2365, -0.1321, -0.4747,  0.4582],\n",
      "        [-0.0625,  0.1423,  0.3471,  0.2752],\n",
      "        [-0.2466,  0.4839, -0.2552, -0.2967],\n",
      "        [-0.0430, -0.0006,  0.3648, -0.2745],\n",
      "        [ 0.2277, -0.0562,  0.1463, -0.2437]])), ('0.module_3_in_block2.0.bias', tensor([-0.4697, -0.2725,  0.3708,  0.4209, -0.2694, -0.3013, -0.2613,  0.4456])), ('0.module_3_in_block2.2.weight', tensor([[-0.3231, -0.1789,  0.1594,  0.3129,  0.0484, -0.1223,  0.3484, -0.1981],\n",
      "        [-0.0408, -0.2054,  0.1262, -0.0506,  0.3312,  0.2756, -0.1860, -0.1287]])), ('0.module_3_in_block2.2.bias', tensor([0.0371, 0.2903])), ('1.weight', tensor([[0.6026, 0.2960]])), ('1.bias', tensor([-0.3938]))])\n"
     ]
    }
   ],
   "source": [
    "# params managing\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(2,5),# first layer\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5,3)\n",
    ")\n",
    "# get params\n",
    "print(net[2].state_dict()) # return a dict of params including weight and bias\n",
    "print(net[2].named_parameters()) # return a generator containing named params\n",
    "print(net[2].bias) # parameter class\n",
    "print(net[2].bias.data)\n",
    "print([(name, params.shape )for name, params in net[0].named_parameters()])\n",
    "\n",
    "# multiple block\n",
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4,8),nn.ReLU(),\n",
    "                         nn.Linear(8,2),nn.ReLU())\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        net.add_module(f'module_{i}_in_block2',block1())\n",
    "    return net\n",
    "rgnet = nn.Sequential(block2(),nn.Linear(2,1))\n",
    "print(rgnet)\n",
    "# then try to get the params of the rgnet\n",
    "print(rgnet.state_dict()) # output three layter params\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "Sequential(\n  (0): Linear(in_features=2, out_features=5, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=5, out_features=3, bias=True)\n)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init params\n",
    "\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight,mean=0,std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "def init_constant(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight,100)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "def init_custom(m):\n",
    "    pass\n",
    "\n",
    "net.apply(init_normal) # input the init function to net"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# parameters binding\n",
    "share = nn.Linear(8,8) # shared layer\n",
    "net = nn.Sequential(nn.Linear(4,8),nn.ReLU(),\n",
    "                    share,nn.ReLU(),\n",
    "                    share,nn.ReLU(),\n",
    "                    nn.Linear(8,1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_make_subclass(): argument 'data' (position 2) must be Tensor, not list",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_15561/1144186753.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      9\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m0.1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m \u001B[0mcustomize_layer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mCenteredLayer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcustomize_layer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mFloatTensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcustomize_layer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnamed_parameters\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_15561/1144186753.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m      5\u001B[0m         \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m         \u001B[0;31m# self.weight=self.params todo\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mweight2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mParameter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m4\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m5\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m6\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m6\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m6\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m6\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m6\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m6\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m6\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m0.1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Applications/YOLOX/demo/MegEngine/PVMegengine/lib/python3.8/site-packages/torch/nn/parameter.py\u001B[0m in \u001B[0;36m__new__\u001B[0;34m(cls, data, requires_grad)\u001B[0m\n\u001B[1;32m     24\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mdata\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     25\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 26\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_make_subclass\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcls\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrequires_grad\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     27\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     28\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__deepcopy__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmemo\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: _make_subclass(): argument 'data' (position 2) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "#customize layer\n",
    "\n",
    "class CenteredLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.weight=self.params todo\n",
    "        self.weight2 = nn.Parameter([[1,2,3,4,5,6],[6,6,6,6,6,6]])\n",
    "    def forward(self,X):\n",
    "        return X-0.1\n",
    "\n",
    "customize_layer = CenteredLayer()\n",
    "print(customize_layer(torch.FloatTensor([0,0,1,2,3])))\n",
    "print(customize_layer.named_parameters())\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load() missing 1 required positional argument: 'f'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_15561/1791936221.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# w/r files\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msave\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: load() missing 1 required positional argument: 'f'"
     ]
    }
   ],
   "source": [
    "# w/r files\n",
    "torch.load()\n",
    "torch.save()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda', index=0)"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gpu\n",
    "\n",
    "torch.cuda.device_count()\n",
    "x = torch.tensor([123],device='cuda:0')\n",
    "x.device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}