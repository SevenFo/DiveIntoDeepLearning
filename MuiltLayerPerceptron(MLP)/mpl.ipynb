{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "this is the first multilayer network, muiltlayer perceptron\n",
    "aka MLP"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## not use models in torch\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "def get_fashion_mnist_labels(labels):\n",
    "    \"\"\"返回Fashion-MNIST数据集的文本标签。\"\"\"\n",
    "    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "    return [text_labels[int(i)] for i in labels]\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "def get_dataloader_workers():\n",
    "    return 8\n",
    "def load_data_fashion_mnist(batch_size ,resize = None):\n",
    "    trans = [transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0,transforms.Resize(resize))\n",
    "    trans = transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root='../data',train=True,transform=trans, download=True)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root='../data',train=False,transform=trans, download=True)\n",
    "\n",
    "    return (\n",
    "        data.DataLoader(mnist_train,batch_size,shuffle=True,num_workers=get_dataloader_workers()),\n",
    "        data.DataLoader(mnist_test,batch_size,shuffle=False,num_workers=get_dataloader_workers())\n",
    "    )\n",
    "class Accumulator:  #@save\n",
    "    \"\"\"vector adder。\"\"\"\n",
    "    '''\n",
    "    zip函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，\n",
    "    然后返回由这些元组组成的列表。\n",
    "    如果各个迭代器的元素个数不一致，则返回列表长度与最短的对象相同，\n",
    "    利用 * 号操作符，可以将元组解压为列表。\n",
    "    '''\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "def accuracy(y_hat,y):\n",
    "    '''\n",
    "    :param y_hat: the possibilities of every type in examples\n",
    "    :param y: the index of corre type\n",
    "    :return:\n",
    "    '''\n",
    "    if len(y_hat.shape) >1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1) # the max of a line => index or type(in this case)\n",
    "        # or get the predict index of correct type\n",
    "        cmp = y_hat.type(y.dtype) == y\n",
    "        # it may out a vector like [f,t,t,t,t,f,f,t]\n",
    "        return float(cmp.type(y.dtype).sum())\n",
    "\n",
    "def evaluate_accuracy(net,data_iter): #@save\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.eval()\n",
    "    metric = Accumulator(2)\n",
    "    for X, y in data_iter: # add the correct number of every batch\n",
    "        metric.add(accuracy(net(X),y),y.numel()) # correct number, data number\n",
    "    return metric[0]/metric[1]\n",
    "def train_epoch(net, train_iter, loss, updater):\n",
    "    \"\"\"\n",
    "    train for one epoch\n",
    "    :param net:\n",
    "    :param train_iter:\n",
    "    :param loss:\n",
    "    :param updater:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.train()\n",
    "    metric = Accumulator(3)\n",
    "    for X, y in train_iter:\n",
    "        y_hat = net(X) # calculate result\n",
    "        l = loss(y_hat,y) # the loss value of every example\n",
    "        if isinstance(updater, torch.optim.Optimizer):\n",
    "            # the updater should get the grad of every params and then updates those params\n",
    "            updater.zero_grad() # set grad to be zero\n",
    "            l.backward() # the backword function well calculate the grad of every node in calculating graph\n",
    "            # the l is the sum of the loss value of every example\n",
    "            updater.step()\n",
    "            metric.add(float(l)*len(y), accuracy(y_hat,y),y.size().numel())\n",
    "        else:\n",
    "            l.sum().backward() # the loss value of all example\n",
    "            updater(X.shape[0])\n",
    "            metric.add(float(l.sum()),accuracy(y_hat,y),y.numel())\n",
    "    return metric[0] / metric[2], metric[1]/metric[2]\n",
    "\n",
    "def train(net, train_iter, test_iter, loss, num_epochs, updater):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_matrics = train_epoch(net,train_iter,loss,updater)\n",
    "        test_acc = evaluate_accuracy(net,test_iter)\n",
    "        print('train: loss:{},acc:{}'.format(train_matrics[0],train_matrics[1]))\n",
    "        print('test: acc:{}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: loss:0.9274468812306722,acc:0.6921833333333334\n",
      "test: acc:0.7593\n",
      "train: loss:0.5683494225184123,acc:0.8025333333333333\n",
      "test: acc:0.8059\n",
      "train: loss:0.502886217546463,acc:0.8246666666666667\n",
      "test: acc:0.816\n",
      "train: loss:0.46216569860776263,acc:0.83795\n",
      "test: acc:0.8326\n",
      "train: loss:0.44189897316296894,acc:0.8454833333333334\n",
      "test: acc:0.8272\n",
      "train: loss:0.4202599067846934,acc:0.852\n",
      "test: acc:0.8348\n",
      "train: loss:0.40500399600664777,acc:0.8577666666666667\n",
      "test: acc:0.8331\n",
      "train: loss:0.3952644912560781,acc:0.8610833333333333\n",
      "test: acc:0.8453\n",
      "train: loss:0.3811689414024353,acc:0.8643166666666666\n",
      "test: acc:0.8363\n",
      "train: loss:0.3731047079722087,acc:0.8688166666666667\n",
      "test: acc:0.855\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size)\n",
    "num_inputs, num_outputs, num_hiddens = 784,10,1024\n",
    "W1 = nn.Parameter(torch.randn(num_inputs,num_hiddens,requires_grad=True)*0.01)\n",
    "b1 = nn.Parameter(torch.zeros(num_hiddens,requires_grad=True))\n",
    "W2 = nn.Parameter(torch.randn(num_hiddens,num_outputs,requires_grad=True)*0.01)\n",
    "b2 = nn.Parameter(torch.zeros(num_outputs,requires_grad=True))\n",
    "\n",
    "params=[W1,b1,W2,b2]\n",
    "\n",
    "def relu(X):\n",
    "    a = torch.zeros_like(X)\n",
    "    return torch.max(X,a)\n",
    "\n",
    "def net(X):\n",
    "    X = X.reshape((-1,num_inputs))\n",
    "    H = relu(X@W1+b1)\n",
    "    return H @ W2 + b2\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "num_eoochs, lr = 10,0.1\n",
    "updater = torch.optim.SGD(params,lr=lr)\n",
    "\n",
    "train(net,train_iter,test_iter,loss,num_eoochs,updater)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: loss:0.5007064084688823,acc:0.8249166666666666\n",
      "test: acc:0.8231\n",
      "train: loss:0.4700194479147593,acc:0.83475\n",
      "test: acc:0.8319\n",
      "train: loss:0.44932981621424356,acc:0.8427\n",
      "test: acc:0.8292\n",
      "train: loss:0.4345707261244456,acc:0.8454166666666667\n",
      "test: acc:0.8323\n",
      "train: loss:0.4195893929640452,acc:0.8518166666666667\n",
      "test: acc:0.8389\n",
      "train: loss:0.4094060695807139,acc:0.8548166666666667\n",
      "test: acc:0.8486\n",
      "train: loss:0.4008685843785604,acc:0.8568166666666667\n",
      "test: acc:0.8519\n",
      "NET L2:8.291254043579102,6.659847736358643\n",
      "===\n",
      "train: loss:1.0455683462778727,acc:0.6364166666666666\n",
      "test: acc:0.7219\n",
      "train: loss:0.6006122962474822,acc:0.7889\n",
      "test: acc:0.7829\n",
      "train: loss:0.518011437590917,acc:0.8180333333333333\n",
      "test: acc:0.8171\n",
      "train: loss:0.48089712238311766,acc:0.8315\n",
      "test: acc:0.8241\n",
      "train: loss:0.45446574199994405,acc:0.8401166666666666\n",
      "test: acc:0.8208\n",
      "train: loss:0.4330106850624085,acc:0.8470166666666666\n",
      "test: acc:0.8375\n",
      "train: loss:0.41847219700813293,acc:0.8532666666666666\n",
      "test: acc:0.839\n",
      "train: loss:0.40645191578865053,acc:0.85625\n",
      "test: acc:0.8353\n",
      "train: loss:0.39342625274658205,acc:0.8603833333333334\n",
      "test: acc:0.8513\n",
      "train: loss:0.3841066804250081,acc:0.8646333333333334\n",
      "test: acc:0.8452\n",
      "NET_decay L2:7.959894180297852,6.265789031982422\n",
      "train: loss:0.3873236755688985,acc:0.8625166666666667\n",
      "test: acc:0.8452\n",
      "train: loss:0.38732367992401123,acc:0.8625166666666667\n",
      "test: acc:0.8452\n",
      "train: loss:0.3873236732006073,acc:0.8625166666666667\n",
      "test: acc:0.8452\n",
      "train: loss:0.38732367509206134,acc:0.8625166666666667\n",
      "test: acc:0.8452\n",
      "train: loss:0.3873236690600713,acc:0.8625166666666667\n",
      "test: acc:0.8452\n",
      "train: loss:0.3873236725171407,acc:0.8625166666666667\n",
      "test: acc:0.8452\n",
      "train: loss:0.38732367464701334,acc:0.8625166666666667\n",
      "test: acc:0.8452\n",
      "train: loss:0.3873236661911011,acc:0.8625166666666667\n",
      "test: acc:0.8452\n",
      "train: loss:0.3873236768563588,acc:0.8625166666666667\n",
      "test: acc:0.8452\n",
      "train: loss:0.387323672246933,acc:0.8625166666666667\n",
      "test: acc:0.8452\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ReLU' object has no attribute 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_5451/3407276210.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     38\u001B[0m \u001B[0mtrain_dropout\u001B[0m \u001B[0;34m=\u001B[0m  \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptim\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSGD\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnet_dropout\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparameters\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mlr\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlr\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m#\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnet_decay\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtrain_iter\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtest_iter\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mloss\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mnum_eoochs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtrainer_d\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 40\u001B[0;31m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"NET_dropout L2:{net_dropout[1].weight.norm().item()},{net_dropout[4].weight.norm().item()}\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     41\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Applications/YOLOX/demo/MegEngine/PVMegengine/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m    945\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mmodules\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    946\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mmodules\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 947\u001B[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001B[0m\u001B[1;32m    948\u001B[0m             type(self).__name__, name))\n\u001B[1;32m    949\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'ReLU' object has no attribute 'weight'"
     ]
    }
   ],
   "source": [
    "## use model in torch\n",
    "\n",
    "net = nn.Sequential(nn.Flatten(),nn.Linear(784,256),nn.ReLU(),nn.Dropout(0.4),nn.Linear(256,10))\n",
    "net_decay = nn.Sequential(nn.Flatten(),nn.Linear(784,256),nn.ReLU(),nn.Linear(256,10))\n",
    "net_dropout = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(784,256),\n",
    "    nn.ReLU(),\n",
    "    # nn.Dropout(0.2),\n",
    "    nn.Linear(256,256),\n",
    "    nn.ReLU(),\n",
    "    # nn.Dropout(0.5),\n",
    "    nn.Linear(256,10))\n",
    "\n",
    "def init_weight(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight,std=0.01)\n",
    "\n",
    "net.apply(init_weight)\n",
    "net_decay.apply(init_weight)\n",
    "net_dropout.apply(init_weight)\n",
    "\n",
    "batch_size,lr,num_eoochs = 256,0.1,10\n",
    "loss = nn.CrossEntropyLoss()\n",
    "trainer = torch.optim.SGD(net.parameters(),lr=lr) #\n",
    "# params = list(net.parameters())# n\n",
    "# print(len(list(params)))# o decay\n",
    "trainer_decay = torch.optim.SGD([{\"params\":net_decay[1].weight,'wight_decay':10},{\"params\":net_decay[1].bias},{\"params\":net_decay[3].weight,'wight_decay':5},{\"params\":net_decay[3].bias}],lr=lr)\n",
    "# for p in net.parameters():\n",
    "#     print(p)\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size)\n",
    "train(net,train_iter,test_iter,loss,num_eoochs,trainer)\n",
    "print(f\"NET L2:{net[1].weight.norm().item()},{net[4].weight.norm().item()}\")\n",
    "print(\"===\")_iter, test_iter = load_data_fashion_mnist(batch_size)\n",
    "train(net_decay,train_iter,test_iter,loss,num_eoochs,trainer_decay)\n",
    "print(f\"NET_decay L2:{net_decay[1].weight.norm().item()},{net_decay[3].weight.norm().item()}\")\n",
    "train_dropout =  torch.optim.SGD(net_dropout.parameters(),lr=lr) #\n",
    "train(net_decay,train_iter,test_iter,loss,num_eoochs,train_dropout)\n",
    "print(f\"NET_dropout L2:{net_dropout[1].weight.norm().item()},{net_dropout[4].weight.norm().item()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### why the L2 of net parameters has increased after add weight decay???"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: loss:0.3649889024098714,acc:0.8715\n",
      "test: acc:0.8535\n",
      "train: loss:0.3649889111359914,acc:0.8715\n",
      "test: acc:0.8535\n",
      "train: loss:0.36498890368143716,acc:0.8715\n",
      "test: acc:0.8535\n",
      "train: loss:0.3649889089743296,acc:0.8715\n",
      "test: acc:0.8535\n",
      "train: loss:0.3649889001051585,acc:0.8715\n",
      "test: acc:0.8535\n",
      "train: loss:0.3649889039198558,acc:0.8715\n",
      "test: acc:0.8535\n",
      "train: loss:0.3649889044602712,acc:0.8715\n",
      "test: acc:0.8535\n",
      "train: loss:0.3649889039993286,acc:0.8715\n",
      "test: acc:0.8535\n",
      "train: loss:0.36498890665372213,acc:0.8715\n",
      "test: acc:0.8535\n",
      "train: loss:0.36498890754381813,acc:0.8715\n",
      "test: acc:0.8535\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ReLU' object has no attribute 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_5451/1849475355.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0mtrain_dropout\u001B[0m \u001B[0;34m=\u001B[0m  \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptim\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSGD\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnet_dropout\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparameters\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mlr\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlr\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m#\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnet_decay\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtrain_iter\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtest_iter\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mloss\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mnum_eoochs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtrainer_d\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 17\u001B[0;31m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"NET_dropout L2:{net_dropout[1].weight.norm().item()},{net_dropout[4].weight.norm().item()}\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/Applications/YOLOX/demo/MegEngine/PVMegengine/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m    945\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mmodules\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    946\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mmodules\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 947\u001B[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001B[0m\u001B[1;32m    948\u001B[0m             type(self).__name__, name))\n\u001B[1;32m    949\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'ReLU' object has no attribute 'weight'"
     ]
    }
   ],
   "source": [
    "# dropout\n",
    "# let some param in a set of params to be zero so as the result would not rely on some params\n",
    "batch_size,lr,num_eoochs = 256,0.5,10\n",
    "net_dropout = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(784,256),\n",
    "    nn.ReLU(),\n",
    "    # nn.Dropout(0.2),\n",
    "    nn.Linear(256,256),\n",
    "    nn.ReLU(),\n",
    "    # nn.Dropout(0.5),\n",
    "    nn.Linear(256,10))\n",
    "net_dropout.apply(init_weight)\n",
    "trainer_d = torch.optim.SGD(net_dropout.parameters(),lr=lr) #\n",
    "train_dropout =  torch.optim.SGD(net_dropout.parameters(),lr=lr) #\n",
    "train(net_decay,train_iter,test_iter,loss,num_eoochs,trainer_d)\n",
    "print(f\"NET_dropout L2:{net_dropout[1].weight.norm().item()},{net_dropout[4].weight.norm().item()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}