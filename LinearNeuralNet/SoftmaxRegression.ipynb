{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "this time we use fashion-mnist to test softmax regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "import onnx\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "\n",
    "trans = transforms.ToTensor() # create a transformer to trans data from PIL TO TENSOR(FLOAT 32)\n",
    "mnist_train = torchvision.datasets.FashionMNIST(root='../data',train=True,transform=trans, download=True)\n",
    "mnist_test = torchvision.datasets.FashionMNIST(root='../data',train=False,transform=trans, download=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%matplotlib inline #%save\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "(60000, 10000)"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mnist_train), len(mnist_test)\n",
    "# mnist_train shape: example line: data(1*28*28) label(1*1*1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "def get_fashion_mnist_labels(labels):\n",
    "    \"\"\"返回Fashion-MNIST数据集的文本标签。\"\"\"\n",
    "    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "    return [text_labels[int(i)] for i in labels]\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "def get_dataloader_workers():\n",
    "    return 8\n",
    "\n",
    "train_iter = data.DataLoader(mnist_train,batch_size=batch_size,shuffle=True,num_workers=get_dataloader_workers())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "def load_data_fashion_mnist(batch_size ,resize = None):\n",
    "    trans = [transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0,transforms.Resize(resize))\n",
    "    trans = transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root='../data',train=True,transform=trans, download=True)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root='../data',train=False,transform=trans, download=True)\n",
    "\n",
    "    return (\n",
    "        data.DataLoader(mnist_train,batch_size,shuffle=True,num_workers=get_dataloader_workers()),\n",
    "        data.DataLoader(mnist_test,batch_size,shuffle=False,num_workers=get_dataloader_workers())\n",
    "    )\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = load_data_fashion_mnist(256)\n",
    "\n",
    "num_inputs = 784 # 28 x 28 x 1\n",
    "num_outputs = 10 # type number\n",
    "W = torch.normal(0,0.01,size=(num_inputs,num_outputs),requires_grad=True)\n",
    "b = torch.zeros(num_outputs,requires_grad=True)\n",
    "\n",
    "print(b)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% initial params\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "## softmax function\n",
    "from IPython import display\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from d2l import sgd\n",
    "class Accumulator:  #@save\n",
    "    \"\"\"vector adder。\"\"\"\n",
    "    '''\n",
    "    zip函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，\n",
    "    然后返回由这些元组组成的列表。\n",
    "    如果各个迭代器的元素个数不一致，则返回列表长度与最短的对象相同，\n",
    "    利用 * 号操作符，可以将元组解压为列表。\n",
    "    '''\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "# class Animator:  #@save\n",
    "#     \"\"\"在动画中绘制数据。\"\"\"\n",
    "#     def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "#                  ylim=None, xscale='linear', yscale='linear',\n",
    "#                  fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
    "#                  figsize=(3.5, 2.5)):\n",
    "#         # 增量地绘制多条线\n",
    "#         if legend is None:\n",
    "#             legend = []\n",
    "#         d2l.use_svg_display()\n",
    "#         self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)\n",
    "#         if nrows * ncols == 1:\n",
    "#             self.axes = [self.axes, ]\n",
    "#         # 使用lambda函数捕获参数\n",
    "#         self.config_axes = lambda: d2l.set_axes(\n",
    "#             self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "#         self.X, self.Y, self.fmts = None, None, fmts\n",
    "#\n",
    "#     def add(self, x, y):\n",
    "#         # 向图表中添加多个数据点\n",
    "#         if not hasattr(y, \"__len__\"):\n",
    "#             y = [y]\n",
    "#         n = len(y)\n",
    "#         if not hasattr(x, \"__len__\"):\n",
    "#             x = [x] * n\n",
    "#         if not self.X:\n",
    "#             self.X = [[] for _ in range(n)]\n",
    "#         if not self.Y:\n",
    "#             self.Y = [[] for _ in range(n)]\n",
    "#         for i, (a, b) in enumerate(zip(x, y)):\n",
    "#             if a is not None and b is not None:\n",
    "#                 self.X[i].append(a)\n",
    "#                 self.Y[i].append(b)\n",
    "#         self.axes[0].cla()\n",
    "#         for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "#             self.axes[0].plot(x, y, fmt)\n",
    "#         self.config_axes()\n",
    "#         display.display(self.fig)\n",
    "#         display.clear_output(wait=True)\n",
    "\n",
    "def softmax(X):\n",
    "    X_exp = torch.exp(X)\n",
    "    return X_exp/X_exp.sum(1, keepdim = True) # calculate sum by line\n",
    "\n",
    "def sgd(params, lr, batch_size): #@save\n",
    "    \"\"\"\n",
    "    date every param in params: b and w\n",
    "    aram params:\n",
    "    :param lr:\n",
    "    :param batch_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_()\n",
    "\n",
    "def net(X):\n",
    "    return softmax(torch.matmul(X.reshape(-1,W.shape[0]),W)+b) # to reshape pics matrix to a line\n",
    "\n",
    "def corss_entropy(y_hat,y):\n",
    "    '''\n",
    "    y_hat: the possibilities of every type in examples\n",
    "    such as\n",
    "    [\n",
    "        [0.1,0.2,0.3,...,0.8],\n",
    "        [0.1,0.2,0.3,...,0.8],\n",
    "        [0.1,0.2,0.3,...,0.8]\n",
    "    ]\n",
    "    '''\n",
    "    return -torch.log(y_hat[range(len(y_hat)),y])\n",
    "\n",
    "def accuracy(y_hat,y):\n",
    "    '''\n",
    "    :param y_hat: the possibilities of every type in examples\n",
    "    :param y: the index of correct type\n",
    "    :return:\n",
    "    '''\n",
    "    if len(y_hat.shape) >1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1) # the max of a line => index or type(in this case)\n",
    "        # or get the predict index of correct type\n",
    "        cmp = y_hat.type(y.dtype) == y\n",
    "        # it may out a vector like [f,t,t,t,t,f,f,t]\n",
    "        return float(cmp.type(y.dtype).sum())\n",
    "\n",
    "def evaluate_accuracy(net,data_iter): #@save\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.eval()\n",
    "    metric = Accumulator(2)\n",
    "    for X, y in data_iter: # add the correct number of every batch\n",
    "        metric.add(accuracy(net(X),y),y.numel()) # correct number, data number\n",
    "    return metric[0]/metric[1]\n",
    "\n",
    "def train_epoch(net, train_iter, loss, updater):\n",
    "    \"\"\"\n",
    "    train for one epoch\n",
    "    :param net:\n",
    "    :param train_iter:\n",
    "    :param loss:\n",
    "    :param updater:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.train()\n",
    "    metric = Accumulator(3)\n",
    "    for X, y in train_iter:\n",
    "        y_hat = net(X) # calculate result\n",
    "        l = loss(y_hat,y) # the loss value of every example\n",
    "        if isinstance(updater, torch.optim.Optimizer):\n",
    "            updater.zero_grad() # the updater should get the grad of every params and the update those params\n",
    "            l.backward() # the backword function well calculate the grad of every node in calculating graph\n",
    "            # the l is the sum of the loss value of every example\n",
    "            updater.step()\n",
    "            metric.add(float(l)*len(y), accuracy(y_hat,y),y.size().numel())\n",
    "        else:\n",
    "            l.sum().backward() # the loss value of all example\n",
    "            updater(X.shape[0])\n",
    "            metric.add(float(l.sum()),accuracy(y_hat,y),y.numel())\n",
    "    return metric[0] / metric[2], metric[1]/metric[2]\n",
    "\n",
    "def train(net, train_iter, test_iter, loss, num_epochs, updater):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_matrics = train_epoch(net,train_iter,loss,updater)\n",
    "        test_acc = evaluate_accuracy(net,test_iter)\n",
    "        print('train: loss:{},acc:{}'.format(train_matrics[0],train_matrics[1]))\n",
    "        print('test: acc:{}'.format(test_acc))\n",
    "lr = 0.1\n",
    "def updater(batch_size):\n",
    "    return sgd([W,b],lr,batch_size)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% create net\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: loss:0.3876960289001465,acc:0.8661\n",
      "test: acc:0.8429\n",
      "train: loss:0.38620220266977945,acc:0.8669833333333333\n",
      "test: acc:0.8365\n",
      "train: loss:0.3866526940027873,acc:0.8664333333333334\n",
      "test: acc:0.84\n",
      "train: loss:0.3862461729685466,acc:0.8664666666666667\n",
      "test: acc:0.8391\n",
      "train: loss:0.38529468275705975,acc:0.8672333333333333\n",
      "test: acc:0.8411\n",
      "train: loss:0.38535523897806806,acc:0.8668\n",
      "test: acc:0.8399\n",
      "train: loss:0.38497399520874026,acc:0.8668\n",
      "test: acc:0.844\n",
      "train: loss:0.38466697476704914,acc:0.8672\n",
      "test: acc:0.8454\n",
      "train: loss:0.38428556276957193,acc:0.8670833333333333\n",
      "test: acc:0.8383\n",
      "train: loss:0.3841831718126933,acc:0.8668833333333333\n",
      "test: acc:0.8424\n",
      "train: loss:0.3837581865310669,acc:0.8671333333333333\n",
      "test: acc:0.8379\n",
      "train: loss:0.3835071088155111,acc:0.86845\n",
      "test: acc:0.8437\n",
      "train: loss:0.3838327527999878,acc:0.8678333333333333\n",
      "test: acc:0.8437\n",
      "train: loss:0.38338331985473634,acc:0.8678833333333333\n",
      "test: acc:0.8426\n",
      "train: loss:0.3826045313199361,acc:0.86775\n",
      "test: acc:0.844\n",
      "train: loss:0.38284586079915367,acc:0.8676833333333334\n",
      "test: acc:0.8442\n",
      "train: loss:0.3824467177073161,acc:0.8678666666666667\n",
      "test: acc:0.8437\n",
      "train: loss:0.38202468554178876,acc:0.8682666666666666\n",
      "test: acc:0.8438\n",
      "train: loss:0.38208035214742025,acc:0.86775\n",
      "test: acc:0.8427\n",
      "train: loss:0.3819047862370809,acc:0.8679166666666667\n",
      "test: acc:0.8379\n",
      "train: loss:0.3817187540690104,acc:0.8683833333333333\n",
      "test: acc:0.8441\n",
      "train: loss:0.38062405732472737,acc:0.8683\n",
      "test: acc:0.8423\n",
      "train: loss:0.38151432615915937,acc:0.8681833333333333\n",
      "test: acc:0.8436\n",
      "train: loss:0.3814058442433675,acc:0.8684\n",
      "test: acc:0.8384\n",
      "train: loss:0.3808074337005615,acc:0.8680333333333333\n",
      "test: acc:0.8423\n",
      "train: loss:0.3805580991109212,acc:0.86855\n",
      "test: acc:0.8418\n",
      "train: loss:0.38053179531097414,acc:0.8681666666666666\n",
      "test: acc:0.8408\n",
      "train: loss:0.3798538594563802,acc:0.8683\n",
      "test: acc:0.8409\n",
      "train: loss:0.37923022616704305,acc:0.8690333333333333\n",
      "test: acc:0.8453\n",
      "train: loss:0.3799667537689209,acc:0.8678166666666667\n",
      "test: acc:0.8431\n",
      "train: loss:0.3794611208597819,acc:0.8684666666666667\n",
      "test: acc:0.8432\n",
      "train: loss:0.37956383657455445,acc:0.8680333333333333\n",
      "test: acc:0.8424\n",
      "train: loss:0.37895419616699216,acc:0.8693\n",
      "test: acc:0.8415\n",
      "train: loss:0.37986343631744385,acc:0.86795\n",
      "test: acc:0.8444\n",
      "train: loss:0.37867318077087403,acc:0.8685333333333334\n",
      "test: acc:0.8408\n",
      "train: loss:0.37845402755737306,acc:0.8689333333333333\n",
      "test: acc:0.8435\n",
      "train: loss:0.37833330262502035,acc:0.86915\n",
      "test: acc:0.8426\n",
      "train: loss:0.37771412703196205,acc:0.86885\n",
      "test: acc:0.844\n",
      "train: loss:0.378073171043396,acc:0.86905\n",
      "test: acc:0.8442\n",
      "train: loss:0.3774853651046753,acc:0.8695333333333334\n",
      "test: acc:0.8388\n",
      "train: loss:0.37790498739878337,acc:0.8688833333333333\n",
      "test: acc:0.8437\n",
      "train: loss:0.3776091623624166,acc:0.8690333333333333\n",
      "test: acc:0.8436\n",
      "train: loss:0.3776718819936117,acc:0.8686833333333334\n",
      "test: acc:0.8424\n",
      "train: loss:0.37730432516733803,acc:0.86835\n",
      "test: acc:0.8452\n",
      "train: loss:0.3772201520284017,acc:0.8688833333333333\n",
      "test: acc:0.8391\n",
      "train: loss:0.3770850724538167,acc:0.8695\n",
      "test: acc:0.8416\n",
      "train: loss:0.3771138230641683,acc:0.8690666666666667\n",
      "test: acc:0.8447\n",
      "train: loss:0.37733035163879397,acc:0.86845\n",
      "test: acc:0.8378\n",
      "train: loss:0.376665642229716,acc:0.8692166666666666\n",
      "test: acc:0.8446\n",
      "train: loss:0.3764972942988078,acc:0.8697333333333334\n",
      "test: acc:0.8458\n",
      "train: loss:0.3761143882751465,acc:0.8687\n",
      "test: acc:0.8441\n",
      "train: loss:0.3759546209017436,acc:0.8707\n",
      "test: acc:0.8377\n",
      "train: loss:0.37474834429423015,acc:0.8702833333333333\n",
      "test: acc:0.8389\n",
      "train: loss:0.3756125463485718,acc:0.8697166666666667\n",
      "test: acc:0.8451\n",
      "train: loss:0.3753624490102132,acc:0.86925\n",
      "test: acc:0.8423\n",
      "train: loss:0.3750973924001058,acc:0.8695166666666667\n",
      "test: acc:0.837\n",
      "train: loss:0.3752298098882039,acc:0.86915\n",
      "test: acc:0.8414\n",
      "train: loss:0.3749120885848999,acc:0.8691833333333333\n",
      "test: acc:0.8442\n",
      "train: loss:0.3752048488299052,acc:0.8694333333333333\n",
      "test: acc:0.8373\n",
      "train: loss:0.3746206975301107,acc:0.8706666666666667\n",
      "test: acc:0.8428\n",
      "train: loss:0.37400122458140056,acc:0.8697833333333334\n",
      "test: acc:0.8393\n",
      "train: loss:0.37431325028737383,acc:0.8702666666666666\n",
      "test: acc:0.8428\n",
      "train: loss:0.374325296497345,acc:0.8704833333333334\n",
      "test: acc:0.8427\n",
      "train: loss:0.3734994972864787,acc:0.8709\n",
      "test: acc:0.844\n",
      "train: loss:0.37430268344879153,acc:0.87005\n",
      "test: acc:0.8431\n",
      "train: loss:0.3741121290206909,acc:0.8696833333333334\n",
      "test: acc:0.8362\n",
      "train: loss:0.37474800345102943,acc:0.8696333333333334\n",
      "test: acc:0.8446\n",
      "train: loss:0.3728549004236857,acc:0.8717333333333334\n",
      "test: acc:0.8406\n",
      "train: loss:0.3735065399805705,acc:0.8703166666666666\n",
      "test: acc:0.8393\n",
      "train: loss:0.37297321014404294,acc:0.87035\n",
      "test: acc:0.84\n",
      "train: loss:0.3733119914372762,acc:0.86925\n",
      "test: acc:0.8417\n",
      "train: loss:0.3732341667811076,acc:0.8706333333333334\n",
      "test: acc:0.8426\n",
      "train: loss:0.37327387879689533,acc:0.87035\n",
      "test: acc:0.8446\n",
      "train: loss:0.3732173360824585,acc:0.8705333333333334\n",
      "test: acc:0.8437\n",
      "train: loss:0.3729131053288778,acc:0.8703333333333333\n",
      "test: acc:0.8387\n",
      "train: loss:0.3736454885482788,acc:0.8706\n",
      "test: acc:0.8444\n",
      "train: loss:0.3717585271835327,acc:0.8708833333333333\n",
      "test: acc:0.8399\n",
      "train: loss:0.3721031377792358,acc:0.8714666666666666\n",
      "test: acc:0.8449\n",
      "train: loss:0.3725171349843343,acc:0.8707666666666667\n",
      "test: acc:0.841\n",
      "train: loss:0.3723147450764974,acc:0.8705666666666667\n",
      "test: acc:0.8457\n",
      "train: loss:0.3718699906984965,acc:0.871\n",
      "test: acc:0.8443\n",
      "train: loss:0.37181972020467124,acc:0.8710666666666667\n",
      "test: acc:0.8443\n",
      "train: loss:0.3722056437174479,acc:0.8709833333333333\n",
      "test: acc:0.8386\n",
      "train: loss:0.37255351428985595,acc:0.8704666666666667\n",
      "test: acc:0.8376\n",
      "train: loss:0.37158374217351275,acc:0.8715\n",
      "test: acc:0.8414\n",
      "train: loss:0.3714782520294189,acc:0.8706333333333334\n",
      "test: acc:0.8437\n",
      "train: loss:0.37118121795654296,acc:0.8716833333333334\n",
      "test: acc:0.8442\n",
      "train: loss:0.3705794355392456,acc:0.8710333333333333\n",
      "test: acc:0.8405\n",
      "train: loss:0.3716464703241984,acc:0.87095\n",
      "test: acc:0.8422\n",
      "train: loss:0.37101962445576986,acc:0.87145\n",
      "test: acc:0.8454\n",
      "train: loss:0.370661466217041,acc:0.8714166666666666\n",
      "test: acc:0.8248\n",
      "train: loss:0.3710570769627889,acc:0.8704\n",
      "test: acc:0.8435\n",
      "train: loss:0.3704695885976156,acc:0.8712166666666666\n",
      "test: acc:0.8442\n",
      "train: loss:0.37000912291208904,acc:0.8716\n",
      "test: acc:0.8422\n",
      "train: loss:0.3705983856201172,acc:0.87135\n",
      "test: acc:0.8454\n",
      "train: loss:0.3702941499710083,acc:0.8708833333333333\n",
      "test: acc:0.8414\n",
      "train: loss:0.3701491700490316,acc:0.8709\n",
      "test: acc:0.8424\n",
      "train: loss:0.37011223990122477,acc:0.8711\n",
      "test: acc:0.8443\n",
      "train: loss:0.3704571183840434,acc:0.8712\n",
      "test: acc:0.844\n",
      "train: loss:0.36932149718602497,acc:0.8709\n",
      "test: acc:0.8422\n",
      "train: loss:0.37015726165771484,acc:0.8715833333333334\n",
      "test: acc:0.8402\n",
      "train: loss:0.37028121744791664,acc:0.8711666666666666\n",
      "test: acc:0.8424\n",
      "train: loss:0.3690799033164978,acc:0.8715833333333334\n",
      "test: acc:0.8336\n",
      "train: loss:0.37004265410105386,acc:0.8712666666666666\n",
      "test: acc:0.8392\n",
      "train: loss:0.3692776112238566,acc:0.8721166666666667\n",
      "test: acc:0.8446\n",
      "train: loss:0.36833157170613606,acc:0.87165\n",
      "test: acc:0.8451\n",
      "train: loss:0.3694157679239909,acc:0.8715333333333334\n",
      "test: acc:0.8409\n",
      "train: loss:0.36931691309611003,acc:0.8719666666666667\n",
      "test: acc:0.8393\n",
      "train: loss:0.36949455070495607,acc:0.8714833333333334\n",
      "test: acc:0.8427\n",
      "train: loss:0.3692613082249959,acc:0.8707666666666667\n",
      "test: acc:0.8407\n",
      "train: loss:0.3686778441429138,acc:0.8718666666666667\n",
      "test: acc:0.8445\n",
      "train: loss:0.36925294841130574,acc:0.8716333333333334\n",
      "test: acc:0.8415\n",
      "train: loss:0.36877907269795734,acc:0.8716833333333334\n",
      "test: acc:0.8466\n",
      "train: loss:0.3683153106689453,acc:0.87195\n",
      "test: acc:0.8456\n",
      "train: loss:0.3681195391337077,acc:0.8718\n",
      "test: acc:0.84\n",
      "train: loss:0.36896956049601237,acc:0.8724666666666666\n",
      "test: acc:0.8435\n",
      "train: loss:0.3682033376693726,acc:0.87165\n",
      "test: acc:0.8436\n",
      "train: loss:0.36895116062164307,acc:0.8711666666666666\n",
      "test: acc:0.8467\n",
      "train: loss:0.3679828106562297,acc:0.8719166666666667\n",
      "test: acc:0.844\n",
      "train: loss:0.36743455963134763,acc:0.8724\n",
      "test: acc:0.8209\n",
      "train: loss:0.36884293394088746,acc:0.8717166666666667\n",
      "test: acc:0.8453\n",
      "train: loss:0.3681502185821533,acc:0.8723\n",
      "test: acc:0.8346\n",
      "train: loss:0.36832997420628866,acc:0.8728333333333333\n",
      "test: acc:0.842\n",
      "train: loss:0.36789943176905315,acc:0.87205\n",
      "test: acc:0.8378\n",
      "train: loss:0.367475652217865,acc:0.8722833333333333\n",
      "test: acc:0.8443\n",
      "train: loss:0.3673082063674927,acc:0.8718166666666667\n",
      "test: acc:0.8423\n",
      "train: loss:0.3682377305984497,acc:0.8709833333333333\n",
      "test: acc:0.84\n",
      "train: loss:0.3674998442967733,acc:0.8720333333333333\n",
      "test: acc:0.8378\n",
      "train: loss:0.3684828482309977,acc:0.8716833333333334\n",
      "test: acc:0.8399\n",
      "train: loss:0.3669991450627645,acc:0.8721166666666667\n",
      "test: acc:0.8447\n",
      "train: loss:0.3676800216674805,acc:0.8718833333333333\n",
      "test: acc:0.8323\n",
      "train: loss:0.3667102413813273,acc:0.8719\n",
      "test: acc:0.8461\n",
      "train: loss:0.36719016056060794,acc:0.8714833333333334\n",
      "test: acc:0.8456\n",
      "train: loss:0.36688552945454916,acc:0.8720666666666667\n",
      "test: acc:0.8439\n",
      "train: loss:0.3668691343307495,acc:0.8728666666666667\n",
      "test: acc:0.8434\n",
      "train: loss:0.366451740582784,acc:0.8724333333333333\n",
      "test: acc:0.842\n",
      "train: loss:0.36652565485636396,acc:0.8719\n",
      "test: acc:0.8437\n",
      "train: loss:0.3660661958058675,acc:0.8728833333333333\n",
      "test: acc:0.8466\n",
      "train: loss:0.3664626459757487,acc:0.8721666666666666\n",
      "test: acc:0.841\n",
      "train: loss:0.36613044395446775,acc:0.8724666666666666\n",
      "test: acc:0.837\n",
      "train: loss:0.36622922852834067,acc:0.87195\n",
      "test: acc:0.8435\n",
      "train: loss:0.3659528279622396,acc:0.8724833333333334\n",
      "test: acc:0.8411\n",
      "train: loss:0.3662366134007772,acc:0.8718333333333333\n",
      "test: acc:0.8439\n",
      "train: loss:0.3661797223409017,acc:0.8723666666666666\n",
      "test: acc:0.8394\n",
      "train: loss:0.366071652730306,acc:0.87255\n",
      "test: acc:0.8431\n",
      "train: loss:0.365709029452006,acc:0.8729833333333333\n",
      "test: acc:0.8415\n",
      "train: loss:0.3663039566675822,acc:0.8727\n",
      "test: acc:0.8425\n",
      "train: loss:0.36597142912546793,acc:0.8720333333333333\n",
      "test: acc:0.8447\n",
      "train: loss:0.36616569147109984,acc:0.8724166666666666\n",
      "test: acc:0.8408\n",
      "train: loss:0.36598676986694334,acc:0.8724\n",
      "test: acc:0.8378\n",
      "train: loss:0.36537192446390787,acc:0.87205\n",
      "test: acc:0.8432\n",
      "train: loss:0.36619075463612877,acc:0.8721333333333333\n",
      "test: acc:0.8439\n",
      "train: loss:0.3656580700556437,acc:0.8720333333333333\n",
      "test: acc:0.8464\n",
      "train: loss:0.3659328042348226,acc:0.8723166666666666\n",
      "test: acc:0.842\n",
      "train: loss:0.3654296775182088,acc:0.8733666666666666\n",
      "test: acc:0.8354\n",
      "train: loss:0.36552184104919433,acc:0.8725333333333334\n",
      "test: acc:0.8462\n",
      "train: loss:0.36546505908966065,acc:0.8729166666666667\n",
      "test: acc:0.8434\n",
      "train: loss:0.36565621344248456,acc:0.8727333333333334\n",
      "test: acc:0.8432\n",
      "train: loss:0.36518191089630125,acc:0.8731333333333333\n",
      "test: acc:0.8432\n",
      "train: loss:0.3655305388768514,acc:0.8728833333333333\n",
      "test: acc:0.8444\n",
      "train: loss:0.364464423084259,acc:0.8732666666666666\n",
      "test: acc:0.8401\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 100\n",
    "train(net,train_iter,test_iter,corss_entropy,num_epoch,updater)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: loss:0.7870307423909505,acc:0.7473333333333333\n",
      "test: acc:0.7871\n",
      "train: loss:0.5693782514413198,acc:0.8141\n",
      "test: acc:0.8107\n",
      "train: loss:0.525407086722056,acc:0.8263\n",
      "test: acc:0.8048\n",
      "train: loss:0.5017995771249135,acc:0.8323\n",
      "test: acc:0.8255\n",
      "train: loss:0.48496064836184183,acc:0.8375166666666667\n",
      "test: acc:0.8276\n",
      "train: loss:0.4731929989973704,acc:0.8402\n",
      "test: acc:0.8248\n",
      "train: loss:0.4648489554087321,acc:0.84205\n",
      "test: acc:0.8305\n",
      "train: loss:0.4579583891073863,acc:0.8449333333333333\n",
      "test: acc:0.8322\n",
      "train: loss:0.4522715295950572,acc:0.8467333333333333\n",
      "test: acc:0.8311\n",
      "train: loss:0.4467018487453461,acc:0.8480166666666666\n",
      "test: acc:0.8349\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "## use the models in torch to train\n",
    "net = nn.Sequential(nn.Flatten(),nn.Linear(784,10)) # flatten layout make a matrix be a vector\n",
    "\n",
    "def init_weight(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std=0.01)\n",
    "net.apply(init_weight) # ?\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "trainer = torch.optim.SGD(net.parameters(),lr = 0.1)\n",
    "\n",
    "num_epoch = 10\n",
    "train(net,train_iter,test_iter,loss,num_epoch,trainer)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}