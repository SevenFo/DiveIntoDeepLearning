{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "net will be trained on cuda:0\n",
      "batch_0: train_l:2.304079532623291, train_accuracy:0.1015625, time_count:0.1982729434967041\n",
      "batch_10: train_l:2.2864859104156494, train_accuracy:0.1484375, time_count:0.1981797218322754\n",
      "batch_20: train_l:2.2666110084170388, train_accuracy:0.16703869047619047, time_count:0.19864797592163086\n",
      "batch_30: train_l:2.249250127423194, train_accuracy:0.19480846774193547, time_count:0.19713163375854492\n",
      "batch_40: train_l:2.2502381220096495, train_accuracy:0.1794969512195122, time_count:0.19765233993530273\n",
      "batch_50: train_l:2.2339439836202883, train_accuracy:0.18244485294117646, time_count:0.19726800918579102\n",
      "batch_60: train_l:2.2086783471654674, train_accuracy:0.19697745901639344, time_count:0.19753289222717285\n",
      "batch_70: train_l:2.1412801406752897, train_accuracy:0.21764964788732394, time_count:0.1972038745880127\n",
      "batch_80: train_l:2.041437046763338, train_accuracy:0.25559413580246915, time_count:0.19713258743286133\n",
      "batch_90: train_l:1.9694457244087051, train_accuracy:0.2817651098901099, time_count:0.1973876953125\n",
      "batch_100: train_l:1.8745527603838703, train_accuracy:0.31373762376237624, time_count:0.19652485847473145\n",
      "batch_110: train_l:1.8024154976681546, train_accuracy:0.33882319819819817, time_count:0.19757890701293945\n",
      "batch_120: train_l:1.731704634083204, train_accuracy:0.36383006198347106, time_count:0.19775104522705078\n",
      "batch_130: train_l:1.668040908929956, train_accuracy:0.3857347328244275, time_count:0.19788122177124023\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_3428/1301451966.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[0mlr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_epochs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0.1\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 29\u001B[0;31m \u001B[0mtrain_ch6_gpu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnet\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtrain_iter\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtest_iter\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mnum_epochs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mlr\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'cuda:0'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/Programs/DiveIntoDeepLearning/d2l.py\u001B[0m in \u001B[0;36mtrain_ch6_gpu\u001B[0;34m(net, train_iter, test_iter, num_epochs, lr, device)\u001B[0m\n\u001B[1;32m    137\u001B[0m             \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    138\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mno_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 139\u001B[0;31m                 \u001B[0mmetric\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ml\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maccuracy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my_hat\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    140\u001B[0m                 \u001B[0;31m# print(f'L:{l * X.shape[0]}, accuracy:{accuracy(y_hat,y)}, X.shape[0]:{X.shape[0]}')\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    141\u001B[0m             \u001B[0mend_time\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Programs/DiveIntoDeepLearning/d2l.py\u001B[0m in \u001B[0;36maccuracy\u001B[0;34m(y_hat, y)\u001B[0m\n\u001B[1;32m     73\u001B[0m         \u001B[0mcmp\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0my_hat\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0my\u001B[0m  \u001B[0;31m# a right index list\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     74\u001B[0m         \u001B[0;31m# it may out a vector like [f,t,t,t,t,f,f,t]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 75\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mfloat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcmp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     76\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     77\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# AlexNet\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import load_data_fashion_mnist, train_ch6_gpu\n",
    "\n",
    "# input size: 1*224*224\n",
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(),  # (224-11+2)/4=53+1=54 -> 96x54x54\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),  # (54-3)/2=25+1=26 -> 96*26*26\n",
    "    nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(),  # 256*26*26\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),  # (26-3)/2=11+1=12 -> 256*12*12\n",
    "    nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(),\n",
    "    nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(),\n",
    "    nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(),  # ??? why\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),  # (12-3)/2=4+1=5 -> 256*5*5\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(256 * 5 * 5, 4096), nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(4096, 4096), nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(4096, 10)\n",
    ")\n",
    "X = torch.randn(1, 1, 224, 224)\n",
    "print(net(X).shape)\n",
    "\n",
    "batch_size = 128\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=224)\n",
    "\n",
    "lr, num_epochs = 0.1, 10\n",
    "train_ch6_gpu(net, train_iter, test_iter, num_epochs, lr, 'cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net will be trained on cuda:0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 3.82 GiB total capacity; 2.57 GiB already allocated; 29.44 MiB free; 2.58 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_4459/2999745603.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     46\u001B[0m \u001B[0mlr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_epochs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_size\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0.05\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m10\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m128\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[0mtrain_iter\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_iter\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mload_data_fashion_mnist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresize\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m224\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m \u001B[0mtrain_ch6_gpu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnet\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_iter\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_iter\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_epochs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'cuda:0'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/Programs/DiveIntoDeepLearning/d2l.py\u001B[0m in \u001B[0;36mtrain_ch6_gpu\u001B[0;34m(net, train_iter, test_iter, num_epochs, lr, device)\u001B[0m\n\u001B[1;32m    134\u001B[0m             \u001B[0my_hat\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnet\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    135\u001B[0m             \u001B[0ml\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mloss\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my_hat\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 136\u001B[0;31m             \u001B[0ml\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    137\u001B[0m             \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    138\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mno_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Applications/YOLOX/demo/MegEngine/PVMegengine/lib/python3.8/site-packages/torch/tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    243\u001B[0m                 \u001B[0mcreate_graph\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    244\u001B[0m                 inputs=inputs)\n\u001B[0;32m--> 245\u001B[0;31m         \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    246\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    247\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Applications/YOLOX/demo/MegEngine/PVMegengine/lib/python3.8/site-packages/torch/autograd/__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    143\u001B[0m         \u001B[0mretain_graph\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    144\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 145\u001B[0;31m     Variable._execution_engine.run_backward(\n\u001B[0m\u001B[1;32m    146\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    147\u001B[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 3.82 GiB total capacity; 2.57 GiB already allocated; 29.44 MiB free; 2.58 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# VGG Block\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def vgg_block(num_convs, in_channels, out_channels):\n",
    "    \"\"\"\n",
    "    create a vgg block, which has a serial of conv2d layer\n",
    "    :param num_convs:\n",
    "    :param in_channels:\n",
    "    :param out_channels:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    for _ in range(num_convs):\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        in_channels = out_channels\n",
    "    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))\n",
    "\n",
    "\n",
    "# VGG net\n",
    "def vgg(conv_arch):\n",
    "    conv_blks = []\n",
    "    in_channels = 1\n",
    "    for (num_convs, out_channels) in conv_arch:\n",
    "        conv_blks.append(vgg_block(num_convs, in_channels=in_channels, out_channels=out_channels))\n",
    "        in_channels = out_channels\n",
    "    return nn.Sequential(\n",
    "        *conv_blks,\n",
    "        nn.Flatten(),\n",
    "        # full connection layers\n",
    "        nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(p=0.5),\n",
    "        nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "        nn.Linear(4096, 10)\n",
    "    )\n",
    "\n",
    "\n",
    "net = vgg(conv_arch)  # VGG-11 net\n",
    "\n",
    "ratio = 4\n",
    "small_conv_arch = [(pair[0], pair[1] // ratio) for pair in conv_arch]\n",
    "net = vgg(small_conv_arch)\n",
    "\n",
    "from d2l import load_data_fashion_mnist, train_ch6_gpu\n",
    "\n",
    "lr, num_epochs, batch_size = 0.05, 10, 128\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=224)\n",
    "train_ch6_gpu(net, train_iter, test_iter, num_epochs, lr, 'cuda:0')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net will be trained on cuda:0\n",
      "batch_0: train_l:2.3009300231933594, train_accuracy:0.078125, time_count:0.2532370090484619\n",
      "batch_10: train_l:2.301686503670432, train_accuracy:0.10866477272727272, time_count:0.21374726295471191\n",
      "batch_20: train_l:2.306446154912313, train_accuracy:0.09933035714285714, time_count:0.21476244926452637\n",
      "batch_30: train_l:2.3069870318135908, train_accuracy:0.09828629032258064, time_count:0.21554088592529297\n",
      "batch_40: train_l:2.305805421457058, train_accuracy:0.10041920731707317, time_count:0.21606230735778809\n",
      "batch_50: train_l:2.3062391982359043, train_accuracy:0.10033700980392157, time_count:0.2150895595550537\n",
      "batch_60: train_l:2.306294671824721, train_accuracy:0.10053790983606557, time_count:0.21730446815490723\n",
      "batch_70: train_l:2.306099102530681, train_accuracy:0.10453345070422536, time_count:0.21565961837768555\n",
      "batch_80: train_l:2.3061735924379327, train_accuracy:0.10387731481481481, time_count:0.21639060974121094\n",
      "batch_90: train_l:2.305754006563962, train_accuracy:0.1043956043956044, time_count:0.21703672409057617\n",
      "batch_100: train_l:2.30517044397864, train_accuracy:0.1047339108910891, time_count:0.21693015098571777\n",
      "batch_110: train_l:2.304700211361722, train_accuracy:0.10564470720720721, time_count:0.21658682823181152\n",
      "batch_120: train_l:2.303859249619413, train_accuracy:0.10769628099173553, time_count:0.2213292121887207\n",
      "batch_130: train_l:2.302847525545659, train_accuracy:0.11134303435114504, time_count:0.21982049942016602\n",
      "batch_140: train_l:2.3016203362891017, train_accuracy:0.11308732269503546, time_count:0.22721529006958008\n",
      "batch_150: train_l:2.300021034202828, train_accuracy:0.11749793046357616, time_count:0.22766637802124023\n",
      "batch_160: train_l:2.2984456764244885, train_accuracy:0.12131211180124224, time_count:0.22497963905334473\n",
      "batch_170: train_l:2.29628933103461, train_accuracy:0.12495431286549707, time_count:0.22671055793762207\n",
      "batch_180: train_l:2.2937203138572735, train_accuracy:0.13026588397790057, time_count:0.22982144355773926\n",
      "batch_190: train_l:2.2911395115378017, train_accuracy:0.1352666884816754, time_count:0.23109197616577148\n",
      "batch_200: train_l:2.287064375568978, train_accuracy:0.14109141791044777, time_count:0.230987548828125\n",
      "batch_210: train_l:2.284196512394042, train_accuracy:0.14588270142180096, time_count:0.23132801055908203\n",
      "batch_220: train_l:2.2804440351632924, train_accuracy:0.15084134615384615, time_count:0.228987455368042\n",
      "batch_230: train_l:2.2821050598507835, train_accuracy:0.1493844696969697, time_count:0.22991037368774414\n",
      "batch_240: train_l:2.28070886600067, train_accuracy:0.15232754149377592, time_count:0.23015952110290527\n",
      "batch_250: train_l:2.2773234682729044, train_accuracy:0.15659238047808766, time_count:0.2329869270324707\n",
      "batch_260: train_l:2.272617735625227, train_accuracy:0.1615780651340996, time_count:0.23053979873657227\n",
      "batch_270: train_l:2.264565711531692, train_accuracy:0.1685309040590406, time_count:0.23177838325500488\n",
      "batch_280: train_l:2.265599563876929, train_accuracy:0.16995662811387902, time_count:0.2319786548614502\n",
      "batch_290: train_l:2.262774568243125, train_accuracy:0.17375429553264604, time_count:0.2309885025024414\n",
      "batch_300: train_l:2.257705208471051, train_accuracy:0.1782859219269103, time_count:0.23100638389587402\n",
      "batch_310: train_l:2.2527256379940117, train_accuracy:0.18287781350482316, time_count:0.23091363906860352\n",
      "batch_320: train_l:2.248667532038466, train_accuracy:0.18616140965732086, time_count:0.22986888885498047\n",
      "batch_330: train_l:2.2433833506892453, train_accuracy:0.1891049848942598, time_count:0.2300097942352295\n",
      "batch_340: train_l:2.2356307503065405, train_accuracy:0.19405241935483872, time_count:0.23181557655334473\n",
      "batch_350: train_l:2.2314063575532703, train_accuracy:0.1964699074074074, time_count:0.2300560474395752\n",
      "batch_360: train_l:2.226800506134773, train_accuracy:0.1983422783933518, time_count:0.2309432029724121\n",
      "batch_370: train_l:2.21946855536047, train_accuracy:0.2012508423180593, time_count:0.2301490306854248\n",
      "batch_380: train_l:2.2133650520029344, train_accuracy:0.20509350393700787, time_count:0.23104429244995117\n",
      "batch_390: train_l:2.211859301228048, train_accuracy:0.20686141304347827, time_count:0.230086088180542\n",
      "batch_400: train_l:2.2060283184646075, train_accuracy:0.209223036159601, time_count:0.23145532608032227\n",
      "batch_410: train_l:2.197343531026167, train_accuracy:0.21251520681265207, time_count:0.23029422760009766\n",
      "batch_420: train_l:2.1885058593863262, train_accuracy:0.21609634798099764, time_count:0.23005294799804688\n",
      "batch_430: train_l:2.1841243701855158, train_accuracy:0.21737238979118328, time_count:0.23168015480041504\n",
      "batch_440: train_l:2.1780475961918735, train_accuracy:0.21937003968253968, time_count:0.230149507522583\n",
      "batch_450: train_l:2.1707346957432985, train_accuracy:0.2220759423503326, time_count:0.23026204109191895\n",
      "batch_460: train_l:2.162977513160209, train_accuracy:0.22508812364425163, time_count:0.22929954528808594\n",
      "================================\n",
      "epoch_0: test_acc:0.3461\n",
      "================================\n",
      "batch_0: train_l:1.6912705898284912, train_accuracy:0.3984375, time_count:0.23537778854370117\n",
      "batch_10: train_l:1.7164814797314731, train_accuracy:0.3913352272727273, time_count:0.2308361530303955\n",
      "batch_20: train_l:1.749281258810134, train_accuracy:0.38244047619047616, time_count:0.2300245761871338\n",
      "batch_30: train_l:1.745583415031433, train_accuracy:0.39314516129032256, time_count:0.22999262809753418\n",
      "batch_40: train_l:1.744300141567137, train_accuracy:0.4018673780487805, time_count:0.23135733604431152\n",
      "batch_50: train_l:1.731435731345532, train_accuracy:0.41161151960784315, time_count:0.23146295547485352\n",
      "batch_60: train_l:1.7536501278642749, train_accuracy:0.4042008196721312, time_count:0.2304244041442871\n",
      "batch_70: train_l:1.7924177529106677, train_accuracy:0.39018485915492956, time_count:0.23005366325378418\n",
      "batch_80: train_l:1.7958232635333213, train_accuracy:0.3888888888888889, time_count:0.23008418083190918\n",
      "batch_90: train_l:1.782439234492543, train_accuracy:0.39783653846153844, time_count:0.23198962211608887\n",
      "batch_100: train_l:1.777191701501903, train_accuracy:0.4026918316831683, time_count:0.23569679260253906\n",
      "batch_110: train_l:1.7677399585912894, train_accuracy:0.40857263513513514, time_count:0.23023653030395508\n",
      "batch_120: train_l:1.76803452416885, train_accuracy:0.40973657024793386, time_count:0.2292158603668213\n",
      "batch_130: train_l:1.758295058294107, train_accuracy:0.4154341603053435, time_count:0.22913360595703125\n",
      "batch_140: train_l:1.7598520339803492, train_accuracy:0.4150598404255319, time_count:0.22916340827941895\n",
      "batch_150: train_l:1.7531167781905623, train_accuracy:0.419908940397351, time_count:0.2292342185974121\n",
      "batch_160: train_l:1.7423302994011352, train_accuracy:0.42410714285714285, time_count:0.22902154922485352\n",
      "batch_170: train_l:1.7388258391653586, train_accuracy:0.42594115497076024, time_count:0.22938799858093262\n",
      "batch_180: train_l:1.7302021071397138, train_accuracy:0.430939226519337, time_count:0.2295377254486084\n",
      "batch_190: train_l:1.7263746199183438, train_accuracy:0.43263252617801046, time_count:0.22919940948486328\n",
      "batch_200: train_l:1.7174365057874081, train_accuracy:0.43683924129353235, time_count:0.2289571762084961\n",
      "batch_210: train_l:1.7080939482738622, train_accuracy:0.440573163507109, time_count:0.2292485237121582\n",
      "batch_220: train_l:1.699810779472282, train_accuracy:0.4443580316742081, time_count:0.22967910766601562\n",
      "batch_230: train_l:1.696277337156849, train_accuracy:0.44636093073593075, time_count:0.22941827774047852\n",
      "batch_240: train_l:1.6920354039837215, train_accuracy:0.4487811203319502, time_count:0.22918057441711426\n",
      "batch_250: train_l:1.6855450954095301, train_accuracy:0.45103959163346613, time_count:0.2292320728302002\n",
      "batch_260: train_l:1.6770782356517981, train_accuracy:0.45381345785440613, time_count:0.22922015190124512\n",
      "batch_270: train_l:1.6674815609886198, train_accuracy:0.4574492619926199, time_count:0.22946977615356445\n",
      "batch_280: train_l:1.6617954621535602, train_accuracy:0.45940836298932386, time_count:0.22913742065429688\n",
      "batch_290: train_l:1.6595742321505988, train_accuracy:0.4604542525773196, time_count:0.22930073738098145\n",
      "batch_300: train_l:1.6525574328495418, train_accuracy:0.46291009136212624, time_count:0.22939825057983398\n",
      "batch_310: train_l:1.6442798425913623, train_accuracy:0.46606209807073956, time_count:0.22913789749145508\n",
      "batch_320: train_l:1.6362374671894442, train_accuracy:0.4692367601246106, time_count:0.2290658950805664\n",
      "batch_330: train_l:1.630594379232009, train_accuracy:0.4712518882175227, time_count:0.2296767234802246\n",
      "batch_340: train_l:1.6225403459890146, train_accuracy:0.47420271260997066, time_count:0.23020434379577637\n",
      "batch_350: train_l:1.614388862566391, train_accuracy:0.47740829772079774, time_count:0.22914552688598633\n",
      "batch_360: train_l:1.6064552223913557, train_accuracy:0.48004674515235457, time_count:0.22910118103027344\n",
      "batch_370: train_l:1.5974112279974226, train_accuracy:0.4837222035040431, time_count:0.22916197776794434\n",
      "batch_380: train_l:1.5913141740588692, train_accuracy:0.48589238845144356, time_count:0.2293107509613037\n",
      "batch_390: train_l:1.5834579327527214, train_accuracy:0.48881074168797956, time_count:0.22935009002685547\n",
      "batch_400: train_l:1.5797287439764884, train_accuracy:0.4903561408977556, time_count:0.22926712036132812\n",
      "batch_410: train_l:1.5736927823718736, train_accuracy:0.4926817214111922, time_count:0.22945308685302734\n",
      "batch_420: train_l:1.5679962351882826, train_accuracy:0.49472980997624705, time_count:0.2296772003173828\n",
      "batch_430: train_l:1.5606553496451498, train_accuracy:0.4967553654292343, time_count:0.22918224334716797\n",
      "batch_440: train_l:1.5570073025026558, train_accuracy:0.4979095804988662, time_count:0.22907423973083496\n",
      "batch_450: train_l:1.5509469966402074, train_accuracy:0.5002598392461197, time_count:0.22915983200073242\n",
      "batch_460: train_l:1.548048819740527, train_accuracy:0.5014574295010846, time_count:0.2298591136932373\n",
      "================================\n",
      "epoch_1: test_acc:0.6135\n",
      "================================\n",
      "batch_0: train_l:1.2674307823181152, train_accuracy:0.578125, time_count:0.2392282485961914\n",
      "batch_10: train_l:1.2494459368965842, train_accuracy:0.6257102272727273, time_count:0.22921323776245117\n",
      "batch_20: train_l:1.221182005746024, train_accuracy:0.6231398809523809, time_count:0.2297370433807373\n",
      "batch_30: train_l:1.2782037642694288, train_accuracy:0.6003024193548387, time_count:0.2291717529296875\n",
      "batch_40: train_l:1.286225231682382, train_accuracy:0.5981326219512195, time_count:0.22999787330627441\n",
      "batch_50: train_l:1.2826013074201696, train_accuracy:0.5992647058823529, time_count:0.2293541431427002\n",
      "batch_60: train_l:1.2715804107853623, train_accuracy:0.6015625, time_count:0.23204469680786133\n",
      "batch_70: train_l:1.268051714964316, train_accuracy:0.6007922535211268, time_count:0.22954726219177246\n",
      "batch_80: train_l:1.266113434308841, train_accuracy:0.6022376543209876, time_count:0.22937679290771484\n",
      "batch_90: train_l:1.2612400343129924, train_accuracy:0.6023351648351648, time_count:0.23044085502624512\n",
      "batch_100: train_l:1.2553262899417688, train_accuracy:0.6047339108910891, time_count:0.23076295852661133\n",
      "batch_110: train_l:1.2490782769950661, train_accuracy:0.6071227477477478, time_count:0.2305600643157959\n",
      "batch_120: train_l:1.242441700509757, train_accuracy:0.6078899793388429, time_count:0.23000407218933105\n",
      "batch_130: train_l:1.2358359253133526, train_accuracy:0.6101502862595419, time_count:0.23075175285339355\n",
      "batch_140: train_l:1.2385634352974857, train_accuracy:0.6101507092198581, time_count:0.23003125190734863\n",
      "batch_150: train_l:1.2334608258001063, train_accuracy:0.6123758278145696, time_count:0.2300395965576172\n",
      "batch_160: train_l:1.2320338432092843, train_accuracy:0.6133540372670807, time_count:0.22999358177185059\n",
      "batch_170: train_l:1.2331846677071867, train_accuracy:0.6127101608187134, time_count:0.23029541969299316\n",
      "batch_180: train_l:1.2332456431336165, train_accuracy:0.6126985497237569, time_count:0.23038625717163086\n",
      "batch_190: train_l:1.2302840171060012, train_accuracy:0.6129744764397905, time_count:0.23028302192687988\n",
      "batch_200: train_l:1.2297951318138274, train_accuracy:0.6123289800995025, time_count:0.2301015853881836\n",
      "batch_210: train_l:1.2301885446101004, train_accuracy:0.6118557464454977, time_count:0.2300586700439453\n",
      "batch_220: train_l:1.2265207985947035, train_accuracy:0.6130161199095022, time_count:0.23015832901000977\n",
      "batch_230: train_l:1.2223265103963545, train_accuracy:0.6138054653679653, time_count:0.23013877868652344\n",
      "batch_240: train_l:1.2156154201733127, train_accuracy:0.6154369813278008, time_count:0.22994327545166016\n",
      "batch_250: train_l:1.212756228161998, train_accuracy:0.6158802290836654, time_count:0.23009800910949707\n",
      "batch_260: train_l:1.2175801339277363, train_accuracy:0.6139547413793104, time_count:0.23035192489624023\n",
      "batch_270: train_l:1.2157012041644417, train_accuracy:0.6142470018450185, time_count:0.23001313209533691\n",
      "batch_280: train_l:1.2128526245996196, train_accuracy:0.6154915480427047, time_count:0.22995829582214355\n",
      "batch_290: train_l:1.2116286826297589, train_accuracy:0.6157914518900344, time_count:0.23033547401428223\n",
      "batch_300: train_l:1.2083704580500276, train_accuracy:0.6164088455149501, time_count:0.23029136657714844\n",
      "batch_310: train_l:1.2064149975776672, train_accuracy:0.6164087620578779, time_count:0.23039698600769043\n",
      "batch_320: train_l:1.2034828672156528, train_accuracy:0.6171144859813084, time_count:0.23016047477722168\n",
      "batch_330: train_l:1.2001097297740486, train_accuracy:0.6180135951661632, time_count:0.23026061058044434\n",
      "batch_340: train_l:1.1981622716548506, train_accuracy:0.6184934017595308, time_count:0.23031187057495117\n",
      "batch_350: train_l:1.200234074368436, train_accuracy:0.6176771723646723, time_count:0.23010993003845215\n",
      "batch_360: train_l:1.1991633289078265, train_accuracy:0.6173822714681441, time_count:0.2304985523223877\n",
      "batch_370: train_l:1.1981849753953056, train_accuracy:0.6173559636118598, time_count:0.230271577835083\n",
      "batch_380: train_l:1.1979467397912593, train_accuracy:0.6171669947506562, time_count:0.22992992401123047\n",
      "batch_390: train_l:1.1950228985618143, train_accuracy:0.6179667519181585, time_count:0.23021268844604492\n",
      "batch_400: train_l:1.191954281116067, train_accuracy:0.6184928304239401, time_count:0.23012948036193848\n",
      "batch_410: train_l:1.190762027916827, train_accuracy:0.618518096107056, time_count:0.23010730743408203\n",
      "batch_420: train_l:1.1898714669243458, train_accuracy:0.6184308194774347, time_count:0.23059892654418945\n",
      "batch_430: train_l:1.187663770883653, train_accuracy:0.6187463747099768, time_count:0.2300872802734375\n",
      "batch_440: train_l:1.1863526668137705, train_accuracy:0.6189413265306123, time_count:0.23126721382141113\n",
      "batch_450: train_l:1.1856693608269195, train_accuracy:0.6189370842572062, time_count:0.23000240325927734\n",
      "batch_460: train_l:1.1847184723214834, train_accuracy:0.6191024945770065, time_count:0.2314310073852539\n",
      "================================\n",
      "epoch_2: test_acc:0.6349\n",
      "================================\n",
      "batch_0: train_l:1.184826374053955, train_accuracy:0.578125, time_count:0.23651766777038574\n",
      "batch_10: train_l:1.0665028799663892, train_accuracy:0.6498579545454546, time_count:0.22908663749694824\n",
      "batch_20: train_l:1.1190037443524314, train_accuracy:0.6313244047619048, time_count:0.22918462753295898\n",
      "batch_30: train_l:1.1140097379684448, train_accuracy:0.6300403225806451, time_count:0.22887921333312988\n",
      "batch_40: train_l:1.094179284281847, train_accuracy:0.6375762195121951, time_count:0.22913670539855957\n",
      "batch_50: train_l:1.0927105651182287, train_accuracy:0.6412377450980392, time_count:0.22899150848388672\n",
      "batch_60: train_l:1.0882381200790405, train_accuracy:0.6413934426229508, time_count:0.22920489311218262\n",
      "batch_70: train_l:1.0915838454810667, train_accuracy:0.6397447183098591, time_count:0.2292938232421875\n",
      "batch_80: train_l:1.0904000143945953, train_accuracy:0.6427469135802469, time_count:0.22905874252319336\n",
      "batch_90: train_l:1.0889660702956903, train_accuracy:0.6440590659340659, time_count:0.22915410995483398\n",
      "batch_100: train_l:1.1069275241086978, train_accuracy:0.6370668316831684, time_count:0.23025131225585938\n",
      "batch_110: train_l:1.1109306205500353, train_accuracy:0.6355574324324325, time_count:0.22879791259765625\n",
      "batch_120: train_l:1.1109346451838154, train_accuracy:0.6352660123966942, time_count:0.22947239875793457\n",
      "batch_130: train_l:1.1127071899312142, train_accuracy:0.6356750954198473, time_count:0.2299056053161621\n",
      "batch_140: train_l:1.1149193906614967, train_accuracy:0.6340314716312057, time_count:0.22956061363220215\n",
      "batch_150: train_l:1.1125453671082755, train_accuracy:0.6351407284768212, time_count:0.2290799617767334\n",
      "batch_160: train_l:1.1146979909505903, train_accuracy:0.6340256211180124, time_count:0.22992825508117676\n",
      "batch_170: train_l:1.112708073959016, train_accuracy:0.6348684210526315, time_count:0.22939705848693848\n",
      "batch_180: train_l:1.1111115536637068, train_accuracy:0.6356180939226519, time_count:0.2293412685394287\n",
      "batch_190: train_l:1.10859957415396, train_accuracy:0.6365755890052356, time_count:0.2299206256866455\n",
      "batch_200: train_l:1.1052867191940992, train_accuracy:0.6378264925373134, time_count:0.23021841049194336\n",
      "batch_210: train_l:1.1022455875907464, train_accuracy:0.6388477488151659, time_count:0.23053312301635742\n",
      "batch_220: train_l:1.1031584777443657, train_accuracy:0.6391756221719457, time_count:0.23140525817871094\n",
      "batch_230: train_l:1.10482849212952, train_accuracy:0.6382913961038961, time_count:0.23046350479125977\n",
      "batch_240: train_l:1.1026034582699977, train_accuracy:0.6385503112033195, time_count:0.23120808601379395\n",
      "batch_250: train_l:1.1015865484556828, train_accuracy:0.6385084661354582, time_count:0.2302548885345459\n",
      "batch_260: train_l:1.1008519443515616, train_accuracy:0.6387092911877394, time_count:0.23032283782958984\n",
      "batch_270: train_l:1.0991139095207862, train_accuracy:0.6394430350553506, time_count:0.23038744926452637\n",
      "batch_280: train_l:1.097828738417914, train_accuracy:0.6397075177935944, time_count:0.23048996925354004\n",
      "batch_290: train_l:1.096954598869245, train_accuracy:0.6397927405498282, time_count:0.23129010200500488\n",
      "batch_300: train_l:1.0968405804364785, train_accuracy:0.6400280315614618, time_count:0.23226404190063477\n",
      "batch_310: train_l:1.0951541861537186, train_accuracy:0.6401477090032154, time_count:0.23043489456176758\n",
      "batch_320: train_l:1.0943515763104519, train_accuracy:0.6404789719626168, time_count:0.2306835651397705\n",
      "batch_330: train_l:1.092865965877775, train_accuracy:0.6404361782477341, time_count:0.2302112579345703\n",
      "batch_340: train_l:1.0912158478739673, train_accuracy:0.6405562683284457, time_count:0.23028063774108887\n",
      "batch_350: train_l:1.0925046140991386, train_accuracy:0.6400685541310541, time_count:0.2302713394165039\n",
      "batch_360: train_l:1.093019446837935, train_accuracy:0.6398242728531855, time_count:0.23118305206298828\n",
      "batch_370: train_l:1.0916251461460906, train_accuracy:0.6402670148247979, time_count:0.23031949996948242\n",
      "batch_380: train_l:1.090922969845649, train_accuracy:0.6402354002624672, time_count:0.23151826858520508\n",
      "batch_390: train_l:1.0918795425263816, train_accuracy:0.6401454603580563, time_count:0.23059964179992676\n",
      "batch_400: train_l:1.0927819102779588, train_accuracy:0.6397288029925187, time_count:0.2328641414642334\n",
      "batch_410: train_l:1.0912493486183983, train_accuracy:0.6400927615571776, time_count:0.23355722427368164\n",
      "batch_420: train_l:1.0901206314139016, train_accuracy:0.6403280878859857, time_count:0.230363130569458\n",
      "batch_430: train_l:1.0888318368287762, train_accuracy:0.640606873549884, time_count:0.2315521240234375\n",
      "batch_440: train_l:1.0884207472238951, train_accuracy:0.6408553004535147, time_count:0.23041248321533203\n",
      "batch_450: train_l:1.0879408994693713, train_accuracy:0.6409714523281597, time_count:0.231367826461792\n",
      "batch_460: train_l:1.0886041060445624, train_accuracy:0.6405911062906724, time_count:0.2313835620880127\n",
      "================================\n",
      "epoch_3: test_acc:0.6447\n",
      "================================\n",
      "batch_0: train_l:1.00397789478302, train_accuracy:0.6640625, time_count:0.23816227912902832\n",
      "batch_10: train_l:1.0227413394234397, train_accuracy:0.6661931818181818, time_count:0.2317371368408203\n",
      "batch_20: train_l:1.0390731522015162, train_accuracy:0.6532738095238095, time_count:0.22947025299072266\n",
      "batch_30: train_l:1.0375680615825038, train_accuracy:0.6532258064516129, time_count:0.23158025741577148\n",
      "batch_40: train_l:1.0489737071642062, train_accuracy:0.6469131097560976, time_count:0.23108839988708496\n",
      "batch_50: train_l:1.047887030769797, train_accuracy:0.6470588235294118, time_count:0.2331531047821045\n",
      "batch_60: train_l:1.0420218256653333, train_accuracy:0.6513831967213115, time_count:0.23157215118408203\n",
      "batch_70: train_l:1.0421000282529373, train_accuracy:0.6504181338028169, time_count:0.23090720176696777\n",
      "batch_80: train_l:1.0519849496123233, train_accuracy:0.6486304012345679, time_count:0.23389601707458496\n",
      "batch_90: train_l:1.0525710477933778, train_accuracy:0.6489526098901099, time_count:0.23043060302734375\n",
      "batch_100: train_l:1.0523643540863943, train_accuracy:0.6492110148514851, time_count:0.2344818115234375\n",
      "batch_110: train_l:1.057555844654908, train_accuracy:0.6471706081081081, time_count:0.23525047302246094\n",
      "batch_120: train_l:1.0567603820611622, train_accuracy:0.6475981404958677, time_count:0.23339128494262695\n",
      "batch_130: train_l:1.0568297091331191, train_accuracy:0.6472447519083969, time_count:0.23406696319580078\n",
      "batch_140: train_l:1.0638554476677102, train_accuracy:0.644281914893617, time_count:0.2317202091217041\n",
      "batch_150: train_l:1.067838062513743, train_accuracy:0.6427980132450332, time_count:0.25228118896484375\n",
      "batch_160: train_l:1.0670106170340354, train_accuracy:0.6437791149068323, time_count:0.2366623878479004\n",
      "batch_170: train_l:1.065592817047186, train_accuracy:0.6440972222222222, time_count:0.23060202598571777\n",
      "batch_180: train_l:1.0652521084685351, train_accuracy:0.6446823204419889, time_count:0.23226118087768555\n",
      "batch_190: train_l:1.0623300733990695, train_accuracy:0.6459015052356021, time_count:0.23723173141479492\n",
      "batch_200: train_l:1.0613715980776506, train_accuracy:0.6458722014925373, time_count:0.2359013557434082\n",
      "batch_210: train_l:1.0619722822266167, train_accuracy:0.6452532582938388, time_count:0.24884486198425293\n",
      "batch_220: train_l:1.0587132063386666, train_accuracy:0.6456447963800905, time_count:0.23308682441711426\n",
      "batch_230: train_l:1.0576683891800058, train_accuracy:0.6454951298701299, time_count:0.2323470115661621\n",
      "batch_240: train_l:1.0582748992809121, train_accuracy:0.6450661307053942, time_count:0.23288536071777344\n",
      "batch_250: train_l:1.0577391834847956, train_accuracy:0.6452626992031872, time_count:0.23249268531799316\n",
      "batch_260: train_l:1.056816202470626, train_accuracy:0.6452646072796935, time_count:0.23151421546936035\n",
      "batch_270: train_l:1.0559889737970274, train_accuracy:0.6452952029520295, time_count:0.23566579818725586\n",
      "batch_280: train_l:1.054957370316855, train_accuracy:0.6458796708185054, time_count:0.23060822486877441\n",
      "batch_290: train_l:1.0539045819302195, train_accuracy:0.6459138745704467, time_count:0.23053455352783203\n",
      "batch_300: train_l:1.0537206251359856, train_accuracy:0.6456343438538206, time_count:0.23328542709350586\n",
      "batch_310: train_l:1.053531595748337, train_accuracy:0.6457998392282959, time_count:0.23285627365112305\n",
      "batch_320: train_l:1.0518451637568131, train_accuracy:0.6465147975077882, time_count:0.23221635818481445\n",
      "batch_330: train_l:1.0536002848083519, train_accuracy:0.6461480362537765, time_count:0.23331308364868164\n",
      "batch_340: train_l:1.0544643000074145, train_accuracy:0.6457111436950147, time_count:0.232680082321167\n",
      "batch_350: train_l:1.0542905386696515, train_accuracy:0.6458555911680912, time_count:0.23222088813781738\n",
      "batch_360: train_l:1.0538292788072305, train_accuracy:0.6463815789473685, time_count:0.2320995330810547\n",
      "batch_370: train_l:1.0539707316542573, train_accuracy:0.6463527628032345, time_count:0.24074029922485352\n",
      "batch_380: train_l:1.0545891509281369, train_accuracy:0.6460793963254593, time_count:0.23258495330810547\n",
      "batch_390: train_l:1.0540784004399233, train_accuracy:0.6460398017902813, time_count:0.2343897819519043\n",
      "batch_400: train_l:1.0545902705549302, train_accuracy:0.6456904613466334, time_count:0.23560380935668945\n",
      "batch_410: train_l:1.05393186755424, train_accuracy:0.6460614355231143, time_count:0.23696041107177734\n",
      "batch_420: train_l:1.05326394216465, train_accuracy:0.6463034441805225, time_count:0.23619604110717773\n",
      "batch_430: train_l:1.0528349908211392, train_accuracy:0.646479843387471, time_count:0.24727797508239746\n",
      "batch_440: train_l:1.052135343724638, train_accuracy:0.6468608276643991, time_count:0.2361612319946289\n",
      "batch_450: train_l:1.0521543711886439, train_accuracy:0.6470863359201774, time_count:0.23648524284362793\n",
      "batch_460: train_l:1.0506445076357, train_accuracy:0.6475223698481561, time_count:0.23820972442626953\n",
      "================================\n",
      "epoch_4: test_acc:0.6473\n",
      "================================\n",
      "batch_0: train_l:1.0311130285263062, train_accuracy:0.6171875, time_count:0.23635363578796387\n",
      "batch_10: train_l:1.0156464197418906, train_accuracy:0.6526988636363636, time_count:0.23519539833068848\n",
      "batch_20: train_l:1.025805584021977, train_accuracy:0.6443452380952381, time_count:0.24123263359069824\n",
      "batch_30: train_l:1.0157708583339569, train_accuracy:0.6484375, time_count:0.23223400115966797\n",
      "batch_40: train_l:1.0153342674418193, train_accuracy:0.6514862804878049, time_count:0.23779630661010742\n",
      "batch_50: train_l:1.0147730930178773, train_accuracy:0.6531862745098039, time_count:0.2378547191619873\n",
      "batch_60: train_l:1.0143674143025132, train_accuracy:0.6542008196721312, time_count:0.2376401424407959\n",
      "batch_70: train_l:1.022336739889333, train_accuracy:0.6509683098591549, time_count:0.23690295219421387\n",
      "batch_80: train_l:1.0177887037948325, train_accuracy:0.6534529320987654, time_count:0.23833703994750977\n",
      "batch_90: train_l:1.0242528869555547, train_accuracy:0.6496394230769231, time_count:0.23749494552612305\n",
      "batch_100: train_l:1.0247094967577717, train_accuracy:0.6492110148514851, time_count:0.23112177848815918\n",
      "batch_110: train_l:1.0267770999186747, train_accuracy:0.6479448198198198, time_count:0.23276257514953613\n",
      "batch_120: train_l:1.0251669263051562, train_accuracy:0.64869576446281, time_count:0.23482918739318848\n",
      "batch_130: train_l:1.0258424232024272, train_accuracy:0.6480796755725191, time_count:0.23699307441711426\n",
      "batch_140: train_l:1.0258036729291822, train_accuracy:0.649324024822695, time_count:0.23623371124267578\n",
      "batch_150: train_l:1.0254760727977121, train_accuracy:0.6498344370860927, time_count:0.23735737800598145\n",
      "batch_160: train_l:1.029011911857202, train_accuracy:0.648146350931677, time_count:0.2351086139678955\n",
      "batch_170: train_l:1.0291997487084907, train_accuracy:0.6482090643274854, time_count:0.23239922523498535\n",
      "batch_180: train_l:1.0277003576742352, train_accuracy:0.6486964779005525, time_count:0.2405087947845459\n",
      "batch_190: train_l:1.0297137448924998, train_accuracy:0.6486011125654451, time_count:0.24491024017333984\n",
      "batch_200: train_l:1.0293903626612764, train_accuracy:0.648554104477612, time_count:0.2332911491394043\n",
      "batch_210: train_l:1.0279342628203296, train_accuracy:0.6488818127962085, time_count:0.23244524002075195\n",
      "batch_220: train_l:1.0264695435088145, train_accuracy:0.6491798642533937, time_count:0.23433732986450195\n",
      "batch_230: train_l:1.0264351912391134, train_accuracy:0.6491815476190477, time_count:0.2326340675354004\n",
      "batch_240: train_l:1.0256395065438204, train_accuracy:0.649150674273859, time_count:0.2617354393005371\n",
      "batch_250: train_l:1.0237744576427565, train_accuracy:0.6502116533864541, time_count:0.23764801025390625\n",
      "batch_260: train_l:1.0214659953026022, train_accuracy:0.6514008620689655, time_count:0.23114418983459473\n",
      "batch_270: train_l:1.02144579165976, train_accuracy:0.6513203413284133, time_count:0.2321021556854248\n",
      "batch_280: train_l:1.0202979509092311, train_accuracy:0.6519684163701067, time_count:0.23720002174377441\n",
      "batch_290: train_l:1.020690421263377, train_accuracy:0.6520887027491409, time_count:0.23678874969482422\n",
      "batch_300: train_l:1.0188421339687714, train_accuracy:0.6528239202657807, time_count:0.23401641845703125\n",
      "batch_310: train_l:1.019505202003614, train_accuracy:0.652532154340836, time_count:0.2316291332244873\n",
      "batch_320: train_l:1.017692596370186, train_accuracy:0.6531834112149533, time_count:0.2359452247619629\n",
      "batch_330: train_l:1.0169023052083042, train_accuracy:0.653606495468278, time_count:0.23395967483520508\n",
      "batch_340: train_l:1.0147360800997602, train_accuracy:0.6538902126099707, time_count:0.231658935546875\n",
      "batch_350: train_l:1.0150557725857465, train_accuracy:0.6537348646723646, time_count:0.2362363338470459\n",
      "batch_360: train_l:1.0143474474177796, train_accuracy:0.654064231301939, time_count:0.24005436897277832\n",
      "batch_370: train_l:1.012159612461563, train_accuracy:0.6548180592991913, time_count:0.236555814743042\n",
      "batch_380: train_l:1.0125267827917586, train_accuracy:0.6548351377952756, time_count:0.23965740203857422\n",
      "batch_390: train_l:1.0106870740880747, train_accuracy:0.6552309782608695, time_count:0.23648905754089355\n",
      "batch_400: train_l:1.0114361212735163, train_accuracy:0.6551394950124688, time_count:0.2380530834197998\n",
      "batch_410: train_l:1.011597155219447, train_accuracy:0.6551094890510949, time_count:0.23697829246520996\n",
      "batch_420: train_l:1.0117147382251441, train_accuracy:0.6551365795724465, time_count:0.23799872398376465\n",
      "batch_430: train_l:1.0121717730973547, train_accuracy:0.6551624129930395, time_count:0.236968994140625\n",
      "batch_440: train_l:1.0111020103333488, train_accuracy:0.6553465136054422, time_count:0.24965953826904297\n",
      "batch_450: train_l:1.0088971536069646, train_accuracy:0.6561980321507761, time_count:0.23055243492126465\n",
      "batch_460: train_l:1.007658446429867, train_accuracy:0.6563855748373102, time_count:0.23930740356445312\n",
      "================================\n",
      "epoch_5: test_acc:0.656\n",
      "================================\n",
      "batch_0: train_l:0.9128092527389526, train_accuracy:0.671875, time_count:0.23547744750976562\n",
      "batch_10: train_l:0.9796124805103649, train_accuracy:0.6647727272727273, time_count:0.24061203002929688\n",
      "batch_20: train_l:0.9866791509446644, train_accuracy:0.6603422619047619, time_count:0.24562788009643555\n",
      "batch_30: train_l:0.9873026628648082, train_accuracy:0.6585181451612904, time_count:0.23510169982910156\n",
      "batch_40: train_l:0.9756488509294463, train_accuracy:0.6600609756097561, time_count:0.24004697799682617\n",
      "batch_50: train_l:0.979266554701562, train_accuracy:0.6590073529411765, time_count:0.23821735382080078\n",
      "batch_60: train_l:0.9806185950998401, train_accuracy:0.6594518442622951, time_count:0.23503923416137695\n",
      "batch_70: train_l:0.9814763648409239, train_accuracy:0.6592209507042254, time_count:0.23458290100097656\n",
      "batch_80: train_l:0.9848360161722443, train_accuracy:0.6578896604938271, time_count:0.2375640869140625\n",
      "batch_90: train_l:0.9913336863884559, train_accuracy:0.65625, time_count:0.23520398139953613\n",
      "batch_100: train_l:0.9879379632449387, train_accuracy:0.6567914603960396, time_count:0.2340223789215088\n",
      "batch_110: train_l:0.9886890311498899, train_accuracy:0.6561796171171171, time_count:0.2406151294708252\n",
      "batch_120: train_l:0.9833040961549302, train_accuracy:0.6579932851239669, time_count:0.2360515594482422\n",
      "batch_130: train_l:0.9835726081869984, train_accuracy:0.6575620229007634, time_count:0.24270844459533691\n",
      "batch_140: train_l:0.9818689260922425, train_accuracy:0.6582446808510638, time_count:0.2389392852783203\n",
      "batch_150: train_l:0.9778116112513258, train_accuracy:0.6589403973509934, time_count:0.24068808555603027\n",
      "batch_160: train_l:0.9769020924657028, train_accuracy:0.6583850931677019, time_count:0.23843979835510254\n",
      "batch_170: train_l:0.9789392554271988, train_accuracy:0.6581231725146199, time_count:0.2324061393737793\n",
      "batch_180: train_l:0.9779097655201485, train_accuracy:0.6584081491712708, time_count:0.23458194732666016\n",
      "batch_190: train_l:0.9767473926718947, train_accuracy:0.6588678010471204, time_count:0.23613929748535156\n",
      "batch_200: train_l:0.9781247653178314, train_accuracy:0.658271144278607, time_count:0.2346186637878418\n",
      "batch_210: train_l:0.9783723235695283, train_accuracy:0.6575829383886256, time_count:0.2309885025024414\n",
      "batch_220: train_l:0.979388970864844, train_accuracy:0.657381221719457, time_count:0.2328176498413086\n",
      "batch_230: train_l:0.975971629867306, train_accuracy:0.6584145021645021, time_count:0.23026442527770996\n",
      "batch_240: train_l:0.9742179547602705, train_accuracy:0.6585191908713693, time_count:0.2326798439025879\n",
      "batch_250: train_l:0.9756565834896498, train_accuracy:0.6578996513944223, time_count:0.2336585521697998\n",
      "batch_260: train_l:0.9736797859842293, train_accuracy:0.6588840996168582, time_count:0.23328280448913574\n",
      "batch_270: train_l:0.972280450412708, train_accuracy:0.6594787822878229, time_count:0.23859810829162598\n",
      "batch_280: train_l:0.971652769957573, train_accuracy:0.6595862989323843, time_count:0.2319953441619873\n",
      "batch_290: train_l:0.9711997363575545, train_accuracy:0.6599280498281787, time_count:0.23157453536987305\n",
      "batch_300: train_l:0.9701391696137843, train_accuracy:0.6603509136212624, time_count:0.2303485870361328\n",
      "batch_310: train_l:0.9692776785786129, train_accuracy:0.6603697749196141, time_count:0.2380235195159912\n",
      "batch_320: train_l:0.9680403996479474, train_accuracy:0.6608742211838006, time_count:0.2361311912536621\n",
      "batch_330: train_l:0.9662165272632034, train_accuracy:0.6616078172205438, time_count:0.2302706241607666\n",
      "batch_340: train_l:0.9647548163741215, train_accuracy:0.6619776392961877, time_count:0.23107051849365234\n",
      "batch_350: train_l:0.964050755371735, train_accuracy:0.6622818732193733, time_count:0.23555612564086914\n",
      "batch_360: train_l:0.9632181398095847, train_accuracy:0.6625908933518005, time_count:0.2337489128112793\n",
      "batch_370: train_l:0.9636020065960537, train_accuracy:0.662483153638814, time_count:0.23431611061096191\n",
      "batch_380: train_l:0.9646673039814305, train_accuracy:0.6621555118110236, time_count:0.23304533958435059\n",
      "batch_390: train_l:0.962062562060783, train_accuracy:0.6629835358056266, time_count:0.23456573486328125\n",
      "batch_400: train_l:0.9624182468935141, train_accuracy:0.6625233790523691, time_count:0.23366785049438477\n",
      "batch_410: train_l:0.9617345043052432, train_accuracy:0.6628649635036497, time_count:0.23319530487060547\n",
      "batch_420: train_l:0.9612226491869204, train_accuracy:0.6631160926365796, time_count:0.23469066619873047\n",
      "batch_430: train_l:0.9608246234619424, train_accuracy:0.6631924303944315, time_count:0.23697447776794434\n",
      "batch_440: train_l:0.9615323959834992, train_accuracy:0.6628932823129252, time_count:0.23041963577270508\n",
      "batch_450: train_l:0.9605486271386665, train_accuracy:0.6631963691796009, time_count:0.24803376197814941\n",
      "batch_460: train_l:0.9594256615431862, train_accuracy:0.663384625813449, time_count:0.23439908027648926\n",
      "================================\n",
      "epoch_6: test_acc:0.6743\n",
      "================================\n",
      "batch_0: train_l:0.9774616360664368, train_accuracy:0.6640625, time_count:0.23625993728637695\n",
      "batch_10: train_l:0.9423280520872637, train_accuracy:0.6669034090909091, time_count:0.23202228546142578\n",
      "batch_20: train_l:0.914340112890516, train_accuracy:0.6759672619047619, time_count:0.23131752014160156\n",
      "batch_30: train_l:0.9173809866751393, train_accuracy:0.6776713709677419, time_count:0.23160076141357422\n",
      "batch_40: train_l:0.9275159210693545, train_accuracy:0.6743521341463414, time_count:0.23288393020629883\n",
      "batch_50: train_l:0.9175956856970694, train_accuracy:0.6753982843137255, time_count:0.23337459564208984\n",
      "batch_60: train_l:0.9252624931882639, train_accuracy:0.6743084016393442, time_count:0.2342672348022461\n",
      "batch_70: train_l:0.928073225726544, train_accuracy:0.6726452464788732, time_count:0.23310136795043945\n",
      "batch_80: train_l:0.9304056601759828, train_accuracy:0.6727430555555556, time_count:0.23296833038330078\n",
      "batch_90: train_l:0.9266177121099535, train_accuracy:0.6729910714285714, time_count:0.2336726188659668\n",
      "batch_100: train_l:0.9289121291424969, train_accuracy:0.6723391089108911, time_count:0.23495125770568848\n",
      "batch_110: train_l:0.9283992035968883, train_accuracy:0.6722972972972973, time_count:0.231154203414917\n",
      "batch_120: train_l:0.9279983954981339, train_accuracy:0.6725852272727273, time_count:0.23456120491027832\n",
      "batch_130: train_l:0.9298995751461, train_accuracy:0.6719942748091603, time_count:0.23453426361083984\n",
      "batch_140: train_l:0.9291206479918027, train_accuracy:0.6730385638297872, time_count:0.2341766357421875\n",
      "batch_150: train_l:0.9304418216477957, train_accuracy:0.6727028145695364, time_count:0.23427104949951172\n",
      "batch_160: train_l:0.9310238253996239, train_accuracy:0.672360248447205, time_count:0.23468422889709473\n",
      "batch_170: train_l:0.9301103757835968, train_accuracy:0.6726059941520468, time_count:0.23460006713867188\n",
      "batch_180: train_l:0.9291824029954099, train_accuracy:0.6732993784530387, time_count:0.23530316352844238\n",
      "batch_190: train_l:0.9296640213871501, train_accuracy:0.6728566753926701, time_count:0.24376273155212402\n",
      "batch_200: train_l:0.9287823124311456, train_accuracy:0.6729633084577115, time_count:0.23160004615783691\n",
      "batch_210: train_l:0.9311100654127473, train_accuracy:0.6723193127962085, time_count:0.23507118225097656\n",
      "batch_220: train_l:0.930346923715928, train_accuracy:0.6718396493212669, time_count:0.24670100212097168\n",
      "batch_230: train_l:0.9338042398035784, train_accuracy:0.6705560064935064, time_count:0.23570609092712402\n",
      "batch_240: train_l:0.9323366526746156, train_accuracy:0.6718101659751037, time_count:0.23023295402526855\n",
      "batch_250: train_l:0.9320262062597084, train_accuracy:0.6719995019920318, time_count:0.23106122016906738\n",
      "batch_260: train_l:0.9322903674681068, train_accuracy:0.6725335249042146, time_count:0.2326798439025879\n",
      "batch_270: train_l:0.9313598422546668, train_accuracy:0.6727686808118081, time_count:0.232682466506958\n",
      "batch_280: train_l:0.9303094146514702, train_accuracy:0.6738211743772242, time_count:0.23260092735290527\n",
      "batch_290: train_l:0.9296948471429831, train_accuracy:0.6739422250859106, time_count:0.23284173011779785\n",
      "batch_300: train_l:0.9283910849165679, train_accuracy:0.6742369186046512, time_count:0.23379945755004883\n",
      "batch_310: train_l:0.9285907707030366, train_accuracy:0.6741860932475884, time_count:0.25888538360595703\n",
      "batch_320: train_l:0.9288286388477432, train_accuracy:0.6741871105919003, time_count:0.23043036460876465\n",
      "batch_330: train_l:0.9273283483398645, train_accuracy:0.6747309290030211, time_count:0.23217177391052246\n",
      "batch_340: train_l:0.9264171873369524, train_accuracy:0.6750595674486803, time_count:0.24339866638183594\n",
      "batch_350: train_l:0.925303346750743, train_accuracy:0.6756365740740741, time_count:0.23196005821228027\n",
      "batch_360: train_l:0.9241762691257402, train_accuracy:0.6761166897506925, time_count:0.23227858543395996\n",
      "batch_370: train_l:0.9241548765380427, train_accuracy:0.6761918800539084, time_count:0.2329270839691162\n",
      "batch_380: train_l:0.922381585507881, train_accuracy:0.6767757545931758, time_count:0.23309659957885742\n",
      "batch_390: train_l:0.9204509122597287, train_accuracy:0.6774496483375959, time_count:0.23259210586547852\n",
      "batch_400: train_l:0.920476545568119, train_accuracy:0.6772326995012469, time_count:0.2318730354309082\n",
      "batch_410: train_l:0.9196087986303362, train_accuracy:0.6779197080291971, time_count:0.232558012008667\n",
      "batch_420: train_l:0.9199708950774403, train_accuracy:0.6776647862232779, time_count:0.23215508460998535\n",
      "batch_430: train_l:0.9183979460368853, train_accuracy:0.6782736368909513, time_count:0.2327280044555664\n",
      "batch_440: train_l:0.9185656988972168, train_accuracy:0.6781816893424036, time_count:0.23240351676940918\n",
      "batch_450: train_l:0.9181199087007611, train_accuracy:0.6782843680709535, time_count:0.23260760307312012\n",
      "batch_460: train_l:0.9192438536766036, train_accuracy:0.6779419739696312, time_count:0.23219585418701172\n",
      "================================\n",
      "epoch_7: test_acc:0.6788\n",
      "================================\n",
      "batch_0: train_l:0.9615483283996582, train_accuracy:0.6484375, time_count:0.23922038078308105\n",
      "batch_10: train_l:0.9624302007935264, train_accuracy:0.65625, time_count:0.23175644874572754\n",
      "batch_20: train_l:0.9415197457586016, train_accuracy:0.6659226190476191, time_count:0.23269939422607422\n",
      "batch_30: train_l:0.9439485957545619, train_accuracy:0.6670866935483871, time_count:0.2381758689880371\n",
      "batch_40: train_l:0.9366525978576846, train_accuracy:0.6703506097560976, time_count:0.2320401668548584\n",
      "batch_50: train_l:0.9261437759679907, train_accuracy:0.6757046568627451, time_count:0.2308199405670166\n",
      "batch_60: train_l:0.9234769100048503, train_accuracy:0.6762295081967213, time_count:0.233656644821167\n",
      "batch_70: train_l:0.9313275243195009, train_accuracy:0.6742957746478874, time_count:0.2339305877685547\n",
      "batch_80: train_l:0.9307554091936276, train_accuracy:0.6747685185185185, time_count:0.23407506942749023\n",
      "batch_90: train_l:0.9259584035192218, train_accuracy:0.6767685439560439, time_count:0.23425054550170898\n",
      "batch_100: train_l:0.922372303976871, train_accuracy:0.6781404702970297, time_count:0.2342214584350586\n",
      "batch_110: train_l:0.9148308838809933, train_accuracy:0.6800394144144144, time_count:0.23528647422790527\n",
      "batch_120: train_l:0.9094629120235601, train_accuracy:0.6825284090909091, time_count:0.23447585105895996\n",
      "batch_130: train_l:0.9076827018315555, train_accuracy:0.6826097328244275, time_count:0.23366189002990723\n",
      "batch_140: train_l:0.9059683938398428, train_accuracy:0.6836214539007093, time_count:0.23484206199645996\n",
      "batch_150: train_l:0.9027211496372096, train_accuracy:0.6843956953642384, time_count:0.23396611213684082\n",
      "batch_160: train_l:0.8984523571055868, train_accuracy:0.6857531055900621, time_count:0.23442983627319336\n",
      "batch_170: train_l:0.898148246675904, train_accuracy:0.6866776315789473, time_count:0.23424339294433594\n",
      "batch_180: train_l:0.899406183490437, train_accuracy:0.6865504143646409, time_count:0.23412084579467773\n",
      "batch_190: train_l:0.8960912233871939, train_accuracy:0.68782722513089, time_count:0.2339153289794922\n",
      "batch_200: train_l:0.8949194162046138, train_accuracy:0.6881996268656716, time_count:0.23361754417419434\n",
      "batch_210: train_l:0.8948936179915875, train_accuracy:0.6884626777251185, time_count:0.23345279693603516\n",
      "batch_220: train_l:0.8952509679945346, train_accuracy:0.6882423642533937, time_count:0.23324847221374512\n",
      "batch_230: train_l:0.8955718456924736, train_accuracy:0.6882778679653679, time_count:0.2328486442565918\n",
      "batch_240: train_l:0.8952874353317799, train_accuracy:0.6883104253112033, time_count:0.23463916778564453\n",
      "batch_250: train_l:0.8923239128523139, train_accuracy:0.6887450199203188, time_count:0.23041844367980957\n",
      "batch_260: train_l:0.8896755624091488, train_accuracy:0.689385775862069, time_count:0.23257923126220703\n",
      "batch_270: train_l:0.8914233626474753, train_accuracy:0.6881630535055351, time_count:0.2335655689239502\n",
      "batch_280: train_l:0.8880850602723526, train_accuracy:0.6890569395017794, time_count:0.2322216033935547\n",
      "batch_290: train_l:0.8849513381207522, train_accuracy:0.6902115549828178, time_count:0.23761796951293945\n",
      "batch_300: train_l:0.8838639304883457, train_accuracy:0.690874169435216, time_count:0.24946093559265137\n",
      "batch_310: train_l:0.880441663732866, train_accuracy:0.6921724276527331, time_count:0.23137760162353516\n",
      "batch_320: train_l:0.8770416824617119, train_accuracy:0.6935358255451713, time_count:0.23141169548034668\n",
      "batch_330: train_l:0.8740941338308629, train_accuracy:0.6938727341389728, time_count:0.2317671775817871\n",
      "batch_340: train_l:0.8699674094177761, train_accuracy:0.694945931085044, time_count:0.23238778114318848\n",
      "batch_350: train_l:0.8667050781752649, train_accuracy:0.6958244301994302, time_count:0.23238658905029297\n",
      "batch_360: train_l:0.8640887096648071, train_accuracy:0.6962430747922438, time_count:0.23271965980529785\n",
      "batch_370: train_l:0.8605259157255332, train_accuracy:0.6972919474393531, time_count:0.23163628578186035\n",
      "batch_380: train_l:0.8595435725735241, train_accuracy:0.6978141404199475, time_count:0.2320423126220703\n",
      "batch_390: train_l:0.8566649810737356, train_accuracy:0.6987492007672634, time_count:0.23247814178466797\n",
      "batch_400: train_l:0.8531180071711838, train_accuracy:0.6997545199501247, time_count:0.23222661018371582\n",
      "batch_410: train_l:0.8493739631924316, train_accuracy:0.7007489355231143, time_count:0.23186874389648438\n",
      "batch_420: train_l:0.8459988709300261, train_accuracy:0.7018074524940617, time_count:0.2319049835205078\n",
      "batch_430: train_l:0.8423963721556342, train_accuracy:0.7028893561484919, time_count:0.23213648796081543\n",
      "batch_440: train_l:0.83947543289656, train_accuracy:0.7038159013605442, time_count:0.23193812370300293\n",
      "batch_450: train_l:0.8373677926158694, train_accuracy:0.7043375831485588, time_count:0.2318403720855713\n",
      "batch_460: train_l:0.8344023938809972, train_accuracy:0.7051755694143167, time_count:0.23144793510437012\n",
      "================================\n",
      "epoch_8: test_acc:0.7475\n",
      "================================\n",
      "batch_0: train_l:0.6708853840827942, train_accuracy:0.734375, time_count:0.23256731033325195\n",
      "batch_10: train_l:0.681717412038283, train_accuracy:0.7365056818181818, time_count:0.23081231117248535\n",
      "batch_20: train_l:0.703740554196494, train_accuracy:0.7340029761904762, time_count:0.23286938667297363\n",
      "batch_30: train_l:0.7164497317806366, train_accuracy:0.7310987903225806, time_count:0.23291444778442383\n",
      "batch_40: train_l:0.7083590263273658, train_accuracy:0.7351371951219512, time_count:0.2325878143310547\n",
      "batch_50: train_l:0.6934041579564413, train_accuracy:0.7383578431372549, time_count:0.23254036903381348\n",
      "batch_60: train_l:0.6934393917927977, train_accuracy:0.7398821721311475, time_count:0.23284387588500977\n",
      "batch_70: train_l:0.6819176132410345, train_accuracy:0.7448283450704225, time_count:0.23325514793395996\n",
      "batch_80: train_l:0.6807784353509362, train_accuracy:0.7432484567901234, time_count:0.23287224769592285\n",
      "batch_90: train_l:0.6804848113557794, train_accuracy:0.7438186813186813, time_count:0.23292064666748047\n",
      "batch_100: train_l:0.6867630266317046, train_accuracy:0.7432704207920792, time_count:0.23621821403503418\n",
      "batch_110: train_l:0.6827549566556742, train_accuracy:0.7454251126126126, time_count:0.23326706886291504\n",
      "batch_120: train_l:0.683078156768783, train_accuracy:0.745286673553719, time_count:0.23295044898986816\n",
      "batch_130: train_l:0.6833851726455543, train_accuracy:0.744930820610687, time_count:0.23220300674438477\n",
      "batch_140: train_l:0.6812987509348714, train_accuracy:0.7452349290780141, time_count:0.23349308967590332\n",
      "batch_150: train_l:0.6838905132369489, train_accuracy:0.7449296357615894, time_count:0.23285365104675293\n",
      "batch_160: train_l:0.6821691941770708, train_accuracy:0.7454871894409938, time_count:0.23312878608703613\n",
      "batch_170: train_l:0.6857121998106527, train_accuracy:0.7439236111111112, time_count:0.23277568817138672\n",
      "batch_180: train_l:0.6844403707520079, train_accuracy:0.7443024861878453, time_count:0.23190712928771973\n",
      "batch_190: train_l:0.6840831612417211, train_accuracy:0.7440281413612565, time_count:0.230926513671875\n",
      "batch_200: train_l:0.6806703989778585, train_accuracy:0.7451414800995025, time_count:0.2311699390411377\n",
      "batch_210: train_l:0.6826519952001164, train_accuracy:0.7446682464454977, time_count:0.23121070861816406\n",
      "batch_220: train_l:0.6837242663715759, train_accuracy:0.7442731900452488, time_count:0.23288249969482422\n",
      "batch_230: train_l:0.6825883342074109, train_accuracy:0.7443181818181818, time_count:0.23216557502746582\n",
      "batch_240: train_l:0.6786858976134621, train_accuracy:0.7454292012448133, time_count:0.23294305801391602\n",
      "batch_250: train_l:0.6756615295590632, train_accuracy:0.7463271912350598, time_count:0.2324056625366211\n",
      "batch_260: train_l:0.6753696231778097, train_accuracy:0.7464978448275862, time_count:0.23470616340637207\n",
      "batch_270: train_l:0.6750942526928173, train_accuracy:0.7462523062730627, time_count:0.23358893394470215\n",
      "batch_280: train_l:0.6754041351245391, train_accuracy:0.7460520462633452, time_count:0.23447513580322266\n",
      "batch_290: train_l:0.6752256235920686, train_accuracy:0.7463487972508591, time_count:0.2339153289794922\n",
      "batch_300: train_l:0.6749955410775156, train_accuracy:0.7462105481727574, time_count:0.23649382591247559\n",
      "batch_310: train_l:0.6731028864429695, train_accuracy:0.7466840836012861, time_count:0.24789667129516602\n",
      "batch_320: train_l:0.6732922058053478, train_accuracy:0.746690031152648, time_count:0.24202227592468262\n",
      "batch_330: train_l:0.6720760461967157, train_accuracy:0.747356495468278, time_count:0.2319936752319336\n",
      "batch_340: train_l:0.6717250148746625, train_accuracy:0.7473194648093842, time_count:0.23406291007995605\n",
      "batch_350: train_l:0.6690884202803642, train_accuracy:0.7483974358974359, time_count:0.23546218872070312\n",
      "batch_360: train_l:0.6740298300073417, train_accuracy:0.7467754501385041, time_count:0.23563098907470703\n",
      "batch_370: train_l:0.674829670120121, train_accuracy:0.7464622641509434, time_count:0.2357780933380127\n",
      "batch_380: train_l:0.6751289490483251, train_accuracy:0.7465141076115486, time_count:0.23040318489074707\n",
      "batch_390: train_l:0.6734520056668449, train_accuracy:0.747022858056266, time_count:0.23080754280090332\n",
      "batch_400: train_l:0.6705493033675481, train_accuracy:0.7479738154613467, time_count:0.231123685836792\n",
      "batch_410: train_l:0.6700753513043815, train_accuracy:0.7482702250608273, time_count:0.23465776443481445\n",
      "batch_420: train_l:0.6691067390090779, train_accuracy:0.7484783254156769, time_count:0.23186588287353516\n",
      "batch_430: train_l:0.6677207840291087, train_accuracy:0.7488399071925754, time_count:0.23514127731323242\n",
      "batch_440: train_l:0.6663601971537618, train_accuracy:0.7493622448979592, time_count:0.23351097106933594\n",
      "batch_450: train_l:0.6671791547153583, train_accuracy:0.749047256097561, time_count:0.2362840175628662\n",
      "batch_460: train_l:0.6680082076930173, train_accuracy:0.7485934110629068, time_count:0.2369370460510254\n",
      "================================\n",
      "epoch_9: test_acc:0.7535\n",
      "================================\n",
      "batch_0: train_l:0.7601705193519592, train_accuracy:0.703125, time_count:0.2445204257965088\n",
      "batch_10: train_l:0.677562415599823, train_accuracy:0.7457386363636364, time_count:0.24084758758544922\n",
      "batch_20: train_l:0.6556832180136726, train_accuracy:0.7555803571428571, time_count:0.23671507835388184\n",
      "batch_30: train_l:0.6659263881944841, train_accuracy:0.7520161290322581, time_count:0.23566436767578125\n",
      "batch_40: train_l:0.653394119768608, train_accuracy:0.7570503048780488, time_count:0.2365267276763916\n",
      "batch_50: train_l:0.6503256728836134, train_accuracy:0.7581188725490197, time_count:0.23781228065490723\n",
      "batch_60: train_l:0.6470706692484559, train_accuracy:0.7598616803278688, time_count:0.24522924423217773\n",
      "batch_70: train_l:0.6489788323221072, train_accuracy:0.7581426056338029, time_count:0.2433626651763916\n",
      "batch_80: train_l:0.6443590054541458, train_accuracy:0.7585841049382716, time_count:0.23408150672912598\n",
      "batch_90: train_l:0.6423586675753961, train_accuracy:0.759271978021978, time_count:0.2398056983947754\n",
      "batch_100: train_l:0.6444699318692235, train_accuracy:0.7575804455445545, time_count:0.23925256729125977\n",
      "batch_110: train_l:0.6447600730367609, train_accuracy:0.7578125, time_count:0.23893284797668457\n",
      "batch_120: train_l:0.6442571235589745, train_accuracy:0.757941632231405, time_count:0.23934030532836914\n",
      "batch_130: train_l:0.6433956834651132, train_accuracy:0.7585281488549618, time_count:0.24072980880737305\n",
      "batch_140: train_l:0.6451003912915575, train_accuracy:0.7582003546099291, time_count:0.24278712272644043\n",
      "batch_150: train_l:0.6419929293607245, train_accuracy:0.7595198675496688, time_count:0.24723172187805176\n",
      "batch_160: train_l:0.6376417597628529, train_accuracy:0.7612092391304348, time_count:0.2430858612060547\n",
      "batch_170: train_l:0.6395730470356188, train_accuracy:0.7606451023391813, time_count:0.24405670166015625\n",
      "batch_180: train_l:0.6386181277464766, train_accuracy:0.7609202348066298, time_count:0.24209356307983398\n",
      "batch_190: train_l:0.6381195693115913, train_accuracy:0.7614937827225131, time_count:0.2465817928314209\n",
      "batch_200: train_l:0.636926476042069, train_accuracy:0.761699315920398, time_count:0.24375653266906738\n",
      "batch_210: train_l:0.6368325313120656, train_accuracy:0.7614780805687204, time_count:0.24181556701660156\n",
      "batch_220: train_l:0.6360042146428139, train_accuracy:0.7618071266968326, time_count:0.2424030303955078\n",
      "batch_230: train_l:0.6350333304890307, train_accuracy:0.7622767857142857, time_count:0.2440652847290039\n",
      "batch_240: train_l:0.6340992221941097, train_accuracy:0.7628695539419087, time_count:0.2445516586303711\n",
      "batch_250: train_l:0.6321540397714334, train_accuracy:0.7634462151394422, time_count:0.2579777240753174\n",
      "batch_260: train_l:0.6336476470547161, train_accuracy:0.7632602969348659, time_count:0.24257111549377441\n",
      "batch_270: train_l:0.6326044339315477, train_accuracy:0.7636070110701108, time_count:0.24149799346923828\n",
      "batch_280: train_l:0.6313302294215274, train_accuracy:0.7637622330960854, time_count:0.2410566806793213\n",
      "batch_290: train_l:0.6324986233744015, train_accuracy:0.7632624570446735, time_count:0.24238252639770508\n",
      "batch_300: train_l:0.6322427847456694, train_accuracy:0.7632371262458472, time_count:0.24108099937438965\n",
      "batch_310: train_l:0.6293408137617387, train_accuracy:0.7643689710610932, time_count:0.26389098167419434\n",
      "batch_320: train_l:0.6278873861207398, train_accuracy:0.7646271417445483, time_count:0.24120068550109863\n",
      "batch_330: train_l:0.627911135509295, train_accuracy:0.7645864803625377, time_count:0.24054980278015137\n",
      "batch_340: train_l:0.6267316837814196, train_accuracy:0.764983504398827, time_count:0.2437889575958252\n",
      "batch_350: train_l:0.6268102511381491, train_accuracy:0.7651575854700855, time_count:0.2430434226989746\n",
      "batch_360: train_l:0.6248724510134752, train_accuracy:0.7655600761772853, time_count:0.24154114723205566\n",
      "batch_370: train_l:0.6249032525039426, train_accuracy:0.7653723045822103, time_count:0.24614548683166504\n",
      "batch_380: train_l:0.6241556054025185, train_accuracy:0.765625, time_count:0.25207042694091797\n",
      "batch_390: train_l:0.6239453555677857, train_accuracy:0.765565057544757, time_count:0.24175500869750977\n",
      "batch_400: train_l:0.6242059367404614, train_accuracy:0.7655665523690773, time_count:0.24361395835876465\n",
      "batch_410: train_l:0.6243566775699021, train_accuracy:0.7655489659367397, time_count:0.24495530128479004\n",
      "batch_420: train_l:0.623612221445437, train_accuracy:0.7658476840855107, time_count:0.24633097648620605\n",
      "batch_430: train_l:0.6234138190331426, train_accuracy:0.7659875290023201, time_count:0.24914884567260742\n",
      "batch_440: train_l:0.623104126680465, train_accuracy:0.7662273242630385, time_count:0.24040961265563965\n",
      "batch_450: train_l:0.6223856491550902, train_accuracy:0.7663525498891353, time_count:0.25077295303344727\n",
      "batch_460: train_l:0.6226672369202926, train_accuracy:0.7662011930585684, time_count:0.2377638816833496\n",
      "================================\n",
      "epoch_10: test_acc:0.7643\n",
      "================================\n",
      "batch_0: train_l:0.5850462317466736, train_accuracy:0.7734375, time_count:0.23779845237731934\n",
      "batch_10: train_l:0.5790677097710696, train_accuracy:0.7833806818181818, time_count:0.24112892150878906\n",
      "batch_20: train_l:0.6111430256139665, train_accuracy:0.7693452380952381, time_count:0.23845171928405762\n",
      "batch_30: train_l:0.6074924488221446, train_accuracy:0.7676411290322581, time_count:0.25785183906555176\n",
      "batch_40: train_l:0.5979950275362992, train_accuracy:0.7749618902439024, time_count:0.24347138404846191\n",
      "batch_50: train_l:0.6025029263075661, train_accuracy:0.7709865196078431, time_count:0.2435770034790039\n",
      "batch_60: train_l:0.6086385675141068, train_accuracy:0.7694672131147541, time_count:0.24957728385925293\n",
      "batch_70: train_l:0.611012361838784, train_accuracy:0.7677156690140845, time_count:0.24843621253967285\n",
      "batch_80: train_l:0.6110605343624398, train_accuracy:0.7687114197530864, time_count:0.24972915649414062\n",
      "batch_90: train_l:0.6102972744585393, train_accuracy:0.7689732142857143, time_count:0.24814438819885254\n",
      "batch_100: train_l:0.6087834200646618, train_accuracy:0.770884900990099, time_count:0.249985933303833\n",
      "batch_110: train_l:0.6041645472114151, train_accuracy:0.7717483108108109, time_count:0.24219202995300293\n",
      "batch_120: train_l:0.5995263816896549, train_accuracy:0.7728564049586777, time_count:0.2407379150390625\n",
      "batch_130: train_l:0.6005579956615245, train_accuracy:0.7728411259541985, time_count:0.2647833824157715\n",
      "batch_140: train_l:0.59766759834391, train_accuracy:0.7740469858156028, time_count:0.2478020191192627\n",
      "batch_150: train_l:0.597056828389894, train_accuracy:0.7743687913907285, time_count:0.24016261100769043\n",
      "batch_160: train_l:0.5942310041152172, train_accuracy:0.7751358695652174, time_count:0.2413043975830078\n",
      "batch_170: train_l:0.5934727286037645, train_accuracy:0.7753106725146199, time_count:0.24564146995544434\n",
      "batch_180: train_l:0.5919167570646297, train_accuracy:0.7762862569060773, time_count:0.23270463943481445\n",
      "batch_190: train_l:0.5916307156622722, train_accuracy:0.7766688481675392, time_count:0.24094700813293457\n",
      "batch_200: train_l:0.5929445609524475, train_accuracy:0.7763914800995025, time_count:0.24254822731018066\n",
      "batch_210: train_l:0.5943325433120908, train_accuracy:0.775881220379147, time_count:0.2415628433227539\n",
      "batch_220: train_l:0.5954878489356236, train_accuracy:0.7754524886877828, time_count:0.24411725997924805\n",
      "batch_230: train_l:0.596556258020979, train_accuracy:0.7744859307359307, time_count:0.23976516723632812\n",
      "batch_240: train_l:0.5975733223792429, train_accuracy:0.7743127593360996, time_count:0.24548912048339844\n",
      "batch_250: train_l:0.598067388472804, train_accuracy:0.7739977589641435, time_count:0.24794816970825195\n",
      "batch_260: train_l:0.5984241211094619, train_accuracy:0.7737068965517241, time_count:0.3017241954803467\n",
      "batch_270: train_l:0.5985975650403772, train_accuracy:0.7736104704797048, time_count:0.23435091972351074\n",
      "batch_280: train_l:0.5997339343897389, train_accuracy:0.7733818950177936, time_count:0.24109339714050293\n",
      "batch_290: train_l:0.6013516697686973, train_accuracy:0.7727126288659794, time_count:0.27057814598083496\n",
      "batch_300: train_l:0.6004159365381513, train_accuracy:0.7730481727574751, time_count:0.24421191215515137\n",
      "batch_310: train_l:0.6004831812006101, train_accuracy:0.7733370176848875, time_count:0.24348998069763184\n",
      "batch_320: train_l:0.6004099814319908, train_accuracy:0.7734131619937694, time_count:0.24472522735595703\n",
      "batch_330: train_l:0.6002120828520495, train_accuracy:0.7734847054380665, time_count:0.24539923667907715\n",
      "batch_340: train_l:0.5984718342505592, train_accuracy:0.7741248167155426, time_count:0.24857497215270996\n",
      "batch_350: train_l:0.5977841627563846, train_accuracy:0.7744836182336182, time_count:0.24246954917907715\n",
      "batch_360: train_l:0.5978738119067247, train_accuracy:0.7744979224376731, time_count:0.24491429328918457\n",
      "batch_370: train_l:0.5964819083637947, train_accuracy:0.7747430929919138, time_count:0.24247074127197266\n",
      "batch_380: train_l:0.5966361884682823, train_accuracy:0.775016404199475, time_count:0.24214506149291992\n",
      "batch_390: train_l:0.5976381607525184, train_accuracy:0.7744964833759591, time_count:0.23265743255615234\n",
      "batch_400: train_l:0.5974239719627504, train_accuracy:0.774762312967581, time_count:0.2367715835571289\n",
      "batch_410: train_l:0.5959708436127127, train_accuracy:0.7752052919708029, time_count:0.2389509677886963\n",
      "batch_420: train_l:0.5951481323485703, train_accuracy:0.7756643408551069, time_count:0.23714184761047363\n",
      "batch_430: train_l:0.5945891584431765, train_accuracy:0.7757758120649652, time_count:0.24011969566345215\n",
      "batch_440: train_l:0.5942841175597271, train_accuracy:0.7759708049886621, time_count:0.2397315502166748\n",
      "batch_450: train_l:0.5937995611299697, train_accuracy:0.7759666019955654, time_count:0.24731826782226562\n",
      "batch_460: train_l:0.5933479971844307, train_accuracy:0.7760303687635575, time_count:0.2366783618927002\n",
      "================================\n",
      "epoch_11: test_acc:0.7709\n",
      "================================\n",
      "batch_0: train_l:0.5696988105773926, train_accuracy:0.75, time_count:0.2396707534790039\n",
      "batch_10: train_l:0.5912627902897921, train_accuracy:0.7826704545454546, time_count:0.23045659065246582\n",
      "batch_20: train_l:0.5984900409267062, train_accuracy:0.7730654761904762, time_count:0.23570728302001953\n",
      "batch_30: train_l:0.5898556670834941, train_accuracy:0.7777217741935484, time_count:0.24158501625061035\n",
      "batch_40: train_l:0.5845079167587001, train_accuracy:0.782202743902439, time_count:0.24427103996276855\n",
      "batch_50: train_l:0.5836778312337165, train_accuracy:0.7797181372549019, time_count:0.23740077018737793\n",
      "batch_60: train_l:0.5904831456356361, train_accuracy:0.7776639344262295, time_count:0.23764824867248535\n",
      "batch_70: train_l:0.5890811820265273, train_accuracy:0.7783890845070423, time_count:0.23740768432617188\n",
      "batch_80: train_l:0.5907237180221228, train_accuracy:0.7765239197530864, time_count:0.23728275299072266\n",
      "batch_90: train_l:0.584736827310625, train_accuracy:0.7786744505494505, time_count:0.23895549774169922\n",
      "batch_100: train_l:0.5784494959481872, train_accuracy:0.7814047029702971, time_count:0.2346968650817871\n",
      "batch_110: train_l:0.5848898552022539, train_accuracy:0.7796311936936937, time_count:0.24042463302612305\n",
      "batch_120: train_l:0.5826517341058116, train_accuracy:0.7807980371900827, time_count:0.24028539657592773\n",
      "batch_130: train_l:0.5782016998028937, train_accuracy:0.7825023854961832, time_count:0.23966765403747559\n",
      "batch_140: train_l:0.5799814616957455, train_accuracy:0.78125, time_count:0.23660945892333984\n",
      "batch_150: train_l:0.5797431924879946, train_accuracy:0.78125, time_count:0.234938383102417\n",
      "batch_160: train_l:0.5811857701458546, train_accuracy:0.7811044254658385, time_count:0.2343754768371582\n",
      "batch_170: train_l:0.5802792483254483, train_accuracy:0.7811586257309941, time_count:0.23522639274597168\n",
      "batch_180: train_l:0.5804628183169919, train_accuracy:0.7811205110497238, time_count:0.23592376708984375\n",
      "batch_190: train_l:0.5793832205040916, train_accuracy:0.7814545157068062, time_count:0.236724853515625\n",
      "batch_200: train_l:0.5776098205972073, train_accuracy:0.7816386815920398, time_count:0.23656630516052246\n",
      "batch_210: train_l:0.5770440662359174, train_accuracy:0.7818794431279621, time_count:0.2402656078338623\n",
      "batch_220: train_l:0.5790788745718305, train_accuracy:0.7813207013574661, time_count:0.24026250839233398\n",
      "batch_230: train_l:0.581735543487392, train_accuracy:0.7799310064935064, time_count:0.24300408363342285\n",
      "batch_240: train_l:0.5794381974149047, train_accuracy:0.7808609958506224, time_count:0.2402040958404541\n",
      "batch_250: train_l:0.5797890421166363, train_accuracy:0.7805963645418327, time_count:0.24214744567871094\n",
      "batch_260: train_l:0.5809244660810492, train_accuracy:0.7798132183908046, time_count:0.23874568939208984\n",
      "batch_270: train_l:0.5808759814478814, train_accuracy:0.7797509225092251, time_count:0.25414109230041504\n",
      "batch_280: train_l:0.5819209586046769, train_accuracy:0.7793872330960854, time_count:0.23563122749328613\n",
      "batch_290: train_l:0.5812521249158276, train_accuracy:0.7798002577319587, time_count:0.23579835891723633\n",
      "batch_300: train_l:0.5809086742393202, train_accuracy:0.7798743770764119, time_count:0.23640012741088867\n",
      "batch_310: train_l:0.5791718510377829, train_accuracy:0.7804712620578779, time_count:0.2350444793701172\n",
      "batch_320: train_l:0.5785405133557839, train_accuracy:0.7808119158878505, time_count:0.23511266708374023\n",
      "batch_330: train_l:0.5784016351505349, train_accuracy:0.7809903700906344, time_count:0.23489665985107422\n",
      "batch_340: train_l:0.5791124735584706, train_accuracy:0.7804481304985337, time_count:0.23459649085998535\n",
      "batch_350: train_l:0.5790451505245306, train_accuracy:0.7807603276353277, time_count:0.23578381538391113\n",
      "batch_360: train_l:0.5771707856093747, train_accuracy:0.7815962603878116, time_count:0.2357616424560547\n",
      "batch_370: train_l:0.5774060672505525, train_accuracy:0.7812710579514824, time_count:0.23476123809814453\n",
      "batch_380: train_l:0.5792789901961178, train_accuracy:0.7806758530183727, time_count:0.23619699478149414\n",
      "batch_390: train_l:0.5769991482157841, train_accuracy:0.7813898657289002, time_count:0.2370011806488037\n",
      "batch_400: train_l:0.5772512957936807, train_accuracy:0.7811525872817955, time_count:0.23451614379882812\n",
      "batch_410: train_l:0.5772920697564916, train_accuracy:0.7812119829683698, time_count:0.2347090244293213\n",
      "batch_420: train_l:0.5781099833105635, train_accuracy:0.781082986935867, time_count:0.23630905151367188\n",
      "batch_430: train_l:0.5775423007747137, train_accuracy:0.7814493909512761, time_count:0.24221396446228027\n",
      "batch_440: train_l:0.5768495142595027, train_accuracy:0.7816220238095238, time_count:0.23058176040649414\n",
      "batch_450: train_l:0.5764437394501629, train_accuracy:0.7818562915742794, time_count:0.24110007286071777\n",
      "batch_460: train_l:0.5760988112126928, train_accuracy:0.7818600867678959, time_count:0.23172497749328613\n",
      "================================\n",
      "epoch_12: test_acc:0.7797\n",
      "================================\n",
      "batch_0: train_l:0.5589337348937988, train_accuracy:0.8125, time_count:0.23915672302246094\n",
      "batch_10: train_l:0.5510008822787892, train_accuracy:0.7954545454545454, time_count:0.24489092826843262\n",
      "batch_20: train_l:0.561440775791804, train_accuracy:0.7875744047619048, time_count:0.23439812660217285\n",
      "batch_30: train_l:0.5572937063632473, train_accuracy:0.7895665322580645, time_count:0.23436665534973145\n",
      "batch_40: train_l:0.5766160495397521, train_accuracy:0.780297256097561, time_count:0.2340412139892578\n",
      "batch_50: train_l:0.5715026551601934, train_accuracy:0.782015931372549, time_count:0.2342386245727539\n",
      "batch_60: train_l:0.5688917583129445, train_accuracy:0.7839395491803278, time_count:0.23394179344177246\n",
      "batch_70: train_l:0.5630088633214924, train_accuracy:0.7871919014084507, time_count:0.23541545867919922\n",
      "batch_80: train_l:0.5678200994008853, train_accuracy:0.7849151234567902, time_count:0.2420637607574463\n",
      "batch_90: train_l:0.5651446990259401, train_accuracy:0.7849416208791209, time_count:0.23872995376586914\n",
      "batch_100: train_l:0.5683843540673209, train_accuracy:0.7833384900990099, time_count:0.23891901969909668\n",
      "batch_110: train_l:0.5682067798601614, train_accuracy:0.7832911036036037, time_count:0.2394709587097168\n",
      "batch_120: train_l:0.5628365512229194, train_accuracy:0.7858987603305785, time_count:0.2393355369567871\n",
      "batch_130: train_l:0.5639396969598668, train_accuracy:0.7851860687022901, time_count:0.23980164527893066\n",
      "batch_140: train_l:0.564300905094079, train_accuracy:0.7839095744680851, time_count:0.23929452896118164\n",
      "batch_150: train_l:0.5630183176489065, train_accuracy:0.7840438741721855, time_count:0.2399158477783203\n",
      "batch_160: train_l:0.5638016243899091, train_accuracy:0.7837732919254659, time_count:0.24034547805786133\n",
      "batch_170: train_l:0.5632152848424967, train_accuracy:0.7843110380116959, time_count:0.23993992805480957\n",
      "batch_180: train_l:0.5626906006705037, train_accuracy:0.7846167127071824, time_count:0.24001741409301758\n",
      "batch_190: train_l:0.5647166182545467, train_accuracy:0.7840314136125655, time_count:0.25786328315734863\n",
      "batch_200: train_l:0.5622808814641849, train_accuracy:0.7845537935323383, time_count:0.23932266235351562\n",
      "batch_210: train_l:0.5627712518400491, train_accuracy:0.7843972156398105, time_count:0.2404334545135498\n",
      "batch_220: train_l:0.5639945063655732, train_accuracy:0.7840073529411765, time_count:0.23994874954223633\n",
      "batch_230: train_l:0.5626918975408975, train_accuracy:0.7844629329004329, time_count:0.24483466148376465\n",
      "batch_240: train_l:0.5620092901698781, train_accuracy:0.7847834543568465, time_count:0.24357247352600098\n",
      "batch_250: train_l:0.5601093973296573, train_accuracy:0.7854208167330677, time_count:0.2434101104736328\n",
      "batch_260: train_l:0.5598772413657542, train_accuracy:0.7855902777777778, time_count:0.24427318572998047\n",
      "batch_270: train_l:0.5600556679537375, train_accuracy:0.785372463099631, time_count:0.24303054809570312\n",
      "batch_280: train_l:0.5596301184430241, train_accuracy:0.7853925711743772, time_count:0.24245262145996094\n",
      "batch_290: train_l:0.5600977578933296, train_accuracy:0.7854649914089347, time_count:0.2414710521697998\n",
      "batch_300: train_l:0.5605826440245606, train_accuracy:0.7852730481727574, time_count:0.24153876304626465\n",
      "batch_310: train_l:0.5582726641866556, train_accuracy:0.7862238745980707, time_count:0.23325705528259277\n",
      "batch_320: train_l:0.5582962281236025, train_accuracy:0.7863853193146417, time_count:0.23512649536132812\n",
      "batch_330: train_l:0.5583804051681588, train_accuracy:0.786418995468278, time_count:0.24161291122436523\n",
      "batch_340: train_l:0.5577375485051063, train_accuracy:0.7866110703812317, time_count:0.23160219192504883\n",
      "batch_350: train_l:0.558786208897914, train_accuracy:0.7863025284900285, time_count:0.23594379425048828\n",
      "batch_360: train_l:0.5585277930025909, train_accuracy:0.7862274930747922, time_count:0.24494647979736328\n",
      "batch_370: train_l:0.5590022774237506, train_accuracy:0.7860512129380054, time_count:0.2358231544494629\n",
      "batch_380: train_l:0.5594976975379653, train_accuracy:0.7858021653543307, time_count:0.23593878746032715\n",
      "batch_390: train_l:0.5589563352677523, train_accuracy:0.7861053388746803, time_count:0.23627591133117676\n",
      "batch_400: train_l:0.5592653740968491, train_accuracy:0.7862959788029925, time_count:0.23648548126220703\n",
      "batch_410: train_l:0.5596541056087707, train_accuracy:0.7864203163017032, time_count:0.23881030082702637\n",
      "batch_420: train_l:0.559701316642648, train_accuracy:0.7863160629453682, time_count:0.23813533782958984\n",
      "batch_430: train_l:0.5597018923809246, train_accuracy:0.7864341647331786, time_count:0.2407996654510498\n",
      "batch_440: train_l:0.55930792690675, train_accuracy:0.7865823412698413, time_count:0.24104094505310059\n",
      "batch_450: train_l:0.5582510534913471, train_accuracy:0.787035753880266, time_count:0.23698925971984863\n",
      "batch_460: train_l:0.5586002480699287, train_accuracy:0.7869271963123644, time_count:0.23738718032836914\n",
      "================================\n",
      "epoch_13: test_acc:0.79\n",
      "================================\n",
      "batch_0: train_l:0.6032623052597046, train_accuracy:0.7734375, time_count:0.2380518913269043\n",
      "batch_10: train_l:0.5520998131145131, train_accuracy:0.7919034090909091, time_count:0.24112939834594727\n",
      "batch_20: train_l:0.5504742931751978, train_accuracy:0.7875744047619048, time_count:0.24135875701904297\n",
      "batch_30: train_l:0.5382136885196932, train_accuracy:0.7925907258064516, time_count:0.23774147033691406\n",
      "batch_40: train_l:0.5499341931284928, train_accuracy:0.7892530487804879, time_count:0.25133466720581055\n",
      "batch_50: train_l:0.5557741511101816, train_accuracy:0.7873774509803921, time_count:0.23194336891174316\n",
      "batch_60: train_l:0.5543045616540753, train_accuracy:0.7875256147540983, time_count:0.24050688743591309\n",
      "batch_70: train_l:0.5549559794681173, train_accuracy:0.7866417253521126, time_count:0.23137998580932617\n",
      "batch_80: train_l:0.5501723775157222, train_accuracy:0.7888695987654321, time_count:0.233992338180542\n",
      "batch_90: train_l:0.5476313429874379, train_accuracy:0.7901785714285714, time_count:0.23409390449523926\n",
      "batch_100: train_l:0.5482837454517289, train_accuracy:0.7899133663366337, time_count:0.23413419723510742\n",
      "batch_110: train_l:0.5458115774231989, train_accuracy:0.7908220720720721, time_count:0.23822522163391113\n",
      "batch_120: train_l:0.5450956924887728, train_accuracy:0.7912577479338843, time_count:0.2371668815612793\n",
      "batch_130: train_l:0.5433343594310848, train_accuracy:0.792581106870229, time_count:0.23978185653686523\n",
      "batch_140: train_l:0.5461909071350775, train_accuracy:0.7912234042553191, time_count:0.23463821411132812\n",
      "batch_150: train_l:0.5479907428981453, train_accuracy:0.7903559602649006, time_count:0.2552347183227539\n",
      "batch_160: train_l:0.5468045915505901, train_accuracy:0.7913916925465838, time_count:0.24208664894104004\n",
      "batch_170: train_l:0.5450327926211886, train_accuracy:0.7929002192982456, time_count:0.24205994606018066\n",
      "batch_180: train_l:0.544514584771836, train_accuracy:0.7935082872928176, time_count:0.2438187599182129\n",
      "batch_190: train_l:0.5442375341010968, train_accuracy:0.7930710078534031, time_count:0.242081880569458\n",
      "batch_200: train_l:0.545536094340519, train_accuracy:0.792949315920398, time_count:0.24114060401916504\n",
      "batch_210: train_l:0.5448867859433613, train_accuracy:0.7929502369668247, time_count:0.242340087890625\n",
      "batch_220: train_l:0.5444215351370125, train_accuracy:0.7931631787330317, time_count:0.24162912368774414\n",
      "batch_230: train_l:0.5441954578414108, train_accuracy:0.7931547619047619, time_count:0.24097514152526855\n",
      "batch_240: train_l:0.5441147913823978, train_accuracy:0.7933739626556017, time_count:0.2597031593322754\n",
      "batch_250: train_l:0.542784630658617, train_accuracy:0.7937935756972112, time_count:0.24481463432312012\n",
      "batch_260: train_l:0.5442202825427512, train_accuracy:0.7930735153256705, time_count:0.24512839317321777\n",
      "batch_270: train_l:0.5447533008577199, train_accuracy:0.7928390221402214, time_count:0.24434709548950195\n",
      "batch_280: train_l:0.5455840045447027, train_accuracy:0.7921485765124555, time_count:0.24711132049560547\n",
      "batch_290: train_l:0.5450049823092431, train_accuracy:0.7921230670103093, time_count:0.24225497245788574\n",
      "batch_300: train_l:0.5435614290823572, train_accuracy:0.7926962209302325, time_count:0.24226641654968262\n",
      "batch_310: train_l:0.5431426970521737, train_accuracy:0.7931822749196141, time_count:0.24315977096557617\n",
      "batch_320: train_l:0.5446485727561226, train_accuracy:0.7924454828660437, time_count:0.2409510612487793\n",
      "batch_330: train_l:0.544226407734649, train_accuracy:0.7923668806646526, time_count:0.24536538124084473\n",
      "batch_340: train_l:0.544086832891811, train_accuracy:0.7924990835777126, time_count:0.24254798889160156\n",
      "batch_350: train_l:0.5448518099098804, train_accuracy:0.7923789173789174, time_count:0.24144697189331055\n",
      "batch_360: train_l:0.5456840764122326, train_accuracy:0.7920489958448753, time_count:0.24154973030090332\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import load_data_fashion_mnist, train_ch6_gpu\n",
    "\n",
    "\n",
    "# NiN Network in Network, use a c_in*1*1 conv kernel to replace a full connection layer, witch extremely decrease parameters\n",
    "def nin_block(in_channels, out_channels, kernel_size, strides, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=strides, padding=padding), nn.ReLU(),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU(),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU()\n",
    "    )\n",
    "\n",
    "\n",
    "#  so way use two 1x1 conv kernel?\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nin_block(1, 96, 11, 4, 0), nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    nin_block(96, 256, 5, 1, 2), nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    nin_block(256, 384, 3, 1, 1), nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    nn.Dropout(p=0.5),\n",
    "    # use a nin to output classification result\n",
    "    nin_block(384, 10, kernel_size=3, strides=1, padding=1),\n",
    "    nn.AdaptiveAvgPool2d((1, 1)),\n",
    "    nn.Flatten()\n",
    ")\n",
    "\n",
    "lr, num_epochs, batch_size = 0.1, 50, 128\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=224)\n",
    "train_ch6_gpu(net, train_iter, test_iter, num_epochs, lr, 'cuda:0')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net will be trained on cuda:0\n",
      "batch_0: train_l:2.310108184814453, train_accuracy:0.09375, time_count:0.23650741577148438\n",
      "batch_10: train_l:2.3037409348921343, train_accuracy:0.08096590909090909, time_count:0.20711088180541992\n",
      "batch_20: train_l:2.303336801983061, train_accuracy:0.0896577380952381, time_count:0.2083423137664795\n",
      "batch_30: train_l:2.30305254074835, train_accuracy:0.09173387096774194, time_count:0.20851373672485352\n",
      "batch_40: train_l:2.302480121938194, train_accuracy:0.0975609756097561, time_count:0.2091217041015625\n",
      "batch_50: train_l:2.3028254462223425, train_accuracy:0.09666053921568628, time_count:0.211592435836792\n",
      "batch_60: train_l:2.302575017585129, train_accuracy:0.09784836065573771, time_count:0.21390533447265625\n",
      "batch_70: train_l:2.3026920976773115, train_accuracy:0.09881161971830986, time_count:0.22005558013916016\n",
      "batch_80: train_l:2.302569354021991, train_accuracy:0.10146604938271606, time_count:0.22761821746826172\n",
      "batch_90: train_l:2.3024109641274255, train_accuracy:0.10104739010989011, time_count:0.23353099822998047\n",
      "batch_100: train_l:2.3023677840091215, train_accuracy:0.1015625, time_count:0.23346400260925293\n",
      "batch_110: train_l:2.302241492915798, train_accuracy:0.10121058558558559, time_count:0.23311758041381836\n",
      "batch_120: train_l:2.302145110674141, train_accuracy:0.10291838842975207, time_count:0.233811616897583\n",
      "batch_130: train_l:2.301977885588435, train_accuracy:0.10448473282442748, time_count:0.23371148109436035\n",
      "batch_140: train_l:2.301718897853337, train_accuracy:0.10460992907801418, time_count:0.2333672046661377\n",
      "batch_150: train_l:2.3014103892623194, train_accuracy:0.10678807947019868, time_count:0.23421454429626465\n",
      "batch_160: train_l:2.3011893959519285, train_accuracy:0.11126746894409938, time_count:0.23353266716003418\n",
      "batch_170: train_l:2.3009852875045866, train_accuracy:0.11426352339181287, time_count:0.23358845710754395\n",
      "batch_180: train_l:2.300621433152678, train_accuracy:0.11645372928176796, time_count:0.2323007583618164\n",
      "batch_190: train_l:2.300240074776854, train_accuracy:0.11878272251308901, time_count:0.2333831787109375\n",
      "batch_200: train_l:2.2995686708991205, train_accuracy:0.12309546019900497, time_count:0.23243236541748047\n",
      "batch_210: train_l:2.2988394402779675, train_accuracy:0.12648104265402843, time_count:0.23308444023132324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fb85299f940>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/siky/Applications/YOLOX/demo/MegEngine/PVMegengine/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/siky/Applications/YOLOX/demo/MegEngine/PVMegengine/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1297, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/popen_fork.py\", line 44, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/lib/python3.8/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_40157/2675509435.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     70\u001B[0m \u001B[0mlr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_epochs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_size\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0.1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m10\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m128\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     71\u001B[0m \u001B[0mtrain_iter\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_iter\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mload_data_fashion_mnist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresize\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m96\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 72\u001B[0;31m \u001B[0mtrain_ch6_gpu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnet\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_iter\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_iter\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_epochs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'cuda:0'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/Programs/DiveIntoDeepLearning/d2l.py\u001B[0m in \u001B[0;36mtrain_ch6_gpu\u001B[0;34m(net, train_iter, test_iter, num_epochs, lr, device)\u001B[0m\n\u001B[1;32m    132\u001B[0m             \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    133\u001B[0m             \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 134\u001B[0;31m             \u001B[0my_hat\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnet\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    135\u001B[0m             \u001B[0ml\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mloss\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my_hat\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    136\u001B[0m             \u001B[0ml\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Applications/YOLOX/demo/MegEngine/PVMegengine/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    887\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    888\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 889\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    890\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    891\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Applications/YOLOX/demo/MegEngine/PVMegengine/lib/python3.8/site-packages/torch/nn/modules/container.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    117\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    118\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 119\u001B[0;31m             \u001B[0minput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodule\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    120\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    121\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Applications/YOLOX/demo/MegEngine/PVMegengine/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    887\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    888\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 889\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    890\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    891\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Applications/YOLOX/demo/MegEngine/PVMegengine/lib/python3.8/site-packages/torch/nn/modules/container.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    117\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    118\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 119\u001B[0;31m             \u001B[0minput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodule\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    120\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    121\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Applications/YOLOX/demo/MegEngine/PVMegengine/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    887\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    888\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 889\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    890\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    891\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_40157/2675509435.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m     23\u001B[0m         \u001B[0mp1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrelu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mp1_1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     24\u001B[0m         \u001B[0mp2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrelu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mp2_2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrelu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mp2_1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 25\u001B[0;31m         \u001B[0mp3\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrelu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mp3_2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrelu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mp3_1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     26\u001B[0m         \u001B[0mp4\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrelu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mp4_2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mp4_1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Applications/YOLOX/demo/MegEngine/PVMegengine/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m    932\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_is_full_backward_hook\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    933\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 934\u001B[0;31m     \u001B[0;32mdef\u001B[0m \u001B[0m__getattr__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mUnion\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mTensor\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'Module'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    935\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;34m'_parameters'\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__dict__\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    936\u001B[0m             \u001B[0m_parameters\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__dict__\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'_parameters'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# GoogleNet\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class Inception(nn.Module):\n",
    "    def __init__(self, in_channel, c1, c2, c3, c4, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # path 1 -> conv 1x1\n",
    "        self.p1_1 = nn.Conv2d(in_channels=in_channel, out_channels=c1, kernel_size=1)\n",
    "\n",
    "        self.p2_1 = nn.Conv2d(in_channels=in_channel, out_channels=c2[0], kernel_size=1)\n",
    "        self.p2_2 = nn.Conv2d(in_channels=c2[0], out_channels=c2[1], kernel_size=3, padding=1)\n",
    "\n",
    "        self.p3_1 = nn.Conv2d(in_channels=in_channel, out_channels=c3[0], kernel_size=1)\n",
    "        self.p3_2 = nn.Conv2d(in_channels=c3[0], out_channels=c3[1], kernel_size=5, padding=2)\n",
    "\n",
    "        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.p4_2 = nn.Conv2d(in_channels=in_channel, out_channels=c4, kernel_size=1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        p1 = F.relu(self.p1_1(X))\n",
    "        p2 = F.relu(self.p2_2(F.relu(self.p2_1(X))))\n",
    "        p3 = F.relu(self.p3_2(F.relu(self.p3_1(X))))\n",
    "        p4 = F.relu(self.p4_2(self.p4_1(X)))\n",
    "\n",
    "        return torch.cat((p1, p2, p3, p4), dim=1)  # cat in channel\n",
    "\n",
    "\n",
    "block_1 = nn.Sequential(\n",
    "    nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    ")\n",
    "\n",
    "block_2 = nn.Sequential(\n",
    "    nn.Conv2d(64, 64, kernel_size=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    ")\n",
    "\n",
    "block_3 = nn.Sequential(\n",
    "    Inception(192, 64, (96, 128), (16, 32), 32),\n",
    "    Inception(256, 128, (128, 192), (32, 96), 64),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    ")\n",
    "\n",
    "block_4 = nn.Sequential(\n",
    "    Inception(480, 192, (96, 208), (16, 48), 64),\n",
    "    Inception(512, 160, (112, 224), (24, 64), 64),\n",
    "    Inception(512, 128, (128, 256), (24, 64), 64),\n",
    "    Inception(512, 112, (144, 288), (32, 64), 64),\n",
    "    Inception(528, 256, (160, 320), (32, 128), 128),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    ")\n",
    "block_5 = nn.Sequential(Inception(832, 256, (160, 320), (32, 128), 128),\n",
    "                   Inception(832, 384, (192, 384), (48, 128), 128),\n",
    "                   nn.AdaptiveAvgPool2d((1,1)),\n",
    "                   nn.Flatten())\n",
    "\n",
    "net = nn.Sequential(block_1,block_2,block_3,block_4,block_5, nn.Linear(1024, 10))\n",
    "\n",
    "from d2l import load_data_fashion_mnist,train_ch6_gpu\n",
    "lr, num_epochs, batch_size = 0.1, 10, 128\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=96)\n",
    "train_ch6_gpu(net, train_iter, test_iter, num_epochs, lr, 'cuda:0')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([4, 3, 6, 6])"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ResNet\n",
    "# Residual_Block\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, in_channel, num_channel, use_1x1conv = False, stride = 1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channel,num_channel,kernel_size=(3,3), padding=(1,1), stride=stride)\n",
    "        self.conv2 = nn.Conv2d(num_channel,num_channel,kernel_size=(3,3), padding=(1,1))\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv2d(in_channel, num_channel, kernel_size=1, stride=stride)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.BatchNorm2d(num_channel)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channel)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        Y = Y + X\n",
    "        return F.relu(Y)\n",
    "\n",
    "blk = Residual(4,3,use_1x1conv=True)\n",
    "X = torch.rand(4,4,6,6)\n",
    "Y = blk(X)\n",
    "Y.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# ResNet\n",
    "# the same as GoogLeNet\n",
    "b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.BatchNorm2d(64), nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "def resnet_block(input_channels, num_channels, num_residuals,\n",
    "                 first_block=False):\n",
    "    blk = []\n",
    "    for i in range(num_residuals):\n",
    "        if i == 0 and not first_block:\n",
    "            blk.append(Residual(input_channels, num_channels,\n",
    "                                use_1x1conv=True, stride=2))\n",
    "        else:\n",
    "            blk.append(Residual(num_channels, num_channels))\n",
    "    return blk\n",
    "b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))\n",
    "b3 = nn.Sequential(*resnet_block(64, 128, 2))\n",
    "b4 = nn.Sequential(*resnet_block(128, 256, 2))\n",
    "b5 = nn.Sequential(*resnet_block(256, 512, 2))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([128, 64, 56, 56])\n",
      "Sequential output shape:\t torch.Size([128, 64, 56, 56])\n",
      "Sequential output shape:\t torch.Size([128, 128, 28, 28])\n",
      "Sequential output shape:\t torch.Size([128, 256, 14, 14])\n",
      "Sequential output shape:\t torch.Size([128, 512, 7, 7])\n",
      "AdaptiveAvgPool2d output shape:\t torch.Size([128, 512, 1, 1])\n",
      "Flatten output shape:\t torch.Size([128, 512])\n",
      "Linear output shape:\t torch.Size([128, 10])\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential(b1,b2,b3,b4,b5,nn.AdaptiveAvgPool2d((1,1)),nn.Flatten(), nn.Linear(512,10))\n",
    "X = torch.rand(size=(128, 1, 224, 224))\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__,'output shape:\\t', X.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net will be trained on cuda:0\n",
      "batch_0: train_l:2.8372650146484375, train_accuracy:0.125, time_count:0.8660681247711182\n",
      "batch_10: train_l:2.040444005619396, train_accuracy:0.40980113636363635, time_count:0.2935636043548584\n",
      "batch_20: train_l:1.4339956499281383, train_accuracy:0.5647321428571429, time_count:0.2933018207550049\n",
      "batch_30: train_l:1.1670774730943865, train_accuracy:0.6349546370967742, time_count:0.29345059394836426\n",
      "batch_40: train_l:1.0215179360494382, train_accuracy:0.6745426829268293, time_count:0.29442453384399414\n",
      "batch_50: train_l:0.9123971713524238, train_accuracy:0.7058823529411765, time_count:0.2938680648803711\n",
      "batch_60: train_l:0.8418931213558697, train_accuracy:0.7260502049180327, time_count:0.29495859146118164\n",
      "batch_70: train_l:0.7851747555631987, train_accuracy:0.7420224471830986, time_count:0.2954397201538086\n",
      "batch_80: train_l:0.7449468911429982, train_accuracy:0.7535686728395061, time_count:0.29618406295776367\n",
      "batch_90: train_l:0.7118597754410335, train_accuracy:0.76171875, time_count:0.2963101863861084\n",
      "batch_100: train_l:0.6799975607064691, train_accuracy:0.7710782797029703, time_count:0.30563879013061523\n",
      "batch_110: train_l:0.6540012464330003, train_accuracy:0.778293918918919, time_count:0.3012056350708008\n",
      "batch_120: train_l:0.6317939782930799, train_accuracy:0.7846720041322314, time_count:0.31085848808288574\n",
      "batch_130: train_l:0.6135120651194157, train_accuracy:0.7902850667938931, time_count:0.3113102912902832\n",
      "batch_140: train_l:0.5958650234743212, train_accuracy:0.7952958776595744, time_count:0.31560349464416504\n",
      "batch_150: train_l:0.5779223925625252, train_accuracy:0.8005484271523179, time_count:0.31558918952941895\n",
      "batch_160: train_l:0.5677534467685297, train_accuracy:0.8033530667701864, time_count:0.31468868255615234\n",
      "batch_170: train_l:0.5526222305165397, train_accuracy:0.8080226608187134, time_count:0.3147857189178467\n",
      "batch_180: train_l:0.5402581187078307, train_accuracy:0.811830973756906, time_count:0.31479573249816895\n",
      "batch_190: train_l:0.5275684683266735, train_accuracy:0.816017670157068, time_count:0.3144838809967041\n",
      "batch_200: train_l:0.5188130032808627, train_accuracy:0.8186411691542289, time_count:0.31488513946533203\n",
      "batch_210: train_l:0.5095674161803666, train_accuracy:0.8218120556872038, time_count:0.31465983390808105\n",
      "batch_220: train_l:0.5026362308414813, train_accuracy:0.8241126979638009, time_count:0.31432032585144043\n",
      "batch_230: train_l:0.4951715142308892, train_accuracy:0.8262479707792207, time_count:0.31528377532958984\n",
      "================================\n",
      "epoch_0: test_acc:0.8815104166666666\n",
      "================================\n",
      "batch_0: train_l:0.36055076122283936, train_accuracy:0.8984375, time_count:0.3139026165008545\n",
      "batch_10: train_l:0.27399280260909686, train_accuracy:0.9009232954545454, time_count:0.3156554698944092\n",
      "batch_20: train_l:0.2809091330993743, train_accuracy:0.8949032738095238, time_count:0.31591033935546875\n",
      "batch_30: train_l:0.28849412693131354, train_accuracy:0.8932711693548387, time_count:0.31668615341186523\n",
      "batch_40: train_l:0.28439194727234723, train_accuracy:0.8952934451219512, time_count:0.3161747455596924\n",
      "batch_50: train_l:0.28212227511639687, train_accuracy:0.8957567401960784, time_count:0.31607627868652344\n",
      "batch_60: train_l:0.2779775660057537, train_accuracy:0.8976050204918032, time_count:0.3163177967071533\n",
      "batch_70: train_l:0.2743941459437491, train_accuracy:0.8987676056338029, time_count:0.31628942489624023\n",
      "batch_80: train_l:0.273412533932262, train_accuracy:0.8989197530864198, time_count:0.3161501884460449\n",
      "batch_90: train_l:0.2694470767791455, train_accuracy:0.9010559752747253, time_count:0.31568169593811035\n",
      "batch_100: train_l:0.2690417772472495, train_accuracy:0.9014542079207921, time_count:0.31505751609802246\n",
      "batch_110: train_l:0.2672583185606175, train_accuracy:0.9018158783783784, time_count:0.3147242069244385\n",
      "batch_120: train_l:0.26739285615357483, train_accuracy:0.9014721074380165, time_count:0.31478238105773926\n",
      "batch_130: train_l:0.265513454912273, train_accuracy:0.9025524809160306, time_count:0.31492018699645996\n",
      "batch_140: train_l:0.2633626251778704, train_accuracy:0.9033964982269503, time_count:0.314939022064209\n",
      "batch_150: train_l:0.26382971619138657, train_accuracy:0.9028352649006622, time_count:0.3145618438720703\n",
      "batch_160: train_l:0.26237790286540985, train_accuracy:0.9034598214285714, time_count:0.314868688583374\n",
      "batch_170: train_l:0.2613059236989384, train_accuracy:0.9039199561403509, time_count:0.31517624855041504\n",
      "batch_180: train_l:0.2609530520208633, train_accuracy:0.903962361878453, time_count:0.3145632743835449\n",
      "batch_190: train_l:0.2613747440551588, train_accuracy:0.9034481348167539, time_count:0.3148818016052246\n",
      "batch_200: train_l:0.2617169356909557, train_accuracy:0.9029656405472637, time_count:0.3146493434906006\n",
      "batch_210: train_l:0.26151492739740706, train_accuracy:0.9027880627962085, time_count:0.3146240711212158\n",
      "batch_220: train_l:0.2605063732528039, train_accuracy:0.9031214649321267, time_count:0.3144567012786865\n",
      "batch_230: train_l:0.2593917330622157, train_accuracy:0.9036289231601732, time_count:0.3148500919342041\n",
      "================================\n",
      "epoch_1: test_acc:0.9000400641025641\n",
      "================================\n",
      "batch_0: train_l:0.18670035898685455, train_accuracy:0.9375, time_count:0.3176760673522949\n",
      "batch_10: train_l:0.18703536011955954, train_accuracy:0.9343039772727273, time_count:0.3161957263946533\n",
      "batch_20: train_l:0.19367843866348267, train_accuracy:0.9302455357142857, time_count:0.31607747077941895\n",
      "batch_30: train_l:0.18955869492023222, train_accuracy:0.9333417338709677, time_count:0.31603240966796875\n",
      "batch_40: train_l:0.187596156466298, train_accuracy:0.9338795731707317, time_count:0.31636691093444824\n",
      "batch_50: train_l:0.19090830490869634, train_accuracy:0.932827818627451, time_count:0.3156092166900635\n",
      "batch_60: train_l:0.19068092327626024, train_accuracy:0.9318007172131147, time_count:0.31621742248535156\n",
      "batch_70: train_l:0.1926279263177388, train_accuracy:0.9310629401408451, time_count:0.3162195682525635\n",
      "batch_80: train_l:0.1936843498253528, train_accuracy:0.9308931327160493, time_count:0.316150426864624\n",
      "batch_90: train_l:0.1959065061349135, train_accuracy:0.9299021291208791, time_count:0.31659507751464844\n",
      "batch_100: train_l:0.19819610661799364, train_accuracy:0.9289139851485149, time_count:0.3149125576019287\n",
      "batch_110: train_l:0.19901980432841154, train_accuracy:0.9282446509009009, time_count:0.31520748138427734\n",
      "batch_120: train_l:0.20037869320920676, train_accuracy:0.9277182334710744, time_count:0.31488752365112305\n",
      "batch_130: train_l:0.20068425042483642, train_accuracy:0.927719465648855, time_count:0.3153111934661865\n",
      "batch_140: train_l:0.19914814121756994, train_accuracy:0.9281360815602837, time_count:0.3147463798522949\n",
      "batch_150: train_l:0.19778130208419648, train_accuracy:0.9282129552980133, time_count:0.3144698143005371\n",
      "batch_160: train_l:0.1971095411673836, train_accuracy:0.9280619177018633, time_count:0.3142266273498535\n",
      "batch_170: train_l:0.1971904197108676, train_accuracy:0.9277686403508771, time_count:0.31470441818237305\n",
      "batch_180: train_l:0.19544739473755188, train_accuracy:0.9281552140883977, time_count:0.314591646194458\n",
      "batch_190: train_l:0.19538586469688965, train_accuracy:0.9282763416230366, time_count:0.314882755279541\n",
      "batch_200: train_l:0.19563580682473397, train_accuracy:0.9284048507462687, time_count:0.31484198570251465\n",
      "batch_210: train_l:0.19508021384901345, train_accuracy:0.9284101007109005, time_count:0.3144035339355469\n",
      "batch_220: train_l:0.19527585339222558, train_accuracy:0.9280260180995475, time_count:0.3147563934326172\n",
      "batch_230: train_l:0.19497962902376662, train_accuracy:0.9280810335497836, time_count:0.3148915767669678\n",
      "================================\n",
      "epoch_2: test_acc:0.8957331730769231\n",
      "================================\n",
      "batch_0: train_l:0.16801254451274872, train_accuracy:0.93359375, time_count:0.31644344329833984\n",
      "batch_10: train_l:0.13498622179031372, train_accuracy:0.9552556818181818, time_count:0.3148963451385498\n",
      "batch_20: train_l:0.14403554300467172, train_accuracy:0.9507068452380952, time_count:0.31447577476501465\n",
      "batch_30: train_l:0.14175746613933193, train_accuracy:0.9507308467741935, time_count:0.3143885135650635\n",
      "batch_40: train_l:0.14859112006861988, train_accuracy:0.948170731707317, time_count:0.3146395683288574\n",
      "batch_50: train_l:0.1445931202056361, train_accuracy:0.9491421568627451, time_count:0.315030574798584\n",
      "batch_60: train_l:0.14537963613134916, train_accuracy:0.947938012295082, time_count:0.3141813278198242\n",
      "batch_70: train_l:0.14603975939918573, train_accuracy:0.9475682218309859, time_count:0.3147242069244385\n",
      "batch_80: train_l:0.14652729117208058, train_accuracy:0.9473861882716049, time_count:0.31473374366760254\n",
      "batch_90: train_l:0.1459743645819989, train_accuracy:0.9476304945054945, time_count:0.31436944007873535\n",
      "batch_100: train_l:0.1456549947362135, train_accuracy:0.947555693069307, time_count:0.31495070457458496\n",
      "batch_110: train_l:0.1445229332726281, train_accuracy:0.9478462837837838, time_count:0.31562185287475586\n",
      "batch_120: train_l:0.14462902427704866, train_accuracy:0.9474108987603306, time_count:0.3146846294403076\n",
      "batch_130: train_l:0.14474447896234863, train_accuracy:0.9473998091603053, time_count:0.3142867088317871\n",
      "batch_140: train_l:0.14776607396754812, train_accuracy:0.9463652482269503, time_count:0.3147008419036865\n",
      "batch_150: train_l:0.14861344768116805, train_accuracy:0.9460368377483444, time_count:0.3146932125091553\n",
      "batch_160: train_l:0.14884561730652862, train_accuracy:0.9455793866459627, time_count:0.31511664390563965\n",
      "batch_170: train_l:0.14914117435439986, train_accuracy:0.9454038742690059, time_count:0.31440067291259766\n",
      "batch_180: train_l:0.14972664831586965, train_accuracy:0.9451182665745856, time_count:0.3148348331451416\n",
      "batch_190: train_l:0.15024899855178064, train_accuracy:0.9449648232984293, time_count:0.3146638870239258\n",
      "batch_200: train_l:0.1501013052774899, train_accuracy:0.9449432524875622, time_count:0.3149855136871338\n",
      "batch_210: train_l:0.15045607135900388, train_accuracy:0.9450903436018957, time_count:0.3150444030761719\n",
      "batch_220: train_l:0.15108007125185627, train_accuracy:0.9448175904977375, time_count:0.3152320384979248\n",
      "batch_230: train_l:0.1512384731725697, train_accuracy:0.9447544642857143, time_count:0.31476402282714844\n",
      "================================\n",
      "epoch_3: test_acc:0.9051482371794872\n",
      "================================\n",
      "batch_0: train_l:0.11989519000053406, train_accuracy:0.95703125, time_count:0.316922664642334\n",
      "batch_10: train_l:0.0850155892019922, train_accuracy:0.9701704545454546, time_count:0.3145625591278076\n",
      "batch_20: train_l:0.09279026108838263, train_accuracy:0.9672619047619048, time_count:0.31458163261413574\n",
      "batch_30: train_l:0.09874398705940093, train_accuracy:0.9650957661290323, time_count:0.3146798610687256\n",
      "batch_40: train_l:0.09963537452787888, train_accuracy:0.9655106707317073, time_count:0.3146045207977295\n",
      "batch_50: train_l:0.1020027274156318, train_accuracy:0.9638480392156863, time_count:0.3149447441101074\n",
      "batch_60: train_l:0.10290070696443808, train_accuracy:0.9633709016393442, time_count:0.314638614654541\n",
      "batch_70: train_l:0.10383064247352976, train_accuracy:0.9628631161971831, time_count:0.3154442310333252\n",
      "batch_80: train_l:0.10423931019541657, train_accuracy:0.9625289351851852, time_count:0.31467342376708984\n",
      "batch_90: train_l:0.10224610534343091, train_accuracy:0.9634271978021978, time_count:0.314532995223999\n",
      "batch_100: train_l:0.10286995908706495, train_accuracy:0.9631420173267327, time_count:0.3149454593658447\n",
      "batch_110: train_l:0.10393052051464717, train_accuracy:0.9626618806306306, time_count:0.31523799896240234\n",
      "batch_120: train_l:0.10439623519778252, train_accuracy:0.9626162190082644, time_count:0.3145730495452881\n",
      "batch_130: train_l:0.10375857964613056, train_accuracy:0.962905534351145, time_count:0.3148343563079834\n",
      "batch_140: train_l:0.10675306155854929, train_accuracy:0.9619348404255319, time_count:0.31472206115722656\n",
      "batch_150: train_l:0.10865012874567745, train_accuracy:0.9612996688741722, time_count:0.3150637149810791\n",
      "batch_160: train_l:0.10899139313686708, train_accuracy:0.9612529114906833, time_count:0.3151419162750244\n",
      "batch_170: train_l:0.11087381480294362, train_accuracy:0.9605720029239766, time_count:0.31474971771240234\n",
      "batch_180: train_l:0.11131381755952019, train_accuracy:0.9605490331491713, time_count:0.31478261947631836\n",
      "batch_190: train_l:0.11304096466037615, train_accuracy:0.9597513089005235, time_count:0.31441736221313477\n",
      "batch_200: train_l:0.1134844256463039, train_accuracy:0.9593633395522388, time_count:0.31465983390808105\n",
      "batch_210: train_l:0.11365365697803656, train_accuracy:0.9591787618483413, time_count:0.3149290084838867\n",
      "batch_220: train_l:0.1137203869338219, train_accuracy:0.9591522907239819, time_count:0.3153421878814697\n",
      "batch_230: train_l:0.11387808080681991, train_accuracy:0.9590097402597403, time_count:0.31441545486450195\n",
      "================================\n",
      "epoch_4: test_acc:0.9165665064102564\n",
      "================================\n",
      "batch_0: train_l:0.05207132548093796, train_accuracy:0.98046875, time_count:0.31638145446777344\n",
      "batch_10: train_l:0.07446736368266019, train_accuracy:0.9776278409090909, time_count:0.3148181438446045\n",
      "batch_20: train_l:0.06847241716015906, train_accuracy:0.9795386904761905, time_count:0.3145565986633301\n",
      "batch_30: train_l:0.06973333620736676, train_accuracy:0.9782006048387096, time_count:0.3148667812347412\n",
      "batch_40: train_l:0.06851587208305918, train_accuracy:0.9780868902439024, time_count:0.31620025634765625\n",
      "batch_50: train_l:0.06732701963069392, train_accuracy:0.9789368872549019, time_count:0.3147850036621094\n",
      "batch_60: train_l:0.06661999017977324, train_accuracy:0.9792520491803278, time_count:0.31548357009887695\n",
      "batch_70: train_l:0.06835081908379642, train_accuracy:0.9783230633802817, time_count:0.31476879119873047\n",
      "batch_80: train_l:0.06796511425924154, train_accuracy:0.978491512345679, time_count:0.314760684967041\n",
      "batch_90: train_l:0.06971245858777356, train_accuracy:0.9774639423076923, time_count:0.31510066986083984\n",
      "batch_100: train_l:0.06949073008012653, train_accuracy:0.9775680693069307, time_count:0.31485462188720703\n",
      "batch_110: train_l:0.07004252282550207, train_accuracy:0.9773015202702703, time_count:0.314805269241333\n",
      "batch_120: train_l:0.07205409644371714, train_accuracy:0.9764979338842975, time_count:0.3145253658294678\n",
      "batch_130: train_l:0.07360638491809368, train_accuracy:0.9758766698473282, time_count:0.3148198127746582\n",
      "batch_140: train_l:0.0742681319329967, train_accuracy:0.975426640070922, time_count:0.31458473205566406\n",
      "batch_150: train_l:0.07468466101744711, train_accuracy:0.9753207781456954, time_count:0.31465744972229004\n",
      "batch_160: train_l:0.07599161925106686, train_accuracy:0.9746457686335404, time_count:0.31500244140625\n",
      "batch_170: train_l:0.07679944893900763, train_accuracy:0.9743009868421053, time_count:0.3146495819091797\n",
      "batch_180: train_l:0.07714712359422808, train_accuracy:0.974059046961326, time_count:0.31449437141418457\n",
      "batch_190: train_l:0.0780481151865883, train_accuracy:0.9736583769633508, time_count:0.3146183490753174\n",
      "batch_200: train_l:0.0789773561968584, train_accuracy:0.9730837997512438, time_count:0.31442689895629883\n",
      "batch_210: train_l:0.0803674055060363, train_accuracy:0.9724155805687204, time_count:0.3146169185638428\n",
      "batch_220: train_l:0.08198053468170478, train_accuracy:0.9718078337104072, time_count:0.3145260810852051\n",
      "batch_230: train_l:0.08283547745025777, train_accuracy:0.971489448051948, time_count:0.31407928466796875\n",
      "================================\n",
      "epoch_5: test_acc:0.9089543269230769\n",
      "================================\n",
      "batch_0: train_l:0.03187122568488121, train_accuracy:0.98828125, time_count:0.31723690032958984\n",
      "batch_10: train_l:0.04584459046071226, train_accuracy:0.9865056818181818, time_count:0.3145925998687744\n",
      "batch_20: train_l:0.04637215676761809, train_accuracy:0.9864211309523809, time_count:0.31455397605895996\n",
      "batch_30: train_l:0.05226288663764154, train_accuracy:0.9837449596774194, time_count:0.3146243095397949\n",
      "batch_40: train_l:0.05119956266589281, train_accuracy:0.9838033536585366, time_count:0.3147151470184326\n",
      "batch_50: train_l:0.0523711221621317, train_accuracy:0.9824601715686274, time_count:0.31435370445251465\n",
      "batch_60: train_l:0.050770259752381044, train_accuracy:0.9832863729508197, time_count:0.3143000602722168\n",
      "batch_70: train_l:0.050524963328326254, train_accuracy:0.983274647887324, time_count:0.31497859954833984\n",
      "batch_80: train_l:0.050317483436730176, train_accuracy:0.9833140432098766, time_count:0.31459903717041016\n",
      "batch_90: train_l:0.04998143228118891, train_accuracy:0.9834735576923077, time_count:0.3149244785308838\n",
      "batch_100: train_l:0.04915643239965533, train_accuracy:0.9838335396039604, time_count:0.3144519329071045\n",
      "batch_110: train_l:0.0498598000152154, train_accuracy:0.9835655968468469, time_count:0.3143422603607178\n",
      "batch_120: train_l:0.05169349253054493, train_accuracy:0.9826962809917356, time_count:0.31470441818237305\n",
      "batch_130: train_l:0.0519292225762633, train_accuracy:0.9824964217557252, time_count:0.3141632080078125\n",
      "batch_140: train_l:0.05151794397714713, train_accuracy:0.9826296542553191, time_count:0.31487393379211426\n",
      "batch_150: train_l:0.05183397096563254, train_accuracy:0.9826676324503312, time_count:0.31470823287963867\n",
      "batch_160: train_l:0.051504886298446184, train_accuracy:0.9827251552795031, time_count:0.31450581550598145\n",
      "batch_170: train_l:0.052198479714536526, train_accuracy:0.9824104532163743, time_count:0.31493520736694336\n",
      "batch_180: train_l:0.05275318764397123, train_accuracy:0.9821952693370166, time_count:0.3143019676208496\n",
      "batch_190: train_l:0.05353965123637496, train_accuracy:0.9819821662303665, time_count:0.3147594928741455\n",
      "batch_200: train_l:0.053665733984231356, train_accuracy:0.9819263059701493, time_count:0.31476545333862305\n",
      "batch_210: train_l:0.0552200550098696, train_accuracy:0.9812833234597157, time_count:0.31440091133117676\n",
      "batch_220: train_l:0.0550750178280729, train_accuracy:0.9813171662895928, time_count:0.31432652473449707\n",
      "batch_230: train_l:0.05662689874053518, train_accuracy:0.9806378517316018, time_count:0.31464719772338867\n",
      "================================\n",
      "epoch_6: test_acc:0.9076522435897436\n",
      "================================\n",
      "batch_0: train_l:0.019625002518296242, train_accuracy:1.0, time_count:0.31707024574279785\n",
      "batch_10: train_l:0.03423893451690674, train_accuracy:0.9914772727272727, time_count:0.31609225273132324\n",
      "batch_20: train_l:0.03580860048532486, train_accuracy:0.9901413690476191, time_count:0.3162405490875244\n",
      "batch_30: train_l:0.034905668047647324, train_accuracy:0.9910534274193549, time_count:0.31566452980041504\n",
      "batch_40: train_l:0.03329852673156959, train_accuracy:0.9917111280487805, time_count:0.31602907180786133\n",
      "batch_50: train_l:0.03203172072329942, train_accuracy:0.9920343137254902, time_count:0.3162097930908203\n",
      "batch_60: train_l:0.03349402980481992, train_accuracy:0.9907786885245902, time_count:0.31612253189086914\n",
      "batch_70: train_l:0.03353016810412978, train_accuracy:0.9904269366197183, time_count:0.3165006637573242\n",
      "batch_80: train_l:0.03576551030539436, train_accuracy:0.9892939814814815, time_count:0.31624579429626465\n",
      "batch_90: train_l:0.03561626337877997, train_accuracy:0.9892685439560439, time_count:0.31586456298828125\n",
      "batch_100: train_l:0.03430756756869873, train_accuracy:0.989944306930693, time_count:0.31624794006347656\n",
      "batch_110: train_l:0.03421737993689808, train_accuracy:0.990111204954955, time_count:0.31613945960998535\n",
      "batch_120: train_l:0.03365418296461263, train_accuracy:0.9902182334710744, time_count:0.31613993644714355\n",
      "batch_130: train_l:0.034042005107261755, train_accuracy:0.9900405534351145, time_count:0.31623291969299316\n",
      "batch_140: train_l:0.034245874851625016, train_accuracy:0.9900265957446809, time_count:0.3155202865600586\n",
      "batch_150: train_l:0.03422008997557179, train_accuracy:0.9899110099337748, time_count:0.31624484062194824\n",
      "batch_160: train_l:0.034228703446732546, train_accuracy:0.9897369953416149, time_count:0.31589674949645996\n",
      "batch_170: train_l:0.034471076537381136, train_accuracy:0.9894919590643275, time_count:0.31591248512268066\n",
      "batch_180: train_l:0.03464467930530316, train_accuracy:0.9893819060773481, time_count:0.3161163330078125\n",
      "batch_190: train_l:0.035578615358830744, train_accuracy:0.9889561518324608, time_count:0.31567883491516113\n",
      "batch_200: train_l:0.03606569700276674, train_accuracy:0.9887865360696517, time_count:0.316298246383667\n",
      "batch_210: train_l:0.036126220307502704, train_accuracy:0.9886885367298578, time_count:0.3164026737213135\n",
      "batch_220: train_l:0.036957644288558765, train_accuracy:0.9883696266968326, time_count:0.316176176071167\n",
      "batch_230: train_l:0.03767408379776911, train_accuracy:0.9880614177489178, time_count:0.3159019947052002\n",
      "================================\n",
      "epoch_7: test_acc:0.9100560897435898\n",
      "================================\n",
      "batch_0: train_l:0.035786811262369156, train_accuracy:0.984375, time_count:0.3167688846588135\n",
      "batch_10: train_l:0.02894093646583232, train_accuracy:0.9918323863636364, time_count:0.31470561027526855\n",
      "batch_20: train_l:0.022972374356218746, train_accuracy:0.9940476190476191, time_count:0.3145010471343994\n",
      "batch_30: train_l:0.022633084877123757, train_accuracy:0.9933215725806451, time_count:0.31481122970581055\n",
      "batch_40: train_l:0.02339139780620249, train_accuracy:0.993140243902439, time_count:0.3157529830932617\n",
      "batch_50: train_l:0.023370929208456303, train_accuracy:0.9931066176470589, time_count:0.31609058380126953\n",
      "batch_60: train_l:0.022287806976953, train_accuracy:0.9935963114754098, time_count:0.3194148540496826\n",
      "batch_70: train_l:0.024254180379474247, train_accuracy:0.9925726232394366, time_count:0.3187534809112549\n",
      "batch_80: train_l:0.024474748956430474, train_accuracy:0.9927179783950617, time_count:0.31890034675598145\n",
      "batch_90: train_l:0.02403477343454302, train_accuracy:0.992917239010989, time_count:0.320284366607666\n",
      "batch_100: train_l:0.024119957880402852, train_accuracy:0.9929223391089109, time_count:0.3220679759979248\n",
      "batch_110: train_l:0.02456042217090726, train_accuracy:0.9925394144144144, time_count:0.3218259811401367\n",
      "batch_120: train_l:0.026414957093767635, train_accuracy:0.9917678202479339, time_count:0.3210163116455078\n",
      "batch_130: train_l:0.026223520680778583, train_accuracy:0.9918893129770993, time_count:0.36251091957092285\n",
      "batch_140: train_l:0.0261635221177646, train_accuracy:0.9919104609929078, time_count:0.33005690574645996\n",
      "batch_150: train_l:0.025872872763048144, train_accuracy:0.9919546771523179, time_count:0.3400900363922119\n",
      "batch_160: train_l:0.025664629446492986, train_accuracy:0.9920176630434783, time_count:0.3262462615966797\n",
      "batch_170: train_l:0.025237838745836103, train_accuracy:0.9921418128654971, time_count:0.32500147819519043\n",
      "batch_180: train_l:0.025291601445484722, train_accuracy:0.9921659185082873, time_count:0.33386921882629395\n",
      "batch_190: train_l:0.02610032796537923, train_accuracy:0.9917580170157068, time_count:0.32536792755126953\n",
      "batch_200: train_l:0.026343031631161768, train_accuracy:0.9916433457711443, time_count:0.32495641708374023\n",
      "batch_210: train_l:0.026936544164997564, train_accuracy:0.9915025177725119, time_count:0.3251953125\n",
      "batch_220: train_l:0.027502360679967777, train_accuracy:0.9911623303167421, time_count:0.325045108795166\n",
      "batch_230: train_l:0.027834370875432765, train_accuracy:0.9909868777056277, time_count:0.32435107231140137\n",
      "================================\n",
      "epoch_8: test_acc:0.8746995192307693\n",
      "================================\n",
      "batch_0: train_l:0.09595362842082977, train_accuracy:0.96875, time_count:0.31886982917785645\n",
      "batch_10: train_l:0.05019387738271193, train_accuracy:0.984375, time_count:0.3225107192993164\n",
      "batch_20: train_l:0.03490800503641367, train_accuracy:0.9901413690476191, time_count:0.32553935050964355\n",
      "batch_30: train_l:0.03017660840264251, train_accuracy:0.9918094758064516, time_count:0.3165898323059082\n",
      "batch_40: train_l:0.02700650036653004, train_accuracy:0.9927591463414634, time_count:0.3285951614379883\n",
      "batch_50: train_l:0.025448060567107272, train_accuracy:0.9934895833333334, time_count:0.3307962417602539\n",
      "batch_60: train_l:0.022918699171821603, train_accuracy:0.9944928278688525, time_count:0.3332352638244629\n",
      "batch_70: train_l:0.021447289721961593, train_accuracy:0.994993397887324, time_count:0.32967233657836914\n",
      "batch_80: train_l:0.020769356106074503, train_accuracy:0.9949363425925926, time_count:0.3315258026123047\n",
      "batch_90: train_l:0.02006124901063331, train_accuracy:0.9952352335164835, time_count:0.32950901985168457\n",
      "batch_100: train_l:0.019538236201030783, train_accuracy:0.9951655321782178, time_count:0.3298807144165039\n",
      "batch_110: train_l:0.019163178934438807, train_accuracy:0.995143581081081, time_count:0.3332962989807129\n",
      "batch_120: train_l:0.019170952706851742, train_accuracy:0.9949315599173554, time_count:0.3329010009765625\n",
      "batch_130: train_l:0.019221473684292712, train_accuracy:0.994930820610687, time_count:0.32954859733581543\n",
      "batch_140: train_l:0.019012112956467674, train_accuracy:0.9950132978723404, time_count:0.33177757263183594\n",
      "batch_150: train_l:0.018685972299550147, train_accuracy:0.9950848509933775, time_count:0.32993578910827637\n",
      "batch_160: train_l:0.01912586790688845, train_accuracy:0.9949534161490683, time_count:0.330411434173584\n",
      "batch_170: train_l:0.01931945971425688, train_accuracy:0.9949515716374269, time_count:0.3227517604827881\n",
      "batch_180: train_l:0.01912367049841344, train_accuracy:0.9949067679558011, time_count:0.3208653926849365\n",
      "batch_190: train_l:0.019264354555823728, train_accuracy:0.994825752617801, time_count:0.3296687602996826\n",
      "batch_200: train_l:0.01986429949212282, train_accuracy:0.9946167599502488, time_count:0.3285694122314453\n",
      "batch_210: train_l:0.02014095461552177, train_accuracy:0.9944090639810427, time_count:0.3348886966705322\n",
      "batch_220: train_l:0.020470831667523982, train_accuracy:0.9942908653846154, time_count:0.33356499671936035\n",
      "batch_230: train_l:0.021667420362158643, train_accuracy:0.9939123376623377, time_count:0.3310258388519287\n",
      "================================\n",
      "epoch_9: test_acc:0.9176682692307693\n",
      "================================\n"
     ]
    }
   ],
   "source": [
    "from d2l import load_data_fashion_mnist\n",
    "from d2l import train_ch6_gpu\n",
    "lr, num_epochs, batch_size = 0.05, 10, 256\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=96)\n",
    "train_ch6_gpu(net, train_iter, test_iter, num_epochs, lr, 'cuda:0')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 23, 64, 64])\n",
      "torch.Size([128, 10, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# DenseNet: replace ResNet with cat operation: cat output with input in channel dimention\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def conv_block(in_channels, num_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.BatchNorm2d(in_channels), # out_channels = in_channels,\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels,num_channels,kernel_size=3, padding=1)\n",
    "    )\n",
    "\n",
    "class DenseBlock(nn.Module):  # the dense block contains some conv_block with same output_channels\n",
    "    def __init__(self, num_convs, in_channels, num_channels):\n",
    "        super(DenseBlock,self).__init__()\n",
    "        layer = []\n",
    "        for i in range(num_convs):\n",
    "            layer.append(conv_block(in_channels+num_channels*i, num_channels))\n",
    "        self.net = nn.Sequential(*layer)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for blk in self.net:\n",
    "            Y = blk(X)\n",
    "            X = torch.cat((X,Y),dim=1)  # connect output of every layer with its input as the input of next layer\n",
    "        return X\n",
    "\n",
    "blk = DenseBlock(2,3,10)\n",
    "X = torch.rand(128,3,64,64)\n",
    "Y = blk(X)\n",
    "print(Y.shape)\n",
    "\n",
    "def transition_block(in_channels, num_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.BatchNorm2d(in_channels), nn.ReLU(),  # an improved block construction\n",
    "        nn.Conv2d(in_channels,num_channels,kernel_size=1),  # use 1x1 conv layer to decrease channels number to simplify net\n",
    "        nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "    )\n",
    "\n",
    "tblk = transition_block(23,10)\n",
    "print(tblk(Y).shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# imp DenseNet\n",
    "b1 = nn.Sequential(\n",
    "    nn.Conv2d(1,64,kernel_size=7, stride=2, padding=3),\n",
    "    nn.BatchNorm2d(64), nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    ")\n",
    "\n",
    "num_channels = 64\n",
    "growth_rate = 32  # how many channels will be increased when passed a conv layer\n",
    "num_convs_in_dense_blocks = [4,4,4,4]\n",
    "blks = []  # list to contain all sense block\n",
    "for i, num_convs in enumerate(num_convs_in_dense_blocks):\n",
    "    blks.append(DenseBlock(num_convs, num_channels, growth_rate))  # for dense block, output channels == input channels\n",
    "    num_channels += num_convs * growth_rate\n",
    "    if i != len(num_convs_in_dense_blocks) -1 :  # inset a transition_layer between two dense block to decrease channel number and h,w\n",
    "        blks.append(transition_block(num_channels,num_channels//2))\n",
    "        num_channels = num_channels // 2\n",
    "\n",
    "net = nn.Sequential(\n",
    "    b1, *blks,\n",
    "    nn.BatchNorm2d(num_channels), nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d((1,1)),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(num_channels,10)\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net will be trained on cuda:0\n",
      "batch_0: train_l:2.4899649620056152, train_accuracy:0.06640625, time_count:0.5744199752807617\n",
      "batch_10: train_l:1.5340485030954534, train_accuracy:0.5095880681818182, time_count:0.2806875705718994\n",
      "batch_20: train_l:1.227696838833037, train_accuracy:0.6069568452380952, time_count:0.2815587520599365\n",
      "batch_30: train_l:1.0672901368910266, train_accuracy:0.653351814516129, time_count:0.2812154293060303\n",
      "batch_40: train_l:0.9729181528091431, train_accuracy:0.6804496951219512, time_count:0.2823641300201416\n",
      "batch_50: train_l:0.9042483509755602, train_accuracy:0.7001378676470589, time_count:0.2823350429534912\n",
      "batch_60: train_l:0.8512513647313977, train_accuracy:0.7149718237704918, time_count:0.28495335578918457\n",
      "batch_70: train_l:0.809513379990215, train_accuracy:0.727387764084507, time_count:0.28977036476135254\n",
      "batch_80: train_l:0.7742350414211367, train_accuracy:0.7377989969135802, time_count:0.29097485542297363\n",
      "batch_90: train_l:0.7403636843293577, train_accuracy:0.7478537087912088, time_count:0.29653358459472656\n",
      "batch_100: train_l:0.7108869632281879, train_accuracy:0.7570776608910891, time_count:0.29849815368652344\n",
      "batch_110: train_l:0.6862934862708187, train_accuracy:0.7646396396396397, time_count:0.2988622188568115\n",
      "batch_120: train_l:0.6651756650160167, train_accuracy:0.7717910640495868, time_count:0.30179595947265625\n",
      "batch_130: train_l:0.6468516613235911, train_accuracy:0.7774332061068703, time_count:0.3027939796447754\n",
      "batch_140: train_l:0.6285906148717758, train_accuracy:0.7836325354609929, time_count:0.30326032638549805\n",
      "batch_150: train_l:0.6104070635031391, train_accuracy:0.7900455298013245, time_count:0.3029670715332031\n",
      "batch_160: train_l:0.5978860340503432, train_accuracy:0.793453998447205, time_count:0.30353522300720215\n",
      "batch_170: train_l:0.5864925994510539, train_accuracy:0.7969663742690059, time_count:0.3027160167694092\n",
      "batch_180: train_l:0.574843741088941, train_accuracy:0.8006085980662984, time_count:0.3031919002532959\n",
      "batch_190: train_l:0.5637374869815966, train_accuracy:0.8041557591623036, time_count:0.3031198978424072\n",
      "batch_200: train_l:0.5543039135671967, train_accuracy:0.8071944962686567, time_count:0.30353283882141113\n",
      "batch_210: train_l:0.546126592498255, train_accuracy:0.8095379146919431, time_count:0.30316734313964844\n",
      "batch_220: train_l:0.5376641753032736, train_accuracy:0.8124823246606335, time_count:0.3029639720916748\n",
      "batch_230: train_l:0.5292283483655937, train_accuracy:0.8153916396103896, time_count:0.3041410446166992\n",
      "================================\n",
      "epoch_0: test_acc:0.8370392628205128\n",
      "================================\n",
      "batch_0: train_l:0.34617921710014343, train_accuracy:0.87890625, time_count:0.30316877365112305\n",
      "batch_10: train_l:0.33114963498982514, train_accuracy:0.8831676136363636, time_count:0.3030693531036377\n",
      "batch_20: train_l:0.32730304627191453, train_accuracy:0.8826264880952381, time_count:0.30197954177856445\n",
      "batch_30: train_l:0.3246096459127242, train_accuracy:0.8831905241935484, time_count:0.30178117752075195\n",
      "batch_40: train_l:0.3288464382654283, train_accuracy:0.8815739329268293, time_count:0.3020188808441162\n",
      "batch_50: train_l:0.3300529680415696, train_accuracy:0.8802083333333334, time_count:0.3020474910736084\n",
      "batch_60: train_l:0.32352443967686323, train_accuracy:0.8836449795081968, time_count:0.3017897605895996\n",
      "batch_70: train_l:0.3238762552049798, train_accuracy:0.8838028169014085, time_count:0.301624059677124\n",
      "batch_80: train_l:0.32170729743845666, train_accuracy:0.8837287808641975, time_count:0.30196642875671387\n",
      "batch_90: train_l:0.3221344428730535, train_accuracy:0.8831988324175825, time_count:0.3016386032104492\n",
      "batch_100: train_l:0.31966951843535546, train_accuracy:0.8843208539603961, time_count:0.3030233383178711\n",
      "batch_110: train_l:0.3157056995608785, train_accuracy:0.8858037725225225, time_count:0.3018839359283447\n",
      "batch_120: train_l:0.3132170297143873, train_accuracy:0.8867510330578512, time_count:0.30188727378845215\n",
      "batch_130: train_l:0.31281714437117103, train_accuracy:0.8872554866412213, time_count:0.30208826065063477\n",
      "batch_140: train_l:0.3108886460251842, train_accuracy:0.8873559397163121, time_count:0.3021891117095947\n",
      "batch_150: train_l:0.31119452377423545, train_accuracy:0.8874689569536424, time_count:0.30176615715026855\n",
      "batch_160: train_l:0.3089272676047331, train_accuracy:0.8882715450310559, time_count:0.3027472496032715\n",
      "batch_170: train_l:0.3074798534313838, train_accuracy:0.8886376096491229, time_count:0.30147576332092285\n",
      "batch_180: train_l:0.30720820677214566, train_accuracy:0.8884668508287292, time_count:0.3021106719970703\n",
      "batch_190: train_l:0.3074217036132413, train_accuracy:0.8882730693717278, time_count:0.3017287254333496\n",
      "batch_200: train_l:0.3070300595084233, train_accuracy:0.8883317786069652, time_count:0.30185413360595703\n",
      "batch_210: train_l:0.3072127037421222, train_accuracy:0.8881627665876777, time_count:0.30193591117858887\n",
      "batch_220: train_l:0.30595746144180386, train_accuracy:0.8886453619909502, time_count:0.30155372619628906\n",
      "batch_230: train_l:0.30541177700350297, train_accuracy:0.8888494318181818, time_count:0.30167222023010254\n",
      "================================\n",
      "epoch_1: test_acc:0.8907251602564102\n",
      "================================\n",
      "batch_0: train_l:0.2444940060377121, train_accuracy:0.921875, time_count:0.30436205863952637\n",
      "batch_10: train_l:0.26663123206658795, train_accuracy:0.8984375, time_count:0.3016514778137207\n",
      "batch_20: train_l:0.2624211609363556, train_accuracy:0.9017857142857143, time_count:0.3020815849304199\n",
      "batch_30: train_l:0.26872952907316144, train_accuracy:0.9005796370967742, time_count:0.30372190475463867\n",
      "batch_40: train_l:0.2663714158825758, train_accuracy:0.9025342987804879, time_count:0.30185556411743164\n",
      "batch_50: train_l:0.26731565974506677, train_accuracy:0.9028033088235294, time_count:0.3025994300842285\n",
      "batch_60: train_l:0.2657428291000304, train_accuracy:0.902984118852459, time_count:0.3015315532684326\n",
      "batch_70: train_l:0.26743338288555685, train_accuracy:0.9016835387323944, time_count:0.3027381896972656\n",
      "batch_80: train_l:0.26442714863353306, train_accuracy:0.9025366512345679, time_count:0.30248117446899414\n",
      "batch_90: train_l:0.264207009267021, train_accuracy:0.9026442307692307, time_count:0.30196237564086914\n",
      "batch_100: train_l:0.2626797353277112, train_accuracy:0.9033106435643564, time_count:0.3016073703765869\n",
      "batch_110: train_l:0.26462191665494766, train_accuracy:0.9024845157657657, time_count:0.30156421661376953\n",
      "batch_120: train_l:0.26302944160689995, train_accuracy:0.9034090909090909, time_count:0.30198192596435547\n",
      "batch_130: train_l:0.2613453652340037, train_accuracy:0.9041626908396947, time_count:0.30169224739074707\n",
      "batch_140: train_l:0.26140608908014096, train_accuracy:0.9045046542553191, time_count:0.30179452896118164\n",
      "batch_150: train_l:0.25901821413577, train_accuracy:0.9054221854304636, time_count:0.30214905738830566\n",
      "batch_160: train_l:0.25952155989889775, train_accuracy:0.9049640916149069, time_count:0.3016548156738281\n",
      "batch_170: train_l:0.2597876998939012, train_accuracy:0.9048336988304093, time_count:0.30216217041015625\n",
      "batch_180: train_l:0.2586063084022775, train_accuracy:0.9048256215469613, time_count:0.30242133140563965\n",
      "batch_190: train_l:0.25802484763230327, train_accuracy:0.9053910340314136, time_count:0.3017578125\n",
      "batch_200: train_l:0.25619749546940646, train_accuracy:0.906075093283582, time_count:0.3017261028289795\n",
      "batch_210: train_l:0.25630595806933126, train_accuracy:0.9057131220379147, time_count:0.3031599521636963\n",
      "batch_220: train_l:0.255847515968176, train_accuracy:0.9058611425339367, time_count:0.3017761707305908\n",
      "batch_230: train_l:0.25486670318362, train_accuracy:0.9063345508658008, time_count:0.3035128116607666\n",
      "================================\n",
      "epoch_2: test_acc:0.8171073717948718\n",
      "================================\n",
      "batch_0: train_l:0.1875506043434143, train_accuracy:0.9375, time_count:0.30319666862487793\n",
      "batch_10: train_l:0.23707440495491028, train_accuracy:0.9094460227272727, time_count:0.30219054222106934\n",
      "batch_20: train_l:0.22878707235767728, train_accuracy:0.9166666666666666, time_count:0.30179667472839355\n",
      "batch_30: train_l:0.22595093567525187, train_accuracy:0.9199848790322581, time_count:0.3021669387817383\n",
      "batch_40: train_l:0.22505071468469573, train_accuracy:0.9188262195121951, time_count:0.3019890785217285\n",
      "batch_50: train_l:0.22689647359006546, train_accuracy:0.9188878676470589, time_count:0.3025639057159424\n",
      "batch_60: train_l:0.22776019206789674, train_accuracy:0.9184810450819673, time_count:0.30291152000427246\n",
      "batch_70: train_l:0.22858795124880024, train_accuracy:0.9178036971830986, time_count:0.3027522563934326\n",
      "batch_80: train_l:0.2289523791383814, train_accuracy:0.9171489197530864, time_count:0.301727294921875\n",
      "batch_90: train_l:0.22724762427937853, train_accuracy:0.9180546016483516, time_count:0.30270862579345703\n",
      "batch_100: train_l:0.22630540702012505, train_accuracy:0.9182008044554455, time_count:0.3018038272857666\n",
      "batch_110: train_l:0.22486040882162145, train_accuracy:0.9190948761261262, time_count:0.30179572105407715\n",
      "batch_120: train_l:0.22517152931079393, train_accuracy:0.9188726756198347, time_count:0.30249667167663574\n",
      "batch_130: train_l:0.22448508600697262, train_accuracy:0.9187440362595419, time_count:0.3026580810546875\n",
      "batch_140: train_l:0.2274918259246975, train_accuracy:0.9173869680851063, time_count:0.3032243251800537\n",
      "batch_150: train_l:0.2256029812705438, train_accuracy:0.9183567880794702, time_count:0.30277228355407715\n",
      "batch_160: train_l:0.2255281743055545, train_accuracy:0.9182113742236024, time_count:0.30248117446899414\n",
      "batch_170: train_l:0.2258502499401918, train_accuracy:0.9178545321637427, time_count:0.3016932010650635\n",
      "batch_180: train_l:0.22504670308769079, train_accuracy:0.9182061464088398, time_count:0.3029172420501709\n",
      "batch_190: train_l:0.22445262695482265, train_accuracy:0.9184391361256544, time_count:0.30199289321899414\n",
      "batch_200: train_l:0.22418913237787597, train_accuracy:0.9183379975124378, time_count:0.30182409286499023\n",
      "batch_210: train_l:0.22403943559852257, train_accuracy:0.91848711492891, time_count:0.30224180221557617\n",
      "batch_220: train_l:0.22408279783315788, train_accuracy:0.9183752828054299, time_count:0.30219340324401855\n",
      "batch_230: train_l:0.22440395468757265, train_accuracy:0.9183745941558441, time_count:0.30214405059814453\n",
      "================================\n",
      "epoch_3: test_acc:0.8369391025641025\n",
      "================================\n",
      "batch_0: train_l:0.24218320846557617, train_accuracy:0.9140625, time_count:0.3052992820739746\n",
      "batch_10: train_l:0.21514341235160828, train_accuracy:0.9183238636363636, time_count:0.30178308486938477\n",
      "batch_20: train_l:0.21255616346995035, train_accuracy:0.9203869047619048, time_count:0.3024873733520508\n",
      "batch_30: train_l:0.20558227406394097, train_accuracy:0.9236391129032258, time_count:0.3023695945739746\n",
      "batch_40: train_l:0.20494987833790662, train_accuracy:0.9233993902439024, time_count:0.30286669731140137\n",
      "batch_50: train_l:0.2066231435712646, train_accuracy:0.9237132352941176, time_count:0.3019537925720215\n",
      "batch_60: train_l:0.20720680509922934, train_accuracy:0.9234759221311475, time_count:0.30263185501098633\n",
      "batch_70: train_l:0.20719151125407556, train_accuracy:0.9239106514084507, time_count:0.30235862731933594\n",
      "batch_80: train_l:0.20629888027906418, train_accuracy:0.9244309413580247, time_count:0.30182862281799316\n",
      "batch_90: train_l:0.20602854259394027, train_accuracy:0.9245793269230769, time_count:0.30254268646240234\n",
      "batch_100: train_l:0.20724305871984747, train_accuracy:0.9238474628712872, time_count:0.30273962020874023\n",
      "batch_110: train_l:0.20727907879664018, train_accuracy:0.9238105292792793, time_count:0.30311012268066406\n",
      "batch_120: train_l:0.20747530737445374, train_accuracy:0.9237797004132231, time_count:0.3031623363494873\n",
      "batch_130: train_l:0.20674913012571916, train_accuracy:0.923843034351145, time_count:0.30219101905822754\n",
      "batch_140: train_l:0.20699512139491155, train_accuracy:0.9239527925531915, time_count:0.30179476737976074\n",
      "batch_150: train_l:0.20574881486742702, train_accuracy:0.9249275662251656, time_count:0.3020772933959961\n",
      "batch_160: train_l:0.20631922869393543, train_accuracy:0.9248350155279503, time_count:0.30480241775512695\n",
      "batch_170: train_l:0.20512926434738593, train_accuracy:0.9249817251461988, time_count:0.3019840717315674\n",
      "batch_180: train_l:0.20603393573951984, train_accuracy:0.9246374309392266, time_count:0.3027150630950928\n",
      "batch_190: train_l:0.20639970044347003, train_accuracy:0.9247177683246073, time_count:0.3036465644836426\n",
      "batch_200: train_l:0.20507755760678012, train_accuracy:0.9254314365671642, time_count:0.3022589683532715\n",
      "batch_210: train_l:0.20483341399951005, train_accuracy:0.9255405805687204, time_count:0.30237436294555664\n",
      "batch_220: train_l:0.20525507854795025, train_accuracy:0.925162613122172, time_count:0.30176639556884766\n",
      "batch_230: train_l:0.20476246283296898, train_accuracy:0.9253415854978355, time_count:0.30312657356262207\n",
      "================================\n",
      "epoch_4: test_acc:0.8913261217948718\n",
      "================================\n",
      "batch_0: train_l:0.24116653203964233, train_accuracy:0.8984375, time_count:0.3069941997528076\n",
      "batch_10: train_l:0.1878209886225787, train_accuracy:0.9293323863636364, time_count:0.3027803897857666\n",
      "batch_20: train_l:0.1820675649103664, train_accuracy:0.9313616071428571, time_count:0.30171847343444824\n",
      "batch_30: train_l:0.18621563118311665, train_accuracy:0.9304435483870968, time_count:0.3022804260253906\n",
      "batch_40: train_l:0.19102507684288955, train_accuracy:0.9274009146341463, time_count:0.3025174140930176\n",
      "batch_50: train_l:0.19510560146733827, train_accuracy:0.9261642156862745, time_count:0.3017420768737793\n",
      "batch_60: train_l:0.19543441665954278, train_accuracy:0.926421618852459, time_count:0.3027026653289795\n",
      "batch_70: train_l:0.19181806163888582, train_accuracy:0.9284220950704225, time_count:0.30249571800231934\n",
      "batch_80: train_l:0.1901921957363317, train_accuracy:0.9299286265432098, time_count:0.302262544631958\n",
      "batch_90: train_l:0.19089437218812796, train_accuracy:0.929945054945055, time_count:0.30179929733276367\n",
      "batch_100: train_l:0.18991217009796954, train_accuracy:0.9305383663366337, time_count:0.30213427543640137\n",
      "batch_110: train_l:0.19012413845137432, train_accuracy:0.930567286036036, time_count:0.30199217796325684\n",
      "batch_120: train_l:0.18954539317483746, train_accuracy:0.9308819731404959, time_count:0.3018331527709961\n",
      "batch_130: train_l:0.19021478244139037, train_accuracy:0.9305522423664122, time_count:0.30228686332702637\n",
      "batch_140: train_l:0.19036254355459348, train_accuracy:0.9305186170212766, time_count:0.30224156379699707\n",
      "batch_150: train_l:0.190854298111224, train_accuracy:0.9303083609271523, time_count:0.30216002464294434\n",
      "batch_160: train_l:0.1908243875603498, train_accuracy:0.9302212732919255, time_count:0.30291056632995605\n",
      "batch_170: train_l:0.1908308796105329, train_accuracy:0.9300301535087719, time_count:0.3024318218231201\n",
      "batch_180: train_l:0.1912404623561801, train_accuracy:0.9299033149171271, time_count:0.3031775951385498\n",
      "batch_190: train_l:0.1905700812477092, train_accuracy:0.9300556282722513, time_count:0.3026309013366699\n",
      "batch_200: train_l:0.19049761503638318, train_accuracy:0.9299790111940298, time_count:0.30329060554504395\n",
      "batch_210: train_l:0.18932491781022312, train_accuracy:0.9303169431279621, time_count:0.30231142044067383\n",
      "batch_220: train_l:0.18848399554981904, train_accuracy:0.9306242929864253, time_count:0.30232667922973633\n",
      "batch_230: train_l:0.1886330284339525, train_accuracy:0.9304822781385281, time_count:0.3023550510406494\n",
      "================================\n",
      "epoch_5: test_acc:0.9072516025641025\n",
      "================================\n",
      "batch_0: train_l:0.21072477102279663, train_accuracy:0.9296875, time_count:0.308164119720459\n",
      "batch_10: train_l:0.16317039795897223, train_accuracy:0.9364346590909091, time_count:0.3032102584838867\n",
      "batch_20: train_l:0.17250177157776697, train_accuracy:0.9345238095238095, time_count:0.3033149242401123\n",
      "batch_30: train_l:0.17031449512127908, train_accuracy:0.9368699596774194, time_count:0.3040335178375244\n",
      "batch_40: train_l:0.1707991930406268, train_accuracy:0.936451981707317, time_count:0.30355215072631836\n",
      "batch_50: train_l:0.17377793628211116, train_accuracy:0.9358149509803921, time_count:0.3039669990539551\n",
      "batch_60: train_l:0.1727722735434282, train_accuracy:0.9360271516393442, time_count:0.30448365211486816\n",
      "batch_70: train_l:0.17166569682074265, train_accuracy:0.9360145246478874, time_count:0.30351877212524414\n",
      "batch_80: train_l:0.17038689940064042, train_accuracy:0.9369695216049383, time_count:0.30385541915893555\n",
      "batch_90: train_l:0.17045668365208658, train_accuracy:0.9371136675824175, time_count:0.30325746536254883\n",
      "batch_100: train_l:0.1717783355034224, train_accuracy:0.9363784034653465, time_count:0.31189393997192383\n",
      "batch_110: train_l:0.17216968328297674, train_accuracy:0.9367609797297297, time_count:0.3037729263305664\n",
      "batch_120: train_l:0.1717323502602656, train_accuracy:0.9368543388429752, time_count:0.30475354194641113\n",
      "batch_130: train_l:0.17383534074512147, train_accuracy:0.9360090648854962, time_count:0.3222968578338623\n",
      "batch_140: train_l:0.17379055948967628, train_accuracy:0.9361425088652482, time_count:0.3028273582458496\n",
      "batch_150: train_l:0.1743074638756695, train_accuracy:0.9362065397350994, time_count:0.30394482612609863\n",
      "batch_160: train_l:0.1733000880998114, train_accuracy:0.9366993400621118, time_count:0.30388331413269043\n",
      "batch_170: train_l:0.17297473663126517, train_accuracy:0.9365862573099415, time_count:0.3033175468444824\n",
      "batch_180: train_l:0.1720645566012978, train_accuracy:0.9370252071823204, time_count:0.30397796630859375\n",
      "batch_190: train_l:0.17154440747973806, train_accuracy:0.9373363874345549, time_count:0.3079831600189209\n",
      "batch_200: train_l:0.17212648096665814, train_accuracy:0.9372473569651741, time_count:0.3036773204803467\n",
      "batch_210: train_l:0.17277876439535222, train_accuracy:0.9370371741706162, time_count:0.30525755882263184\n",
      "batch_220: train_l:0.17321251474354601, train_accuracy:0.9368636877828054, time_count:0.30469298362731934\n",
      "batch_230: train_l:0.1732376937742357, train_accuracy:0.9369250541125541, time_count:0.3080415725708008\n",
      "================================\n",
      "epoch_6: test_acc:0.9060496794871795\n",
      "================================\n",
      "batch_0: train_l:0.12024681270122528, train_accuracy:0.95703125, time_count:0.30477070808410645\n",
      "batch_10: train_l:0.13822818682952362, train_accuracy:0.9488636363636364, time_count:0.30334949493408203\n",
      "batch_20: train_l:0.14869073352643422, train_accuracy:0.9453125, time_count:0.30580735206604004\n",
      "batch_30: train_l:0.14848225804105883, train_accuracy:0.9458165322580645, time_count:0.305574893951416\n",
      "batch_40: train_l:0.14901653586364375, train_accuracy:0.9459794207317073, time_count:0.3054959774017334\n",
      "batch_50: train_l:0.1516957651166355, train_accuracy:0.9452359068627451, time_count:0.30535101890563965\n",
      "batch_60: train_l:0.14969509143809803, train_accuracy:0.9461449795081968, time_count:0.3053925037384033\n",
      "batch_70: train_l:0.15183233910463226, train_accuracy:0.9454225352112676, time_count:0.30501794815063477\n",
      "batch_80: train_l:0.1509648021540524, train_accuracy:0.9450713734567902, time_count:0.3055722713470459\n",
      "batch_90: train_l:0.15259597253995938, train_accuracy:0.9441535027472527, time_count:0.30622148513793945\n",
      "batch_100: train_l:0.15321200665566, train_accuracy:0.9439975247524752, time_count:0.30573463439941406\n",
      "batch_110: train_l:0.1541263611467035, train_accuracy:0.9435529279279279, time_count:0.3050651550292969\n",
      "batch_120: train_l:0.15672820794188286, train_accuracy:0.9427944214876033, time_count:0.3050065040588379\n",
      "batch_130: train_l:0.15687194222029838, train_accuracy:0.9428375477099237, time_count:0.3059122562408447\n",
      "batch_140: train_l:0.15673012011651452, train_accuracy:0.9428745567375887, time_count:0.309938907623291\n",
      "batch_150: train_l:0.15817182787404155, train_accuracy:0.9422340645695364, time_count:0.303372859954834\n",
      "batch_160: train_l:0.15718666806539394, train_accuracy:0.9427406832298136, time_count:0.318035364151001\n",
      "batch_170: train_l:0.1576051067626267, train_accuracy:0.9427311769005848, time_count:0.30783629417419434\n",
      "batch_180: train_l:0.15878091423221716, train_accuracy:0.942075276243094, time_count:0.30698251724243164\n",
      "batch_190: train_l:0.15926000943982788, train_accuracy:0.941794829842932, time_count:0.3090546131134033\n",
      "batch_200: train_l:0.1599627137184143, train_accuracy:0.941425684079602, time_count:0.3273763656616211\n",
      "batch_210: train_l:0.16017239325419422, train_accuracy:0.9414432760663507, time_count:0.3045527935028076\n",
      "batch_220: train_l:0.15980473929400899, train_accuracy:0.9416360294117647, time_count:0.3042745590209961\n",
      "batch_230: train_l:0.16018985011876918, train_accuracy:0.9413555194805194, time_count:0.3071153163909912\n",
      "================================\n",
      "epoch_7: test_acc:0.9006410256410257\n",
      "================================\n",
      "batch_0: train_l:0.11184532940387726, train_accuracy:0.96484375, time_count:0.30690956115722656\n",
      "batch_10: train_l:0.1336290626363321, train_accuracy:0.94921875, time_count:0.3039216995239258\n",
      "batch_20: train_l:0.13550723982708796, train_accuracy:0.9514508928571429, time_count:0.30367088317871094\n",
      "batch_30: train_l:0.14063948489004566, train_accuracy:0.9478326612903226, time_count:0.3037865161895752\n",
      "batch_40: train_l:0.13889290883046826, train_accuracy:0.9491234756097561, time_count:0.30643749237060547\n",
      "batch_50: train_l:0.1444405723436206, train_accuracy:0.9470741421568627, time_count:0.3059701919555664\n",
      "batch_60: train_l:0.14381562503146345, train_accuracy:0.9470414959016393, time_count:0.3016178607940674\n",
      "batch_70: train_l:0.14164942985689136, train_accuracy:0.948393485915493, time_count:0.3038761615753174\n",
      "batch_80: train_l:0.1433586929867297, train_accuracy:0.9478202160493827, time_count:0.30175328254699707\n",
      "batch_90: train_l:0.14403347007848405, train_accuracy:0.9480168269230769, time_count:0.30217647552490234\n",
      "batch_100: train_l:0.1458303324951984, train_accuracy:0.9469368811881188, time_count:0.30283617973327637\n",
      "batch_110: train_l:0.14534828758186047, train_accuracy:0.9469664977477478, time_count:0.3053305149078369\n",
      "batch_120: train_l:0.14585266982720904, train_accuracy:0.9466038223140496, time_count:0.3052968978881836\n",
      "batch_130: train_l:0.14651536674217414, train_accuracy:0.9463263358778626, time_count:0.3046119213104248\n",
      "batch_140: train_l:0.14717392866493118, train_accuracy:0.9458388741134752, time_count:0.3049907684326172\n",
      "batch_150: train_l:0.1472991126163906, train_accuracy:0.9459074917218543, time_count:0.3055739402770996\n",
      "batch_160: train_l:0.14805689616047818, train_accuracy:0.9457492236024845, time_count:0.30268001556396484\n",
      "batch_170: train_l:0.14935333425538583, train_accuracy:0.9453581871345029, time_count:0.32070350646972656\n",
      "batch_180: train_l:0.15078760555929901, train_accuracy:0.9447945441988951, time_count:0.3032491207122803\n",
      "batch_190: train_l:0.1495131442606137, train_accuracy:0.9453125, time_count:0.30423569679260254\n",
      "batch_200: train_l:0.1493120100071181, train_accuracy:0.9454679726368159, time_count:0.3033318519592285\n",
      "batch_210: train_l:0.14999757163332536, train_accuracy:0.9451829087677726, time_count:0.3024439811706543\n",
      "batch_220: train_l:0.1504228393551451, train_accuracy:0.9451534219457014, time_count:0.3114454746246338\n",
      "batch_230: train_l:0.15084720167504762, train_accuracy:0.9449742965367965, time_count:0.30318737030029297\n",
      "================================\n",
      "epoch_8: test_acc:0.8641826923076923\n",
      "================================\n",
      "batch_0: train_l:0.14625170826911926, train_accuracy:0.94140625, time_count:0.30628252029418945\n",
      "batch_10: train_l:0.1281081952831962, train_accuracy:0.9556107954545454, time_count:0.3022594451904297\n",
      "batch_20: train_l:0.12031066595088868, train_accuracy:0.9577752976190477, time_count:0.30524396896362305\n",
      "batch_30: train_l:0.12024371902788838, train_accuracy:0.958039314516129, time_count:0.3057217597961426\n",
      "batch_40: train_l:0.12059814264861549, train_accuracy:0.9574123475609756, time_count:0.3052952289581299\n",
      "batch_50: train_l:0.12256363618607614, train_accuracy:0.9560355392156863, time_count:0.30494260787963867\n",
      "batch_60: train_l:0.12367436424142024, train_accuracy:0.9556224385245902, time_count:0.30550336837768555\n",
      "batch_70: train_l:0.125869546977567, train_accuracy:0.9544454225352113, time_count:0.3051002025604248\n",
      "batch_80: train_l:0.1266935846687835, train_accuracy:0.9539930555555556, time_count:0.30629682540893555\n",
      "batch_90: train_l:0.1296785431248801, train_accuracy:0.9530391483516484, time_count:0.3050541877746582\n",
      "batch_100: train_l:0.13025190814001725, train_accuracy:0.9528542698019802, time_count:0.30683469772338867\n",
      "batch_110: train_l:0.1303845522505743, train_accuracy:0.9531601914414415, time_count:0.3042750358581543\n",
      "batch_120: train_l:0.13057931130828937, train_accuracy:0.9528021694214877, time_count:0.3148164749145508\n",
      "batch_130: train_l:0.13111750474412934, train_accuracy:0.9525584446564885, time_count:0.3135650157928467\n",
      "batch_140: train_l:0.13288959549039814, train_accuracy:0.9518229166666666, time_count:0.30727219581604004\n",
      "batch_150: train_l:0.13336730294472335, train_accuracy:0.9517021937086093, time_count:0.30890321731567383\n",
      "batch_160: train_l:0.13333498061813923, train_accuracy:0.9519604037267081, time_count:0.3087320327758789\n",
      "batch_170: train_l:0.134481410185496, train_accuracy:0.9514574195906432, time_count:0.308086633682251\n",
      "batch_180: train_l:0.13478207925735916, train_accuracy:0.9514632251381215, time_count:0.30844759941101074\n",
      "batch_190: train_l:0.13532754548713175, train_accuracy:0.9512639070680629, time_count:0.3071305751800537\n",
      "batch_200: train_l:0.1364454095488164, train_accuracy:0.9509289490049752, time_count:0.31022047996520996\n",
      "batch_210: train_l:0.13718146138705348, train_accuracy:0.9506072274881516, time_count:0.3086380958557129\n",
      "batch_220: train_l:0.13812829914810432, train_accuracy:0.9500318156108597, time_count:0.30972886085510254\n",
      "batch_230: train_l:0.13844863789809214, train_accuracy:0.9498444264069265, time_count:0.3068671226501465\n",
      "================================\n",
      "epoch_9: test_acc:0.8830128205128205\n",
      "================================\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs, batch_size = 0.1, 10, 256\n",
    "from d2l import load_data_fashion_mnist,train_ch6_gpu\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=96)\n",
    "train_ch6_gpu(net, train_iter, test_iter, num_epochs, lr, 'cuda:0')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}